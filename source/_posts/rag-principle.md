---
title: RAG -Principle
mathjax: true
date: 2024-08-02 00:06:25
cover: https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-principle-4736980.png
categories:
  - AI
  - RAG
tags:
  - AI
  - RAG
  - LLM
---

# LLM 局限

1. 当设计一个 LLM **问答**应用，模型需要处理用户的**领域问题**时，LLM **通常**表现**出色**
2. 但有时提供的答案并**不准确**，甚至出现**错误**
3. 当用户需要**获取实时信息**时，LLM 无法及时提供最新的答案
4. LLM 在**知识**、**理解**和**推理**方面展现了**卓越**的能力，在**复杂交互场景**中表现尤为突出
5. LLM 存在无法忽略的局限性

<!-- more -->

> LLM 局限

| Limitation       | Desc                                                         |
| ---------------- | ------------------------------------------------------------ |
| **领域知识**缺乏 | LLM 的知识来源于**训练数据**，主要为**公开数据集**，无法覆盖**特定领域**或**高度专业化**的内部知识 |
| 信息**过时**     | LLM 难以处理**实时信息**，训练过程**耗时**且**成本高昂**，模型一旦训练完成，就难以处理和获取信息 |
| **幻觉**         | 模型基于**概率**生成文本，有时会输出**看似合理**但**实际错误**的答案 |
| **数据安全**     | 需要在**确保数据安全**的前提下，使 LLM 有效利用**私有数据**进行**推理**和**生成** |

> RAG 应运而生

1. 将**非参数化**的**外部知识库和文档**与 LLM 结合
2. RAG 使 LLM 在**生成内容之前**，能够**先检索**相关信息
   - 弥补 LLM 在**知识专业性**和**时效性**的不足
   - 在确保**数据安全**的同时，充分利用**领域知识**和**私有数据**

> 选择 **RAG** 而不是直接将**所有知识库数据**交给 LLM

1. LLM 能够处理的 **Token 数有限**，输入过多的 Token 会**增加成本**
2. 提供少量相关的**关键信息**能够带来**更优质的回答**

> 将相关的**实时信息**转化为**知识库内容**，并通过**检索模块**检索到与用户查询**高度相关**的文档片段，提供**更有价值的回答**

![5f10a52ebc4d00ed4bc3ceyy756c6d0a](https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/5f10a52ebc4d00ed4bc3ceyy756c6d0a.png)

# RAG 定义

1. RAG 是一种结合**检索**和**生成**的 **NLP** 模型架构
2. RAG 由 **Facebook AI** 于 **2022** 提出，主要是为了提升**生成式模型**在处理**开放域问答**、**对话生成**等复杂任务中的性能
3. RAG 通过引入**外部知识库**
   - 利用检索模块（**Retriever**）从**大量文档**中**提取相关信息**
   - 然后将这些信息**传递**给生成模块（**Generator**），从而生成更**准确**且**有用**的回答
4. 核心思想 - 通过**检索**与**生成**的有机结合，弥补 **LLM** 在处理**领域问题**和**实时任务**时的不足
   - **传统**的**生成模型**在面对**复杂问题**时，由于**知识储备不足**，会生成出**错误**或**无关**的回答
   - **RAG** 通过 **Retriever** 获取**相关的背景信息**，使 **Generator** 能够**参考这些信息**，生成更具**可信度**和**准确性**的答案
   - 增强了生成内容的**准确性**，提高了 LLM 在应对**特定领域知识**和**动态信息**的**适应能力**

# RAG 应用

> RAG 的应用有效**优化**了 **LLM 的固有缺陷**，为 LLM 应用提供了**更高的可靠性**和**场景可落地性**

1. **RAG** 结合了**检索**和**生成**，满足了 **LLM** 在**实时性**、**高准确性**和**领域专有知识获取**的需求
2. **企业**或**领域**的**知识管理**与**问答系统**
   - RAG 能**实时**从**企业**或**领域**的**私有知识库**中检索相关信息
   - 确保生成的回答不仅**准确**且符合企业内部的**最新动态**，解决了 LLM 在处理特定领域知识时的局限性
3. **客户支持**与智能**客服**系统
   - RAG 可以动态地将**用户的询问**与最新的产品信息、客服知识等**外部数据**相结合
   - 生成的回答更加**贴合用户的实际需求**，且满足企业需求
4. **医疗 + 金融**
   - 对**数据准确性**和**时效性**有极高的要求，RAG 通过**实时检索最新**的研究成果、市场动态或文档资料
   - RAG 确保生成的内容不仅**基于最新信息**，同时具备**领域专有知识**的**深度分析**能力

# RAG 流程

> 实现了**检索**和**生成**的有机结合，显著提升了 **LLM** 在**领域任务**中的**准确性**和**实时性**

![7bc529003e05a3ab0561204230a83bdc](https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/7bc529003e05a3ab0561204230a83bdc.png)

## Indexing

> 将**外部文档**转化为**可检索的向量**，支撑后续的**检索**和**生成**环节

1. 将各类数据源及其格式统一解析为**纯文本**格式
2. 根据文本的**语义**或**结构**，将文档分割成为**小而语义完整**的**文本块**（**chunk**）
   - 确保系统能够**高效检索和利用** chunk 中包含的**信息**
3. 然后，使用**文本嵌入模型**（**embedding model**），将这些 **chunk** 进行**向量化**
   - 生成**高维稠密向量**，转换为**计算机**能够**理解**的**语义表示**
4. 最后，将这些**向量**存储在**向量数据库**（**vector database**）中，并**构建索引**，完成**知识库**的**构建**

## Retrieval

> 连接**用户查询**和**知识库**

1. 将用户**查询**（**query**）通过**同样**的**文本嵌入模型**（**embedding model**）转换为**向量**表示
   - 将查询（query）**映射**到知识库内容**相同**的**向量空间**中
2. 通过**相似度**度量方法，**Retriever** 从**向量数据库**中筛选出与 query **最相关**的前 K 个 **chunk**
   - 通过**相似性搜索**，Retriever 有效获取了与 query **切实相关**的**外部知识**
   - 为**生成阶段**提供了**精确**且**有意义**的**上下文支持**
3. 这些 **chunk** 将作为**生成阶段输入**的一部分 

## Generation

> 具备**领域知识**和**私有信息**的**精确内容生成**

1. 检索到的**文本块**（**chunk**）与**原始查询**（**query**）共同构成**增强提示词**（**prompt**），输入到 **LLM**
2. LLM 生成**精确**且具备**上下文关联**的回答
   - 符合用户的**查询意图** + 充分利用**检索到的上下文信息**

# RAG vs Fine-tuning

![rag-fine-tuning](https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-fine-tuning.png)

1. 在 LLM **专业领域**场景的应用中，**RAG**（**外部知识**） 和 **Fine-tuning**（**领域能力**） 都是可行的选择
2. **RAG** - 外部知识 - 动态响应 - 频繁更新
   - 场景 - **频繁**处理**实时**信息、回答**复杂**且**依赖外部知识**的问题、回答需具备**可解释性**
   - RAG 通过结合**检索系统**和**生成模型**，能够**实时利用最新信息**，生成**上下文相关**且**准确**的答案
3. **Fine-tuning** - 领域能力 - 深度优化推理
   - 场景 - 需求**稳定**、领域知识**固定**、不需要频繁更新知识库的场景
   - 通过使用**特定领域的数据**对**模型**进行**深度优化**
   - Fine-tuning 可以提升模型在**特定**任务或领域中的**推理**能力，确保输出内容的**专业性**和**一致性**

> 如果即需要利用最新的外部知识，又需要保持高水平的领域推理能力，可以**结合 RAG 和 Fine-tuning**，以实现最佳的性能和效果
