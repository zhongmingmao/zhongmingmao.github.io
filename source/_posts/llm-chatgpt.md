---
title: LLM - ChatGPT
mathjax: false
date: 2023-09-12 00:06:25
cover: https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/llm-chatgpt.png
categories:
  - AI
  - LLM
tags:
  - AI
  - LLM
  - ChatGPT
---

# Timeline

![image-20240715121726607](https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240715121726607.png)

1. OpenAI 在 `NLP` 领域取得了突破性进展
2. ChatGPT 背后包含了一系列的资源整合 - 技术、资源、大厂背书、国际巨头的通力合作 - 工程 + 产品

<!-- more -->

# NLP

## Transformer

> 基于 Transformer 架构的`语言模型`大体可分为两类

1. 以 `BERT` 为代表的掩码语言模型 - `Masked Language Model` - MLM
2. 以 `GPT` 为代表的自回归语言模型 - `Autoregressive Language Mode` - ALM

> OpenAI

1. 创造造福全人类的安全通用人工智能 - `Artificial general intelligence` - AGI
2. 创立之初就摒弃了传统 AI `模型标注`式的训练方式
   - 可用来标注的数据总是`有限`的，而且很难做得非常`通用`

## Autoregressive

> 基于`自回归`的`无监督`训练

1. BERT 由 Google 发布，非常权威，GPT 早期压力巨大 - `GPT-2` 引入了 `zero-shot`
2. 按照人类语言的习惯，语言本身是有`先后顺序`的，下文依赖上文
   - 自回归语言模型代表了标准的语言模型 - `利用上文信息预测下文`
   - 比传统 AI 预测`更加复杂`，但`上限更高`，有望通向 `AGI`
3. 在 GPT-1 和 GPT-2 的探索中没有取得压倒性的效果
   - 但验证了`标准语言模型`在 `zero-shot` 等当面的潜在能力
4. `无监督自回归`的训练方式，使 GPT 模型可以接受`大量文本数据`
   - GPT-3 的参数规模为 `1750 亿`，使用了大约 `45 TB` 的文本数据，一次训练费用为 `460 万美元`
5. GPT-3 还不具备直接和人类对话的能力，而 `ChatGPT` 所使用的模型为 `GPT-3.5`

## Alignment

> 与人类意识对齐

![image-20240716090842759](https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240716090842759.png)

1. GPT-3 和 GPT-3.5 是不同的系列
2. `GPT-3` 经过`充分训练`，但依然不是一个适合与人类进行对话的模型
3. 从 GPT-3 到 GPT-3.5 再到 InstructGPT 和 ChatGPT，`参数规模`并没有太大变化，主要是经历了各种技术的`微调` 
   - 适配`人类`场景 - RLHF - Reinforcement Learning From Human Feedback

## Emergent Ability

> `突现能力` - 大语言模型展现出来的特有的强大能力 - 复杂推理 + 思维链 - NLP 领域所追求

```
问题：小明每天早饭吃2个馒头，他一个月会吃掉多少包馒头？

一个月按30天说，共吃掉60个馒头，每包5个馒头的话，总共12包。

答案：12
```

1. 语言和数学混在一起，在早期 `GPT-3` 模型上进行类型的`推理`，准确率低于 `40%`
2. 后来在 `code-davinci-002` 上进行推理，准确率能达到 `80%`
   - `code-davinci-002` 在`模型规模`上并没有扩大，而是`基于代码进行训练`的
   - 这些突现能力是大模型经过`大量代码训练`后展现出来的能力
   - `代码训练`和`思维链及复杂推理`有很强的相关性 - 暂无确定证据

## 小结

1. 模型`并非越大越好`
   - GPT-3 的参数规模为 1750 亿
   - 微软与英伟达联合开发的 Megatron-Turing 模型拥有超过 5000 个参数
     - 但在性能方面却不是最好的，因为模型未经过充分训练
2. `RLHF` 并不是最早用在 GPT 上，而且在`恰当的时机`用到了 ChatGPT 上
3. 只有 codex 使用了大量代码进行训练

> `自回归`语言模型 + 充分`无监督`训练 + 大量`代码`训练 + 有监督指令`微调` + `RLHF`

# 超大规模预训练

> 超过 40T 的文本数据，`大模型训练` 首先需要高质量的数据集

## 数据集

### GPT-3

> 基础模型 GPT-3 具有 `1750` 亿个参数，训练数据集大约 `500B` 个 Token

![image-20240716234626294](https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240716234626294.png)

> 原始大约 45T 的纯文本数据，经过`过滤`后，大概是 750G 的`高质量`文本数据

### ChatGPT

1. ChatGPT 属于 GPT-3.5 系列
2. 大概率上，ChatGPT 的参数规模要`小于` GPT-3，其训练数据基于大量`对话型数据`进行指令微调
   - 典型数据集 - Persona-Chat 的数据集、康奈尔电影对话语料库、Ubuntu 对话语料库、DailyDialog
   - 互联网上大量`非结构化`数据的训练 - 网站、书籍、其它文本源
     - ChatGPT 能够从更一般的意义上了解语言的`结构`和`模式`，然后可以针对对话管理或者情感分析等特定应用进行`微调`

## 训练成本

1. GPT-3 的单次训练成本高达 460 万美元，为了`找钱`，OpenAI 从开源转为了`闭源`
2. 早期的 OpenAI 是开源的，创办宗旨为创建`通用人工智能`
3. 为了引入资金，OpenAI 从开源转为闭源，设计了一种`商业模式`来吸引投资人，最主要为`微软`

![image-20240717000304560](https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240717000304560.png)

1. OpenAI 的母公司为 `OpenAI Inc` - 为`非营利性组织`
2. 后来成立一家子公司，`OpenAI LP`，即常说的 `OpenAI` - 为一家纯粹的`商业化公司`，设置了最高 `100` 倍的`回报上限`

# 产品化

1. ChatGPT 在真正`产品化`后愿意`公开免费`给普通用户使用
2. 大部分的 AI 厂家只`发布模型`，技术人员去 `Huggingface` 下载然后部署，但这样会将模型限制在一个`非常小的范围`
3. ChatGPT 发布的是`普通大众用户`都可以使用的产品（使用门槛非常低） - 邮箱注册 + 全天候不限时 + 网页对话

# 优点

1. 适用场景多
   - 代码编写、代码翻译、智能问答、语言识别等
2. 使用效果好
   - 微软小冰由`小模型`组成，只能同时处理`特定类型`的任务，`无法相互关联`，此类产品无法做`通用性回答`
   - ChatGPT 像`真人`在回答，甚至有`记忆`和`感情`
3. 工程化应用
   - ChatGPT 是以`大模型为内核`的整套技术完成了`产品化` - 两个月注册用户过亿的世界级产品
