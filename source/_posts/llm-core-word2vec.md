---
title: LLM Core - Word2Vec
mathjax: true
date: 2024-07-06 00:06:25
cover: https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/word2vec.png
categories:
  - AI
  - LLM
tags:
  - AI
  - LLM
  - NLP
  - Word2Vec
---

# Word2Vec

## 概述

1. 在 **NLP** 中，在**文本预处理**后，进行**特征提取**，涉及到将**词语**转化成**数值**的形式，**方便计算机理解**
2. Word2Vec 的目的 - 将**词语**转换成**向量**形式，使得**计算机**能够**理解**
3. 通过学习**大量文本数据**，捕捉到**词语之间的上下文关系**，进而生成**词的高维表示** - 即**词向量**

<!-- more -->

## 架构

> Word2Vec 有 2 种主要的模型 - **Skip-Gram** + **CBOW**

| Model           | Desc                                     |
| --------------- | ---------------------------------------- |
| **<u>CBOW</u>** | 根据周围的**上下文词汇**来预测**目标词** |
| Skip-Gram       | 根据**目标词**预测其周围的**上下文词汇** |

## 优劣

| Key  | Value                                                        |
| ---- | ------------------------------------------------------------ |
| 优点 | 揭示**词**与**词**之间的**相似性** - 通过计算**向量之间的距离**来找到**语义相近**的词 |
| 缺点 | 无法处理**多义词**，每个词被赋予一个向量，不考虑上下文中的**多种含义** |

# 模型架构

> **连续词袋** - Continuous Bag of Words, **CBOW**
> **跳字模型** - Skip-Gram

![word2vec-model](https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/word2vec-model.png)

## 连续词袋

1. **CBOW** 模型是一种通过**上下文**预测**目标词**的神经网络架构
2. 上下文由目标词周围的一个词或多个词组成，这个数目由**窗口大小**决定
3. **窗口**是指**上下文词语的范围** - 如果窗口为 10，那么模型将使用目标词**前后**各 10 个词

## 跳字模型

1. **Skip-Gram** 模型是一种通过**一个给定的目标词**来预测其**上下文词**的神经网络
2. 与 **CBOW** 模型**相反**，Skip-Gram 每次接收一个词作为输入，并预测它周围的词
3. 适用场景 - 处理**较大数据集** + 捕获**罕见词**

# 构建模型

## 数据收集

> 微博内容数据集 - http://www.nlpir.org/wordpress/download/weibo_content_corpus.rar

```xml
<RECORDS>
  <RECORD>
    <id>423</id>
    <article>@小艳子kiki @光影魔术师之择日而栖 @就是爱黑巧克力 尝试新的外景风格，亲们，我有木有拍婚纱照的潜质？</article>
    <discuss>5</discuss>
    <insertTime>2011/11/18 11:08:16</insertTime>
    <origin>新浪微博</origin>
    <person_id>1043652517</person_id>
    <time>2011/11/13 14:38:32</time>
    <transmit>0</transmit>
  </RECORD>
</RECORDS>
```

## 数据预处理

> 加载数据 -> **分词** -> **去除停用词**

```python
import jieba
import xml.etree.ElementTree as ElementTree

# 读取XML文件并解析
file_path = 'data.xml'
tree = ElementTree.parse(file_path)
root = tree.getroot()

# 获取所有<article>标签的内容
texts = [record.find('article').text for record in root.findall('RECORD')]
print("texts: ", len(texts))  # 227532

# 停用词列表，实际应用中需要根据实际情况扩展
stop_words = {"的", "了", "在", "是", "我", "有", "和", "就"}

# 分词和去除停用词
processed_texts = []
for text in texts:
    if text is not None:
        words = jieba.cut(text)  # 使用jieba分词
        processed_text = [word for word in words if word not in stop_words]  # 去除停用词
        processed_texts.append(processed_text)

# 打印预处理后的文本
print("processed_text: ", len(processed_texts))
for text in processed_texts[0:1]:
    print(text)
```

> 输出

```
texts:  227532
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/mf/yq4wcqc961b_wt11zlchrpr40000gn/T/jieba.cache
Loading model cost 0.317 seconds.
Prefix dict has been built successfully.
processed_text:  227333
['@', '小艳子', 'kiki', ' ', '@', '光影', '魔术师', '之', '择日', '而', '栖', ' ', '@', '就是', '爱黑', '巧克力', ' ', '尝试', '新', '外景', '风格', '，', '亲们', '，', '木有', '拍', '婚纱照', '潜质', '？']
```

## 训练模型

```python
from gensim.models import Word2Vec

# 训练Word2Vec模型
model = Word2Vec(sentences=processed_texts, vector_size=100, window=5, min_count=1, workers=4, sg=0)

# 保存模型
model.save("word2vec.model")
```

> 使用 **gensim** 库来训练 **Word2Vec** 模型

| Parameter   | Desc                                                 |
| ----------- | ---------------------------------------------------- |
| vector_size | 词向量的维度                                         |
| window      | 上下文窗口的大小                                     |
| min_count   | 词频的最小阈值                                       |
| workers     | 训练的线程数                                         |
| sg          | Training algorithm: 1 for skip-gram; otherwise CBOW. |

> 执行完成后，本地会生成 3 个文件

1. **word2vec.model**
   - **主模型**文件，包含了模型的**参数**、**词汇表**等信息
   - 存储了模型的**架构**信息，还包括**词汇频率**、**模型训练状态**等信息
   - 该文件是**加载完整模型**所**必需**的
2. **word2vec.model.wv.vectors.npy**
   - 存储了模型中**所有词汇**的**词向量**
   - Word2Vec 模型通过学习这些词向量来**捕捉词语之间的关系**
   - .npy 是 **NumPy 数组**的文件格式
     - 意味着**词向量**是以 **NumPy 数组**的形式存储的，可以**高效**地**加载**和**处理**
3. **word2vec.model.syn1neg.npy**
   - 存储**训练过程**中使用的**负采样权重**
   - 当设置 Word2Vec 模型的 **negative** 参数**大于 0** 时，启用**负采样**来**优化**模型的**训练过程**
   - 该文件中的**权重**是模型训练中用于**负采样**的部分，对于模型的**学习**和**生成词向量**至关重要

## 评估应用

### 相似词

```python
# 加载模型
from gensim.models import Word2Vec

model = Word2Vec.load("word2vec.model")
print("模型加载完成")

# 使用模型
# 获取一个词的向量
print(model.wv['科技'])

# 找到最相似的词
similar_words = model.wv.most_similar('科技', topn=5)
print(similar_words)
```

> 输出 - <u>IT / 产业 / 创新 / 雪旭 / 影响力</u>

```
模型加载完成
[-0.6348176   0.980737    1.3416016  -1.0943173   0.48774573 -0.11640034
 -1.1861979   0.20502706 -0.8404337  -0.20358181 -1.2326834   3.0182338
  0.47568646 -0.79827523 -1.4702528  -2.394634   -1.0629824  -2.0688376
  1.027351    0.37530035 -0.3730972  -3.559652    4.2347307  -0.87267554
 -1.1799742  -0.9208792   0.0861944   1.6402626  -0.53943187  1.1883249
  2.41559    -0.2823076  -3.535211   -1.1645912   0.94778156 -0.4898642
  0.4626486   2.5746973  -2.647992   -1.5699443  -0.4694093   0.8799693
 -1.8221594   1.7643374   1.1880299   1.5864822  -0.17073062 -0.17437774
 -1.640661   -1.0365794   2.4317803  -0.05093269 -0.13435775  0.54337716
  1.5975007   0.45699328  0.03663439 -0.3113667   0.9799479   0.6356506
  0.05430077  2.5633545   1.5153742   0.4160873  -1.0201625   1.658083
 -0.34198096  1.6939774   0.5338615  -1.074114    1.8961443   0.69968593
 -0.09772874  0.3093397   0.81288207 -2.2023287   0.21640573  2.1103783
  1.3334064  -1.5603698   1.5838771   0.7338234  -2.969703   -1.2002374
  0.9270903   0.12712173 -1.3719993   0.8439459  -1.5193253  -2.1521263
  0.48004547  0.18645923 -2.7400064  -0.81356317 -0.01708121  1.6114978
 -1.203191   -1.6334691   0.6092569  -2.7504272 ]
[('IT', 0.6385517716407776), ('产业', 0.6334002614021301), ('创新', 0.6308589577674866), ('雪旭', 0.6188406944274902), ('影响力', 0.6094681024551392)]
```

### 评估方式

1. 词**相似度**计算
   - 通过比较**模型生成**的词与**人工标注**的词的**相似度**来评估模型，**一致性越高**，说明**效果越好**
   - 常用的数据集 - WordSim-353 / SimLex-999
2. 词**类比**计算
   - 评估模型在解决**类比问题**的能力 - 词 A 之于词 B如同词 C 之于什么
   - 例如 - 北京之于中国如同巴黎之于什么？
3. **OOV 词比率** - <u>Out of vision</u>
   - **评估数据集**中有多少词对包含**未知词**（模型**词汇表外**的词）而被排除在评估之外的**比率**
   - 理想情况下，OOV 率应该**尽可能低**，以确保**评估结果**能**更全面地代表模型的性能**
4. **定性分析**
   - 对于**给定**的词汇，查看模型认为与其**最相似**的其它词汇，判定这些词汇是否符合预期
5. **实际应用**
   - 将 Word2Vec 模型应用到具体的**下游任务**，观察模型**表现**的提升
   - 通过比较使用 Word2Vec 词向量**前后**的任务**表现**，可以**间接评估** Word2Vec 模型的有效性

> 手动创建评估集 - 实际评估过程中，评估数据集可以基于**偏好**进行人工打分

| 相似词 1 | 相似词 2 | 人工评分 |
|-|-|-|
|没有|还有 |1.1|
|腐败|媒体|6|
|微博|工作|2|
|北京|世界|7|
|可以|好|9|
|最 |不是|0|
|问题|情况|8|
|他们|网友|7.3|
|今天|明天|6.9|
|美国|社会|3|
|政府|国家|7.2|
|大家|城管|3.0|
|公司|企业|9|
|孩子|朋友|6.1|
|经济|全国|7|
|时间|出现|2.1|
|分享|转发|8|
|记者|认为|5|
|你们|学生|6|

> 评估方式 - 词**类比**计算

```python
# 加载模型
from gensim.models import Word2Vec

model = Word2Vec.load("word2vec.model")
print("模型加载完成")

# 类比
result = model.wv.evaluate_word_pairs("valid.tsv")
print(result)
```

> 输出 - Pearson / Spearman 相关性系数

```
模型加载完成
(PearsonRResult(statistic=0.5439840080013675, pvalue=0.02398644828373347), SignificanceResult(statistic=0.47695369660310627, pvalue=0.052886954016578384), 10.526315789473683)
```

> Pearson 系数

1. statistic=0.5439840080013675
   - 范围 **-1 ~ 1**，其中 **1** 表示**完全正相关**，而 **-1** 表示**完全负相关**，而 **0** 表示**无相关**
   - **54** 意味着**模型的词向量**与**人工评分**之间存在**中等程度**的**正相关**
2. pvalue=0.02398644828373347
   - 用于检验**相关性**的**统计显著性**，一个常用的**显著性水平阈值**为 **0.05**
   - 值为 0.02，**小于 0.05**，意味着这个相关性是**统计显著**的
     - 可以认为**模型的词向量**与**人工评分**之间的相关性不是**偶然出现**的 - **很有说服力**

> Spearman 系数

1. statistic=0.47695369660310627
   - 范围 **-1 ~ 1**，Spearman 系数考虑的是**变量之间**的**等级关系**，而不是**直接的数值大小**
   - **0.47** 同样意味着**中等程度**的**正相关**
2. pvalue=0.052886954016578384
   - **小于 0.05** ≈ 相关性是**统计显著**的

> OOV 率 - **评估数据集**中出现的词汇**不在模型词汇表**里的概率 - **Out of vision**

1. 10.526315789473683 - 大约有 10.53% 的词对包含至少一个不在模型词汇表中的词
2. 该部分词对在**评估过程**中被**忽略**，**不参与相关性计算**

### 小结

1. 一个好的 Word2Vec 模型应该在**相关性测试**中展现出**与人类判断一致**的**趋势**
2. 指标
   - 比较**高**的 **Pearson** 和 **Spearman** 相关性系数（**＞0.5**）
   - 具有**统计显著性**，即较**低**的 **p** 值（**＜ 0.05**）
   - 并具有**可接受**的 **OOV** 率

# 模型优劣

> 容易理解 + 上手简单 + 应用广泛

## 优点

| Advantage            | Desc                                                         |
| -------------------- | ------------------------------------------------------------ |
| 词嵌入**质量高**     | 能够学习到**富含语义信息**的**高质量词向量**，使得**语义上相近**的词在**向量空间**上也**相近** |
| 捕捉多种**语言规律** | 能够捕捉到一定的**语法和语义规律**，如词类比 - 男人之于女人如同国王之于王后 |
| **效率高**           | 相比于早期的基于**矩阵分解**的词嵌入方法，Word2Vec **训练效率更高** - 适用于**大规模语料库** |
| **可解释性**         | 学习到词向量具有一定的可解释性，可以通过**向量运算**进行词之间的**关系探索** |

## 缺点

> 只认识见过的词 + 无法处理多义词 + 依赖大量文本 + 上下文独立 + 词汇级别

| Disadvantage         | Desc                                                         |
| -------------------- | ------------------------------------------------------------ |
| **OOV** 问题         | 只能对其训练期间**见过**的词汇生成词向量，对于**新出现**或者**罕见**的词汇，**无法直接提供词向量** |
| **语义多样性**       | 为**每个词**生成一个**唯一的词向量**，无法直接处理一个词多种含义的情况，即**多义词**问题 |
| 依赖**大量文本数据** | 为了训练出**高质量**的词向量，需要**大量**的文本数据，在数据量教小的情况下，模型效果可能会**受损** |
| **上下文独立**       | 生成的**词向量**是**静态**的，**不考虑**词在特定句子中的**上下文**<br />**ELMo** 和 **BERT** 是**上下文相关**的词嵌入模型 |
| 缺乏**层次化**表示   | 提供的是**词汇级别**的向量表示，缺乏更**细致**的**语法**和**语义**结构，在**复杂的 NLP 任务**中是必须的 |

# 应用场景

> 能够捕捉到**词语之间**的**复杂语义语法关系**，因此在 **NLP 任务**中**广泛使用**

| Task           | Desc                                                         |
| -------------- | ------------------------------------------------------------ |
| 计算文本相似度 | 比较文本中**词向量**的**平均值**或**加权平均值**<br />文档分类、推荐系统中相似项目的检索、在法律文档中查找相关内容 |
| 情感分析       | 识别用户评论、帖子中的情绪态度                               |
| 机器翻译       | 用来生成**源语言**和**目标语言**的**词向量**，通过这些向量来**改进翻译模型的性能** - 罕见词或短语 |
| 搜索引擎优化   | 理解用户的**查询意图**，提高搜索结果的**相关性**<br />分析**查询**和**文档内容**的**词向量**的**相似度**，提供更准确、更贴近用户意图的搜索结果 |
| 内容推荐系统   | 分析用户的阅读或者购买历史，并推荐**语义上相近**的产品或内容 |
