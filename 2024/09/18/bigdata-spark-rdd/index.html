<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark - RDD | ByteCoding</title><meta name="author" content="zhongmingmao"><meta name="copyright" content="zhongmingmao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="分布式内存 传统的 MapReduce 框架运行缓慢，主要原因是 DAG 的中间计算结果需要写入硬盘来防止运行结果丢失 每次调用中间计算结果都需要进行一次硬盘的读取 反复对硬盘进行读写操作以及潜在的数据复制和序列化操作会大大地提高了计算延迟   新的分布式存储方案 - 保持之前系统的稳定性、错误恢复和可扩展性，并尽可能地减少硬盘 IO 操作 RDD 是基于分布式内存的数据抽象，不仅支持基于工作集的">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark - RDD">
<meta property="og:url" content="https://blog.zhongmingmao.top/2024/09/18/bigdata-spark-rdd/index.html">
<meta property="og:site_name" content="ByteCoding">
<meta property="og:description" content="分布式内存 传统的 MapReduce 框架运行缓慢，主要原因是 DAG 的中间计算结果需要写入硬盘来防止运行结果丢失 每次调用中间计算结果都需要进行一次硬盘的读取 反复对硬盘进行读写操作以及潜在的数据复制和序列化操作会大大地提高了计算延迟   新的分布式存储方案 - 保持之前系统的稳定性、错误恢复和可扩展性，并尽可能地减少硬盘 IO 操作 RDD 是基于分布式内存的数据抽象，不仅支持基于工作集的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-rdd.jpg">
<meta property="article:published_time" content="2024-09-17T16:06:25.000Z">
<meta property="article:modified_time" content="2024-11-17T17:59:51.209Z">
<meta property="article:author" content="zhongmingmao">
<meta property="article:tag" content="Big Data">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="RDD">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-rdd.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Spark - RDD",
  "url": "https://blog.zhongmingmao.top/2024/09/18/bigdata-spark-rdd/",
  "image": "https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-rdd.jpg",
  "datePublished": "2024-09-17T16:06:25.000Z",
  "dateModified": "2024-11-17T17:59:51.209Z",
  "author": [
    {
      "@type": "Person",
      "name": "zhongmingmao",
      "url": "https://blog.zhongmingmao.top"
    }
  ]
}</script><link rel="shortcut icon" href="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png"><link rel="canonical" href="https://blog.zhongmingmao.top/2024/09/18/bigdata-spark-rdd/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":32,"languages":{"author":"Author: zhongmingmao","link":"Link: ","source":"Source: ByteCoding","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark - RDD',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ByteCoding" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/url(https:/cdn.pixabay.com/photo/2021/07/20/03/39/fisherman-6479663_1280.jpg));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">640</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-rdd.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ByteCoding</span></a><a class="nav-page-title" href="/"><span class="site-name">Spark - RDD</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Spark - RDD</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2024-09-17T16:06:25.000Z" title="Created 2024-09-18 00:06:25">2024-09-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/">Big Data</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>10mins</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:512,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2024-11-18 01:59:51&quot;}" hidden></div><h1 id="分布式内存"><a href="#分布式内存" class="headerlink" title="分布式内存"></a>分布式内存</h1><ol>
<li>传统的 <strong>MapReduce</strong> 框架<strong>运行缓慢</strong>，主要原因是 <strong>DAG</strong> 的<strong>中间计算结果</strong>需要写入<strong>硬盘</strong>来<strong>防止运行结果丢失</strong></li>
<li>每次调用中间计算结果都需要进行一次硬盘的读取<ul>
<li>反复对硬盘进行<strong>读写</strong>操作以及潜在的<strong>数据复制</strong>和<strong>序列化</strong>操作会大大地提高了<strong>计算延迟</strong></li>
</ul>
</li>
<li>新的<strong>分布式存储</strong>方案 - 保持之前系统的稳定性、错误恢复和可扩展性，并<strong>尽可能地减少硬盘 IO 操作</strong><ul>
<li>RDD 是基于<strong>分布式内存</strong>的数据抽象，不仅支持基于<strong>工作集</strong>的应用，同时具有<strong>数据流模型</strong>的特点</li>
</ul>
</li>
</ol>
<span id="more"></span>

<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><ol>
<li>分区代表<strong>同一个 RDD</strong> 包含的数据被<strong>存储</strong>在系统的<strong>不同节点</strong>上，这是可以<strong>被并行处理</strong>的前提</li>
<li>在<strong>逻辑</strong>上，可以认为 <strong>RDD</strong> 是一个<strong>大数组</strong>，数组中的每个<strong>元素</strong>代表一个<strong>分区</strong>（<strong>Partition</strong>）</li>
<li>在<strong>物理存储</strong>中，每个<strong>分区</strong>指向一个存放在<strong>内存</strong>或者<strong>硬盘</strong>中的<strong>数据块</strong>（<strong>Block</strong>）<ul>
<li>Block 是<strong>独立</strong>的，可以被存放在分布式系统中的<strong>不同节点</strong></li>
</ul>
</li>
<li>RDD 只是<strong>抽象意义</strong>的<strong>数据集合</strong>，分区内部并不会存储具体的数据</li>
</ol>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/image-20241117230114736.png" alt="image-20241117230114736"></p>
<ol>
<li>RDD 中的每个<strong>分区</strong>都有它在该 RDD 中的 <strong>Index</strong></li>
<li>通过 <strong>RDD_ID</strong> 和 <strong>Partition_Index</strong> 可以<strong>唯一确定</strong>对应 <strong>Block</strong> 的编号<ul>
<li>从而通过<strong>底层存储层</strong>的接口中提取到数据进行处理</li>
</ul>
</li>
<li>在集群中，各个节点的 <strong>Block</strong> 会<strong>尽可能</strong>地存放在<strong>内存</strong>中，只有在<strong>内存不足</strong>时，才会写入<strong>硬盘</strong><ul>
<li>可以最大化地减少硬盘读写的开销</li>
</ul>
</li>
<li>RDD <strong>内部存储</strong>的数据是<strong>只读</strong>的，但可以修改<strong>并行计算单元</strong>的<strong>划分结构</strong>，即<strong>分区数量</strong></li>
</ol>
<h2 id="不可变"><a href="#不可变" class="headerlink" title="不可变"></a>不可变</h2><ol>
<li>不可变性代表每个 <strong>RDD</strong> 都是<strong>只读</strong>的，RDD 所包含的<strong>分区信息</strong>是<strong>不可以被改变</strong>的</li>
<li>已有的 RDD 不可以被改变<ul>
<li>只能对现有的 RDD 进行<strong>转换</strong>（<strong>Transformation</strong>）操作，得到新的 RDD 作为<strong>中间计算的结果</strong></li>
<li><strong>RDD</strong> 与<strong>函数式编程</strong>的 <strong>Collection</strong> 很相似</li>
</ul>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lines = sc.textFile(<span class="string">&quot;data.txt&quot;</span>)</span><br><span class="line">lineLengths = lines.map(lambda s: len(s))</span><br><span class="line">totalLength = lineLengths.reduce(lambda a, b: a + b)</span><br></pre></td></tr></table></figure>

<ol>
<li>读入文本文件 data.txt，创建第一个 RDD lines，每一个<strong>元素</strong>就是<strong>一行文本</strong></li>
<li>调用 map 函数去映射产生第二个 RDD lineLengths，每一个<strong>元素</strong>代表每一行简单文本的<strong>字数</strong></li>
<li>调用 reduce 函数得到第三个 RDD totalLength，<strong>只有一个元素</strong>，代表整个文本的<strong>总字数</strong></li>
</ol>
<blockquote>
<p>优势</p>
</blockquote>
<ol>
<li>对于代表<strong>中间结果</strong>的 <strong>RDD</strong>，需要记录它是通过哪个 RDD 进行了哪些转换操作得到的，即<strong>依赖关系</strong><ul>
<li>而<strong>无需立即</strong>去<strong>具体存储</strong>计算出的数据本身</li>
</ul>
</li>
<li>有助于提升 Spark 的<strong>计算效率</strong>，并且使得<strong>错误恢复</strong>更容易<ul>
<li>在一个有 N 步的计算模型中，如果记载第 <strong>N</strong> 步输出 RDD 的<strong>节点</strong>发生了<strong>故障</strong>，导致<strong>数据丢失</strong></li>
<li>可以从第 <strong>N-1</strong> 步的 RDD 出发，再次计算，<strong>无需重复整个 N 步计算过程</strong></li>
<li>这种<strong>容错</strong>机制，也是 RDD 为什么是 <strong>Resilient</strong> 的原因</li>
</ul>
</li>
</ol>
<h2 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h2><ol>
<li>由于单个 RDD 的<strong>分区</strong>特性，使得它天然支持<strong>并行</strong>操作</li>
<li><strong>不同节点</strong>上的数据可以被<strong>分别处理</strong>，然后<strong>产生一个新的 RDD</strong></li>
</ol>
<h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/image-20241117233647328.png" alt="image-20241117233647328"></p>
<ol>
<li><strong>SparkContext</strong> 是<strong>所有 Spark 功能的入口</strong><ul>
<li>代表了<strong>与 Spark 节点的连接</strong>，可以用来<strong>创建 RDD 对象</strong>，在节点中<strong>广播变量</strong>等</li>
<li><strong>一个线程</strong>只有<strong>一个 SparkContext</strong></li>
</ul>
</li>
<li><strong>SparkConf</strong> 是一些<strong>参数配置信息</strong></li>
<li><strong>Partitions</strong> 代表 RDD 中<strong>数据的逻辑结构</strong><ul>
<li>每个 <strong>Partition</strong> 会映射到<strong>某个节点</strong>上<strong>内存</strong>或者<strong>硬盘</strong>的一个 <strong>Block</strong></li>
</ul>
</li>
<li><strong>Partitioner</strong> 决定了 <strong>RDD</strong> 的<strong>分区方式</strong>，主流的分区方式有两种：<strong>Hash partitioner</strong> 和 <strong>Range partitioner</strong><ul>
<li>Hash - 对数据的 Key 进行<strong>散列分区</strong></li>
<li>Range - 按照 Key 的排序进行<strong>均匀分区</strong></li>
</ul>
</li>
</ol>
<h2 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h2><ol>
<li><strong>Dependencies</strong> 是 RDD 中<strong>最重要</strong>的组件之一</li>
<li>Spark 不需要将<strong>每个中间计算结果</strong>进行<strong>数据复制</strong>以<strong>防止数据丢失</strong><ul>
<li>每一步产生的 RDD 里都会存储它的<strong>依赖关系</strong></li>
<li>依赖关系 - 当前的 RDD 是通过哪个 RDD 经过哪个转换操作得到的</li>
</ul>
</li>
<li>窄依赖 vs 宽依赖<ul>
<li><strong>窄依赖</strong> - 允许子 RDD 的<strong>每个分区</strong>可以被<strong>并行处理</strong>产生<ul>
<li>map - 一个父 RDD 分区里的数据不会分散到不同的子 RDD 分区</li>
</ul>
</li>
<li><strong>宽依赖</strong> - 必须等<strong>父 RDD 的所有分区都被计算好</strong>之后才能开始处理<ul>
<li>groupBy - 一个父 RDD 分区里可能有多种 Key 的数据，因此可能被子 RDD 不同的分区所依赖</li>
</ul>
</li>
</ul>
</li>
<li><strong>同一节点 + 链式执行</strong><ul>
<li><strong>窄依赖</strong>可以支持<strong>同一个节点</strong>上<strong>链式执行</strong>多条命令，map -&gt; filter</li>
<li><strong>宽依赖</strong>需要<strong>父 RDD 的所有分区</strong>都是可用的，可能还需要调用类似 MapReduce 之类的操作进行<strong>跨节点传递</strong></li>
</ul>
</li>
<li><strong>失败恢复</strong><ul>
<li><strong>窄依赖</strong>的失败恢复<strong>更有效</strong>，因为只需要<strong>重新计算丢失的父分区</strong>即可</li>
<li><strong>宽依赖</strong>，则牵涉 RDD 各级的<strong>多个父分区</strong></li>
</ul>
</li>
</ol>
<h3 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h3><blockquote>
<p>Narrow Dependency - 父 RDD 的分区可以<strong>一一对应</strong>到子 RDD 的分区</p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/image-20241117234852574.png" alt="image-20241117234852574"></p>
<h3 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h3><blockquote>
<p>Wide Dependency - <strong>父 RDD 的每个分区可以被多个子 RDD 的分区使用</strong></p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/image-20241117235058972.png" alt="image-20241117235058972"></p>
<h2 id="检查点"><a href="#检查点" class="headerlink" title="检查点"></a>检查点</h2><blockquote>
<p>Checkpoint</p>
</blockquote>
<ol>
<li>基于 RDD 的依赖关系，如果任意一个 RDD 在相应的节点丢失<ul>
<li>只需从<strong>上一步的 RDD</strong> 出发再次计算，便可恢复该 RDD</li>
</ul>
</li>
<li>如果一个 RDD 的<strong>依赖链比较长</strong>，而且中间有<strong>多个 RDD 出现故障</strong>，进行恢复会非常<strong>耗费</strong>时间和计算资源<ul>
<li>引入检查点，可以优化这些情况下的<strong>数据恢复</strong></li>
</ul>
</li>
<li>很多<strong>数据库系统</strong>都有检查点机制<ul>
<li>在连续的 transaction 列表中记录某几个 transaction 后数据的<strong>内容</strong>，从而加快错误恢复</li>
</ul>
</li>
<li>在计算过程中，对于一些计算过程比较<strong>耗时</strong>的 RDD<ul>
<li>将它缓存至<strong>硬盘</strong>或者 <strong>HDFS</strong>，标记该 RDD 被检查点<strong>处理过</strong>，并且<strong>清空</strong>它的<strong>所有依赖关系</strong></li>
<li>同时，新建一个依赖于 <strong>CheckpointRDD</strong> 的依赖关系<ul>
<li><strong>CheckpointRDD</strong> 可以从<strong>硬盘</strong>中<strong>读取 RDD</strong> 和<strong>生成新的分区信息</strong></li>
</ul>
</li>
</ul>
</li>
<li>当某个子 RDD 需要错误恢复时，回溯到该 RDD，发现它被检查点<strong>记录过</strong><ul>
<li>直接去<strong>硬盘</strong>读取该 <strong>RDD</strong>，而<strong>无需再向前回溯计算</strong></li>
</ul>
</li>
</ol>
<h2 id="存储级别"><a href="#存储级别" class="headerlink" title="存储级别"></a>存储级别</h2><blockquote>
<p>Storage Level - <strong>枚举</strong>类型，用来记录 RDD <strong>持久化</strong>时的存储级别</p>
</blockquote>
<table>
<thead>
<tr>
<th>Storage Level</th>
<th>Desc</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MEMORY_ONLY - 默认值</strong></td>
<td>只缓存在内存中，如果内存不足则不缓存多出来的部分</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>缓存在内存中，如果空间不够则缓存在硬盘中</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>只缓存在硬盘中</td>
</tr>
<tr>
<td>MEMORY_ONLY_2 &#x2F; MEMORY_AND_DISK_2</td>
<td>同上，<strong>每个分区</strong>在集群中的<strong>两个节点</strong>上建立<strong>副本</strong></td>
</tr>
</tbody></table>
<blockquote>
<p>相对于 Hadoop，随时可以将<strong>计算好的 RDD</strong> 缓存在<strong>内存</strong>中，以便于下次计算时使用</p>
</blockquote>
<h2 id="迭代-计算"><a href="#迭代-计算" class="headerlink" title="迭代 + 计算"></a>迭代 + 计算</h2><blockquote>
<p>Iterator + Compute - 表示 RDD 怎样通过<strong>父 RDD</strong> 计算得到的</p>
</blockquote>
<ol>
<li><strong>迭代函数</strong>会首先判断<strong>缓存</strong>中是否有想要计算的 RDD，如果有则<strong>直接读取</strong></li>
<li>如果没有，则检查想要计算的 RDD 是否被<strong>检查点</strong>处理过，如果有则<strong>直接读取</strong></li>
<li>如果没有，就<strong>调用计算函数向上递归</strong>，查找父 RDD 进行计算</li>
</ol>
<blockquote>
<p><strong>缓存 -&gt; 检查点</strong></p>
</blockquote>
<h1 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h1><h2 id="转换-Transformation"><a href="#转换-Transformation" class="headerlink" title="转换 - Transformation"></a>转换 - Transformation</h2><ol>
<li>MapReduce 只支持 Map 和 Reduce 操作，而 Spark 支持大量的基本操作</li>
<li>转换 - 将一个 RDD 转换为另一个 RDD</li>
</ol>
<h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><ol>
<li>将一个 RDD 中的所有数据通过一个函数，映射成一个新的 RDD</li>
<li>任何原 RDD 中的元素在新 RDD 中都<strong>有且只有一个</strong>元素与之对应</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="string">&quot;b&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;c&quot;</span>])</span><br><span class="line">rdd2 = rdd.map(lambda x: (x, <span class="number">1</span>)) <span class="comment">// [(&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1), (&#x27;c&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>

<h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><blockquote>
<p>选择原 RDD 里所有数据中<strong>满足特定条件</strong>的数据，返回一个新的 RDD</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">rdd2 = rdd.filter(lambda x: x % <span class="number">2</span> == <span class="number">0</span>) <span class="comment">// [2, 4]</span></span><br></pre></td></tr></table></figure>

<h3 id="MapPartitions"><a href="#MapPartitions" class="headerlink" title="MapPartitions"></a>MapPartitions</h3><ol>
<li>MapPartitions 是 Map 的变种</li>
<li>Map 的输入函数应用于 <strong>RDD 中的每个元素</strong></li>
<li>MapPartitions 的输入函数应用于 <strong>RDD 中的每个分区</strong>，将<strong>每个分区中的内容</strong>作为<strong>整体</strong>来处理</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], <span class="number">2</span>) <span class="comment">// 创建一个有两个分区的 RDD</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span></span>(iterator): <span class="keyword">yield</span> sum(iterator) <span class="comment">// 对分区内的元素求和，1+2=3 / 3+4=7</span></span><br><span class="line">rdd2 = rdd.mapPartitions(f) <span class="comment">// [3, 7]</span></span><br></pre></td></tr></table></figure>

<h3 id="GroupByKey"><a href="#GroupByKey" class="headerlink" title="GroupByKey"></a>GroupByKey</h3><ol>
<li>与 SQL 中的 groupBy 类似，将对象的集合<strong>按照某个 Key 来归类</strong></li>
<li>返回的 RDD 中的<strong>每个 Key</strong> 对应<strong>一个序列</strong></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)])</span><br><span class="line">rdd.groupByKey().collect()</span><br><span class="line"><span class="comment">//&quot;a&quot; [1, 2]</span></span><br><span class="line"><span class="comment">//&quot;b&quot; [1]</span></span><br></pre></td></tr></table></figure>

<h2 id="动作-Action"><a href="#动作-Action" class="headerlink" title="动作 - Action"></a>动作 - Action</h2><blockquote>
<p>动作 - 通过计算返回一个结果</p>
</blockquote>
<h3 id="Collect"><a href="#Collect" class="headerlink" title="Collect"></a>Collect</h3><ol>
<li>Collect 与<strong>函数式编程</strong>中的 Collect 类似，以<strong>数组</strong>的形式，返回 RDD 的<strong>所有元素</strong></li>
<li>Collect 操作只有在<strong>输出数组较小</strong>时使用<ul>
<li>因为所有的数据都会载入到程序的<strong>内存</strong>中，如果<strong>输出数组很大</strong>，则会<strong>占用大量 JVM 内存</strong>，导致<strong>内存溢出</strong></li>
</ul>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="string">&quot;b&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;c&quot;</span>])</span><br><span class="line">rdd.map(lambda x: (x, <span class="number">1</span>)).collect() <span class="comment">// [(&#x27;b&#x27;, 1), (&#x27;a&#x27;, 1), (&#x27;c&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>

<h3 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h3><blockquote>
<p>与 MapReduce 中的 Reduce 类似，将 RDD 中的元素根据一个<strong>输入函数</strong>聚合起来</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from operator <span class="keyword">import</span> add</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).reduce(add)  <span class="comment">// 15</span></span><br></pre></td></tr></table></figure>

<h3 id="Count"><a href="#Count" class="headerlink" title="Count"></a>Count</h3><blockquote>
<p>返回 RDD 中元素的个数</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).count() <span class="comment">// 3</span></span><br></pre></td></tr></table></figure>

<h3 id="CountByKey"><a href="#CountByKey" class="headerlink" title="CountByKey"></a>CountByKey</h3><blockquote>
<p>仅适用于 <strong>Key-Value Pair</strong> 类型的 RDD，返回具有每个 Key 的计数的 Key-Count 字典</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line">sorted(rdd.countByKey().items()) <span class="comment">// [(&#x27;a&#x27;, 2), (&#x27;b&#x27;, 1)]</span></span><br></pre></td></tr></table></figure>

<h2 id="惰性求值"><a href="#惰性求值" class="headerlink" title="惰性求值"></a>惰性求值</h2><ol>
<li>所有的<strong>转换</strong>操作都<strong>很懒</strong>，只是<strong>生成新的 RDD</strong>，并且<strong>记录依赖关系</strong></li>
<li>Spark 并<strong>不会立刻计算</strong>出新 RDD 中各个分区的数值<ul>
<li>直到<strong>遇到一个动作</strong>时，<strong>数据才会被计算</strong>，并输出结果给 <strong>Driver</strong></li>
</ul>
</li>
<li>惰性求值的设计可以让 Spark 的运算更加<strong>高效</strong>和<strong>快速</strong></li>
</ol>
<h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><ol>
<li>Spark 在每次<strong>转换</strong>操作时，使用了<strong>新产生的 RDD</strong> 来<strong>记录计算逻辑</strong><ul>
<li>把作用在 RDD 上的所有计算逻辑<strong>串联</strong>起来，形成一个<strong>链条</strong></li>
</ul>
</li>
<li>当 RDD 进行<strong>动作</strong>操作时<ul>
<li>Spark 会从<strong>计算链</strong>的<strong>最后一个 RDD</strong> 开始，依次从<strong>上一个 RDD</strong> 获取数据并执行计算逻辑，最后输出结果</li>
</ul>
</li>
</ol>
<h1 id="持久化-缓存"><a href="#持久化-缓存" class="headerlink" title="持久化 - 缓存"></a>持久化 - 缓存</h1><blockquote>
<p>类似于 Guava LoadingCache</p>
</blockquote>
<ol>
<li>每当对 RDD 调用一个新的<strong>动作</strong>操作时，<strong>整个 RDD</strong> 都会<strong>从头开始运算</strong></li>
<li>如果某个 RDD 会被<strong>反复重用</strong>的话，每次都重头计算<strong>非常低效</strong> – 进行<strong>持久化</strong>操作</li>
<li>Spark 的 <strong>persist()</strong> 和 <strong>cache()</strong> 方法支持将 <strong>RDD 的数据</strong>缓存值<strong>内存</strong>或<strong>硬盘</strong>中<ul>
<li>下次对<strong>同一 RDD</strong> 进行<strong>动作</strong>操作时，可以直接读取 RDD 的结果，大幅提高 Spark 的<strong>计算效率</strong></li>
</ul>
</li>
<li>缓存 RDD 时，其<strong>所有的依赖关系</strong>也会被一并保存<ul>
<li>持久化的 RDD 有<strong>自动的容错机制</strong></li>
<li>如果 RDD 的<strong>任一分区丢失</strong>了，通过使用原先创建它的<strong>转换</strong>操作，会被<strong>自动重算</strong></li>
</ul>
</li>
<li>持久化可以选择不同的<strong>存储级别</strong>，而 <strong>cache()</strong> 方法的默认值为 <strong>MEMORY_ONLY</strong></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">rdd1 = rdd.map(lambda x: x+<span class="number">5</span>)</span><br><span class="line">rdd2 = rdd1.filter(lambda x: x % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">rdd2.persist() <span class="comment">// 等待第一次动作操作，将结果缓存在内存中</span></span><br><span class="line">count = rdd2.count() <span class="comment">// 3，第一次动作操作，从头开始计算，将结果存储下来</span></span><br><span class="line">first = rdd2.first() <span class="comment">// 6，不是第一次动作操作，无需从头开始计算，复用缓存结果</span></span><br><span class="line">rdd2.unpersist()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>持久化 vs Checkpoint</p>
</blockquote>
<ol>
<li>Checkpoint 是在 <strong>Action</strong> 后执行的，相当于<strong>事务完成</strong>后<strong>备份结果</strong><ul>
<li>既然结果有了，之前的<strong>计算过程</strong>，即 <strong>RDD 的依赖链</strong>，也不需要了，<strong>不必保存</strong></li>
</ul>
</li>
<li>持久化（persist or cache）只是保存<strong>当前 RDD</strong>，并不要求在 <strong>Action</strong> 后调用<ul>
<li>相当于<strong>事务的计算过程</strong>，还没有结果</li>
<li>既然没有结果，当需要恢复、重新计算时就需要<strong>重放计算过程</strong>，自然就不能放弃之前的<strong>依赖链</strong>，需要<strong>保存</strong></li>
<li>需要恢复时，从<strong>最初</strong>或者<strong>最近的 Checkpoint</strong> 开始<strong>重新计算</strong></li>
</ul>
</li>
</ol>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top">zhongmingmao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top/2024/09/18/bigdata-spark-rdd/">https://blog.zhongmingmao.top/2024/09/18/bigdata-spark-rdd/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/big-data/">Big Data</a><a class="post-meta__tags" href="/tags/spark/">Spark</a><a class="post-meta__tags" href="/tags/rdd/">RDD</a></div><div class="post-share"><div class="social-share" data-image="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-rdd.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/09/19/bigdata-spark-sql/" title="Spark - SQL"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-sql.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Spark - SQL</div></div><div class="info-2"><div class="info-item-1">历史Hive 一开始，Hadoop&#x2F;MapReduce 在企业生产中大量使用，在 HDFS 上积累了大量数据 MapReduce 对于开发者而言使用难度较大，大部分开发人员最熟悉的还是传统的关系型数据库 为了方便大多数开发人员使用 Hadoop，诞生了 Hive Hive 提供类似 SQL 的编程接口，HQL 经过语法解析、逻辑计划、物理计划转化成 MapReduce 程序执行 使得开发人员很容易对 HDFS 上存储的数据进行查询和分析      Shark 在 Spark 刚问世时，Spark 团队开发了 Shark 来支持用 SQL 来查询 Spark 的数据 Shark 的本质是 Hive，Shark 修改了 Hive 的内存管理模块，大幅优化了运行速度 Shark 依赖于 Hive，严重影响了 Spark 的发展，Spark 要定义一个统一的技术栈和完整的生态 依赖于 Hive 还会制约 Spark 各个组件的相互集成，Spark 无法利用 Spark 的特性进行深度优化 2014 年 7 月 1 日，Spark 团队将 Shark 交给 Hive 进行管理，即 Hive on Spa...</div></div></div></a><a class="pagination-related" href="/2024/09/17/bigdata-spark-overview/" title="Spark - Overview"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Spark - Overview</div></div><div class="info-2"><div class="info-item-1">MapReduce概述 MapReduce 通过简单的 Map 和 Reduce 的抽象提供了一个编程模型 可以在一个由上百台机器组成的集群上并发处理大量的数据集，而把计算细节隐藏起来   各种各样的复杂数据处理都可以分解为 Map 和 Reduce 的基本元素 复杂的数据处理可以分解成由多个 Job（包含一个 Mapper 和一个 Reducer）组成的 DAG 然后，将每个 Mapper 和 Reducer 放到 Hadoop 集群上执行，得到最终结果      不足 高昂的维护成本 时间性能不达标 MapReduce 模型的抽象层次低 大量的底层逻辑需要开发者手工完成 - 用汇编语言开发游戏   只提供 Map 和 Reduce 操作 很多现实的数据处理场景并不适合用这个模型来描述 实现复杂的操作需要技巧，让整个工程变得庞大且难以维护 维护一个多任务协调的状态机成本很高，且扩展性很差   在 Hadoop 中，每个 Job 的计算结果都会存储在 HDFS 文件存储系统中 每一步计算都要进行硬盘的读取和写入，大大增加了系统的延迟   MapReduce 对于迭代算法的处理性能很差，而且非常耗资源 因...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/09/22/bigdata-spark-flink/" title="Big Data - Spark + Flink"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-flink.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-22</div><div class="info-item-2">Big Data - Spark + Flink</div></div><div class="info-2"><div class="info-item-1">Spark 实时性 无论是 Spark Streaming 还是 Structured Streaming，Spark 流处理的实时性还不够 无法应对实时性要求很高的流处理场景   Spark 的流处理是基于微批处理的思想 把流处理看做批处理的一种特殊形式，没接收到一个时间间隔的数据才会去处理   虽然在 Spark 2.3 中提出连续处理模型，但只支持有限的功能，并不能在大项目中使用 要在流处理的实时性提升，就不能继续用微批处理的模式，而是有数据数据就立即处理，不做等待 Apache Flink 采用了基于操作符（Operator）的连续流模型，可以做到微秒级别的延迟      Flink模型 Flink 中最核心的数据结构是 Stream，代表一个运行在多个分区上的并行流 在 Stream 上可以进行各种转换（Transformation）操作 与 Spark RDD 不同的是，Stream 代表一个数据流而不是静态数据的集合 Stream 所包含的数据随着时间增长而变化的 而且 Stream 上的转换操作都是逐条进行的 - 每当有新数据进入，整个流程都会被执行并更新结果 Flink 比 Spark...</div></div></div></a><a class="pagination-related" href="/2024/09/21/bigdata-spark-structured-streaming/" title="Spark - Structured Streaming"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-structured-streaming.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-21</div><div class="info-item-2">Spark - Structured Streaming</div></div><div class="info-2"><div class="info-item-1">背景 Spark Streaming 将无边界的流数据抽象成 DStream 按特定的时间间隔，把数据流分割成一个个 RDD 进行批处理 DStream API 与 RDD API 高度相似，拥有 RDD 的各种性质   DataSet&#x2F;DataFrame DataSet&#x2F;DataFrame 是高级 API，提供类似于 SQL 的查询接口，方便熟悉关系型数据库的开发人员使用 Spark SQL 执行引擎会自动优化 DataSet&#x2F;DataFrame 程序 用 RDD API 开发的程序本质上需要开发人员手工构造 RDD 的 DAG 执行图，依赖于手工优化     如果拥有 DataSet&#x2F;DataFrame API 的流处理模块 无需去用相对底层的 DStream API 去处理无边界数据，大大提升开发效率   在 2016 年，Spark 2.0 中推出结构化流处理的模块 - Structured Streaming Structured Streaming 基于 Spark SQL 引擎实现 在开发视角，流数据和静态数据没有区别，可以像批处理静态数据那样处理流...</div></div></div></a><a class="pagination-related" href="/2024/09/20/bigdata-spark-streaming/" title="Spark - Streaming"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-streaming.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-20</div><div class="info-item-2">Spark - Streaming</div></div><div class="info-2"><div class="info-item-1">流处理 Spark SQL 中的 DataFrame API 和 DataSet API 都是基于批处理模式对静态数据进行处理 在 2013，Spark 的流处理组件 Spark Streaming 发布，现在的 Spark Streaming 已经非常成熟，应用非常广泛    原理 Spark Streaming 的原理与微积分的思想很类似 微分是无限细分，而积分是对无限细分的每一段进行求和 本质 - 将一个连续的问题转换成了无限个离散的问题   流处理的数据是一系列连续不断变化，且无边界的，永远无法预测下一秒的数据 Spark Streaming 用时间片拆分了无限的数据流 然后对每个数据片用类似于批处理的方法进行处理，输出的数据也是分块的      Spark Streaming 提供一个对于流数据的抽象 DStream DStream 可以由 Kafka、Flume 或者 HDFS 的流数据生成，也可以由别的 DStream 经过各种转换操作得到 底层 DStream 由多个序列化的 RDD 构成，按时间片（如一秒）切分成的每个数据单位都是一个 RDD Spark 核心引擎将对 DStream ...</div></div></div></a><a class="pagination-related" href="/2024/09/19/bigdata-spark-sql/" title="Spark - SQL"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-sql.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-19</div><div class="info-item-2">Spark - SQL</div></div><div class="info-2"><div class="info-item-1">历史Hive 一开始，Hadoop&#x2F;MapReduce 在企业生产中大量使用，在 HDFS 上积累了大量数据 MapReduce 对于开发者而言使用难度较大，大部分开发人员最熟悉的还是传统的关系型数据库 为了方便大多数开发人员使用 Hadoop，诞生了 Hive Hive 提供类似 SQL 的编程接口，HQL 经过语法解析、逻辑计划、物理计划转化成 MapReduce 程序执行 使得开发人员很容易对 HDFS 上存储的数据进行查询和分析      Shark 在 Spark 刚问世时，Spark 团队开发了 Shark 来支持用 SQL 来查询 Spark 的数据 Shark 的本质是 Hive，Shark 修改了 Hive 的内存管理模块，大幅优化了运行速度 Shark 依赖于 Hive，严重影响了 Spark 的发展，Spark 要定义一个统一的技术栈和完整的生态 依赖于 Hive 还会制约 Spark 各个组件的相互集成，Spark 无法利用 Spark 的特性进行深度优化 2014 年 7 月 1 日，Spark 团队将 Shark 交给 Hive 进行管理，即 Hive on Spa...</div></div></div></a><a class="pagination-related" href="/2024/09/17/bigdata-spark-overview/" title="Spark - Overview"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-17</div><div class="info-item-2">Spark - Overview</div></div><div class="info-2"><div class="info-item-1">MapReduce概述 MapReduce 通过简单的 Map 和 Reduce 的抽象提供了一个编程模型 可以在一个由上百台机器组成的集群上并发处理大量的数据集，而把计算细节隐藏起来   各种各样的复杂数据处理都可以分解为 Map 和 Reduce 的基本元素 复杂的数据处理可以分解成由多个 Job（包含一个 Mapper 和一个 Reducer）组成的 DAG 然后，将每个 Mapper 和 Reducer 放到 Hadoop 集群上执行，得到最终结果      不足 高昂的维护成本 时间性能不达标 MapReduce 模型的抽象层次低 大量的底层逻辑需要开发者手工完成 - 用汇编语言开发游戏   只提供 Map 和 Reduce 操作 很多现实的数据处理场景并不适合用这个模型来描述 实现复杂的操作需要技巧，让整个工程变得庞大且难以维护 维护一个多任务协调的状态机成本很高，且扩展性很差   在 Hadoop 中，每个 Job 的计算结果都会存储在 HDFS 文件存储系统中 每一步计算都要进行硬盘的读取和写入，大大增加了系统的延迟   MapReduce 对于迭代算法的处理性能很差，而且非常耗资源 因...</div></div></div></a><a class="pagination-related" href="/2024/10/05/bigdata-beam-future/" title="Beam - Future"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/flink-runner-beam-beam-vision.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-05</div><div class="info-item-2">Beam - Future</div></div><div class="info-2"><div class="info-item-1">技术迭代 2006，Apache Hadoop 发布，基于 MapReduce 计算模型 2009，Spark 计算框架在 加州伯克利大学诞生，于 2010 年开源，于 2014 年成为 Apache 的顶级项目 Spark 的数据处理效率远在 Hadoop 之上   2014，Flink 面世，流批一体，于 2018 年被阿里收购    Apache Beam Apache Beam 根据 Dataflow Model API 实现的，能完全胜任批流一体的任务 Apache Beam 有中间的抽象转换层，工程师无需学习新 Runner 的 API 的语法，减少学习新技术的时间成本 Runner 可以专心优化效率和迭代功能，而不必担心迁移  Beam Runner 迭代非常快 - 如 Flink  </div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zhongmingmao</div><div class="author-info-description">Focus on Infrastructure.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">640</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhongmingmao0625@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Things are always unexpected!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%86%85%E5%AD%98"><span class="toc-number">1.</span> <span class="toc-text">分布式内存</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">2.</span> <span class="toc-text">定义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8C%BA"><span class="toc-number">2.1.</span> <span class="toc-text">分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%8F%AF%E5%8F%98"><span class="toc-number">2.2.</span> <span class="toc-text">不可变</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="toc-number">2.3.</span> <span class="toc-text">并行计算</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%84"><span class="toc-number">3.</span> <span class="toc-text">结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">3.1.</span> <span class="toc-text">依赖关系</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="toc-number">3.1.1.</span> <span class="toc-text">窄依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%BD%E4%BE%9D%E8%B5%96"><span class="toc-number">3.1.2.</span> <span class="toc-text">宽依赖</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="toc-number">3.2.</span> <span class="toc-text">检查点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB"><span class="toc-number">3.3.</span> <span class="toc-text">存储级别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3-%E8%AE%A1%E7%AE%97"><span class="toc-number">3.4.</span> <span class="toc-text">迭代 + 计算</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C"><span class="toc-number">4.</span> <span class="toc-text">操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2-Transformation"><span class="toc-number">4.1.</span> <span class="toc-text">转换 - Transformation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Map"><span class="toc-number">4.1.1.</span> <span class="toc-text">Map</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Filter"><span class="toc-number">4.1.2.</span> <span class="toc-text">Filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapPartitions"><span class="toc-number">4.1.3.</span> <span class="toc-text">MapPartitions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GroupByKey"><span class="toc-number">4.1.4.</span> <span class="toc-text">GroupByKey</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C-Action"><span class="toc-number">4.2.</span> <span class="toc-text">动作 - Action</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Collect"><span class="toc-number">4.2.1.</span> <span class="toc-text">Collect</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reduce"><span class="toc-number">4.2.2.</span> <span class="toc-text">Reduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Count"><span class="toc-number">4.2.3.</span> <span class="toc-text">Count</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CountByKey"><span class="toc-number">4.2.4.</span> <span class="toc-text">CountByKey</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%83%B0%E6%80%A7%E6%B1%82%E5%80%BC"><span class="toc-number">4.3.</span> <span class="toc-text">惰性求值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">4.4.</span> <span class="toc-text">执行流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96-%E7%BC%93%E5%AD%98"><span class="toc-number">5.</span> <span class="toc-text">持久化 - 缓存</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Introduction"/></a><div class="content"><a class="title" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction">Observability - Prometheus Introduction</a><time datetime="2025-01-21T16:06:25.000Z" title="Created 2025-01-22 00:06:25">2025-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java-agent.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java Zero Code"/></a><div class="content"><a class="title" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code">Observability - OpenTelemetry Java Zero Code</a><time datetime="2025-01-20T16:06:25.000Z" title="Created 2025-01-21 00:06:25">2025-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java"/></a><div class="content"><a class="title" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java">Observability - OpenTelemetry Java</a><time datetime="2025-01-19T16:06:25.000Z" title="Created 2025-01-20 00:06:25">2025-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/mcp.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - MCP Overview"/></a><div class="content"><a class="title" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview">AI Agent - MCP Overview</a><time datetime="2025-01-18T16:06:25.000Z" title="Created 2025-01-19 00:06:25">2025-01-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/18/ai-agent-overview/" title="AI Agent - Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/ai-agent.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - Overview"/></a><div class="content"><a class="title" href="/2025/01/18/ai-agent-overview/" title="AI Agent - Overview">AI Agent - Overview</a><time datetime="2025-01-17T16:06:25.000Z" title="Created 2025-01-18 00:06:25">2025-01-18</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2015 - 2025 By zhongmingmao</span></div><div class="footer_custom_text">Life is like a box of chocolates. You can't know what you'll eat until you open it.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div></body></html>