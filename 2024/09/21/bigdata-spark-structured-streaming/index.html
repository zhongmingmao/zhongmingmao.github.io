<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark - Structured Streaming | ByteCoding</title><meta name="author" content="zhongmingmao"><meta name="copyright" content="zhongmingmao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="背景 Spark Streaming 将无边界的流数据抽象成 DStream 按特定的时间间隔，把数据流分割成一个个 RDD 进行批处理 DStream API 与 RDD API 高度相似，拥有 RDD 的各种性质   DataSet&#x2F;DataFrame DataSet&#x2F;DataFrame 是高级 API，提供类似于 SQL 的查询接口，方便熟悉关系型数据库的开发人员使用 S">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark - Structured Streaming">
<meta property="og:url" content="https://blog.zhongmingmao.top/2024/09/21/bigdata-spark-structured-streaming/index.html">
<meta property="og:site_name" content="ByteCoding">
<meta property="og:description" content="背景 Spark Streaming 将无边界的流数据抽象成 DStream 按特定的时间间隔，把数据流分割成一个个 RDD 进行批处理 DStream API 与 RDD API 高度相似，拥有 RDD 的各种性质   DataSet&#x2F;DataFrame DataSet&#x2F;DataFrame 是高级 API，提供类似于 SQL 的查询接口，方便熟悉关系型数据库的开发人员使用 S">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-structured-streaming.jpg">
<meta property="article:published_time" content="2024-09-20T16:06:25.000Z">
<meta property="article:modified_time" content="2024-11-18T11:44:35.080Z">
<meta property="article:author" content="zhongmingmao">
<meta property="article:tag" content="Big Data">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-structured-streaming.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Spark - Structured Streaming",
  "url": "https://blog.zhongmingmao.top/2024/09/21/bigdata-spark-structured-streaming/",
  "image": "https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-structured-streaming.jpg",
  "datePublished": "2024-09-20T16:06:25.000Z",
  "dateModified": "2024-11-18T11:44:35.080Z",
  "author": [
    {
      "@type": "Person",
      "name": "zhongmingmao",
      "url": "https://blog.zhongmingmao.top"
    }
  ]
}</script><link rel="shortcut icon" href="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png"><link rel="canonical" href="https://blog.zhongmingmao.top/2024/09/21/bigdata-spark-structured-streaming/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":32,"languages":{"author":"Author: zhongmingmao","link":"Link: ","source":"Source: ByteCoding","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark - Structured Streaming',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ByteCoding" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/url(https:/cdn.pixabay.com/photo/2021/07/20/03/39/fisherman-6479663_1280.jpg));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">641</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-structured-streaming.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ByteCoding</span></a><a class="nav-page-title" href="/"><span class="site-name">Spark - Structured Streaming</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Spark - Structured Streaming</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2024-09-20T16:06:25.000Z" title="Created 2024-09-21 00:06:25">2024-09-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/">Big Data</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/big-data/spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">1.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>5mins</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:512,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2024-11-18 19:44:35&quot;}" hidden></div><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><ol>
<li>Spark Streaming 将<strong>无边界的流数据</strong>抽象成 <strong>DStream</strong><ul>
<li>按特定的时间间隔，把<strong>数据流</strong>分割成一个个 <strong>RDD</strong> 进行<strong>批处理</strong></li>
<li><strong>DStream</strong> API 与 <strong>RDD</strong> API <strong>高度相似</strong>，拥有 RDD 的各种性质</li>
</ul>
</li>
<li>DataSet&#x2F;DataFrame<ul>
<li>DataSet&#x2F;DataFrame 是<strong>高级 API</strong>，提供类似于 <strong>SQL</strong> 的查询接口，方便熟悉<strong>关系型数据库</strong>的开发人员使用</li>
<li><strong>Spark SQL 执行引擎</strong>会<strong>自动优化 DataSet&#x2F;DataFrame 程序</strong><ul>
<li>用 <strong>RDD API</strong> 开发的程序本质上需要开发人员<strong>手工构造 RDD 的 DAG 执行图</strong>，依赖于<strong>手工优化</strong></li>
</ul>
</li>
</ul>
</li>
<li>如果拥有 <strong>DataSet&#x2F;DataFrame API</strong> 的<strong>流处理</strong>模块<ul>
<li>无需去用<strong>相对底层</strong>的 <strong>DStream API</strong> 去处理<strong>无边界数据</strong>，大大提升<strong>开发效率</strong></li>
</ul>
</li>
<li>在 <strong>2016</strong> 年，<strong>Spark 2.0</strong> 中推出<strong>结构化流处理</strong>的模块 - <strong>Structured Streaming</strong><ul>
<li>Structured Streaming 基于 <strong>Spark SQL</strong> 引擎实现</li>
<li>在开发视角，<strong>流数据</strong>和<strong>静态数据</strong>没有区别，可以像<strong>批处理静态数据</strong>那样处理<strong>流数据</strong></li>
<li>随着<strong>流数据</strong>的<strong>持续输入</strong>，Spark SQL 引擎会<strong>持续地处理</strong>新数据，并更新计算结果</li>
</ul>
</li>
</ol>
<span id="more"></span>

<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><ol>
<li>流数据处理最基本的问题是<strong>如何对不断更新的无边界数据建模</strong></li>
<li><strong>Spark Streaming</strong><ul>
<li>把<strong>流数据</strong>按一定的<strong>时间间隔</strong>分割成许多个小的<strong>数据块</strong>进行<strong>批处理</strong></li>
</ul>
</li>
<li><strong>Structured Streaming</strong><ul>
<li>把数据看成一个<strong>无边界</strong>的<strong>关系型数据库表</strong></li>
<li>每一个<strong>数据</strong>都是表中的<strong>一行</strong>，不断会有<strong>新的数据行</strong>被添加到表中</li>
<li>可以对该表做任何<strong>类似批处理</strong>的<strong>查询</strong>，Spark 会<strong>不断</strong>对新加入的数据进行<strong>处理</strong>，并更新计算结果</li>
</ul>
</li>
</ol>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/image-20241118184008593.png" alt="image-20241118184008593"></p>
<ol>
<li>与 Spark Streaming 类似，<strong>Structured Streaming</strong> 也是将输入的数据按照<strong>时间间隔</strong>（例如 1 秒）划分为<strong>数据段</strong></li>
<li>每一秒都会把新输入的数据<strong>添加到表</strong>中，Spark 也会每秒<strong>更新输出结果</strong><ul>
<li>输出结果也是<strong>表</strong>的形式，输出表可以写入到<strong>硬盘</strong>或者 <strong>HDFS</strong></li>
</ul>
</li>
</ol>
<blockquote>
<p>Structured Streaming 的三种<strong>输出模式</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Desc</th>
</tr>
</thead>
<tbody><tr>
<td>完全模式 - <strong>Complete</strong> Mode</td>
<td><strong>整个</strong>更新过的输出表被写入外部存储</td>
</tr>
<tr>
<td>附加模式 - <strong>Append</strong> Mode</td>
<td>上次触发后<strong>新增的行</strong>才会被写入到外部存储<br />如果<strong>老数据有改动</strong>则不适合该模式</td>
</tr>
<tr>
<td>更新模式 - <strong>Update</strong> Mode</td>
<td>上次触发后<strong>被更新的行</strong>才会被写入外部存储</td>
</tr>
</tbody></table>
<blockquote>
<p>Structured Streaming 并<strong>不会完全存储输入数据</strong></p>
</blockquote>
<ol>
<li><strong>每个时间间隔</strong>，Structured Streaming 都会<strong>读取最新的输入</strong>，进行处理并更新输出表，然后<strong>删除这次输入</strong></li>
<li>Structured Streaming 只会存储更新<strong>输出表所需要</strong>的信息</li>
</ol>
<blockquote>
<p>Structured Streaming 模型根据事件时间（<strong>Event Time</strong>）处理数据时十分方便</p>
</blockquote>
<ol>
<li>事件时间指的是<strong>事件发生</strong>的时间，是数据<strong>本身</strong>的属性；处理时间是 Spark <strong>接收数据</strong>的时间</li>
<li>在 Structured Streaming 模型中，每个<strong>数据</strong>是输入数据表中的<strong>一行</strong>，那么<strong>事件时间</strong>就是行中的<strong>一列</strong></li>
<li>依靠 <strong>DataSet&#x2F;DataFrame API</strong> 提供的类似于 <strong>SQL</strong> 的接口 - 很方便地执行<strong>基于时间窗口</strong>的<strong>查询</strong></li>
</ol>
<h1 id="Streaming-DataFrame-API"><a href="#Streaming-DataFrame-API" class="headerlink" title="Streaming DataFrame API"></a>Streaming DataFrame API</h1><blockquote>
<p>在 <strong>Structured Streaming</strong> 发布后，<strong>DataFrame</strong> 即可以代表<strong>静态的有边界数据</strong>，也可以代表<strong>无边界数据</strong></p>
</blockquote>
<h2 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">socketDataFrame = spark</span><br><span class="line">   .readStream</span><br><span class="line">   .format(<span class="string">&quot;socket&quot;</span>）</span><br><span class="line">   .option(<span class="string">&quot;host&quot;</span>, <span class="string">&quot;localhost&quot;</span>)</span><br><span class="line">   .option(<span class="string">&quot;port&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">   .load()</span><br></pre></td></tr></table></figure>

<ol>
<li><code>SparkSession.readStream()</code> 返回的 <strong>DataStreamReader</strong> 可以用于创建 <strong>Streaming DataFrame</strong></li>
<li>支持多种类型的数据流作为输入，如 <strong>File</strong>、<strong>Kafka</strong>、<strong>Socket</strong> 等</li>
</ol>
<h2 id="查询操作"><a href="#查询操作" class="headerlink" title="查询操作"></a>查询操作</h2><blockquote>
<p><strong>Streaming DataFrame</strong> 与 <strong>Static DataFrame</strong> 都支持 <strong>SQL</strong> 查询（select、where），也支持 <strong>RDD</strong> 转换操作</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = … <span class="comment">// 这个DataFrame代表学校学生的数据流，schema是&#123;name: string, age: number, height: number, grade: string&#125;</span></span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>).where(<span class="string">&quot;age &gt; 10&quot;</span>) <span class="comment">// 返回年龄大于10岁的学生名字列表</span></span><br><span class="line">df.groupBy(<span class="string">&quot;grade&quot;</span>).count() <span class="comment">// 返回每个年级学生的人数</span></span><br><span class="line">df.sort_values([‘age’], ascending=<span class="type">False</span>).head(<span class="number">100</span>) <span class="comment">// 返回100个年龄最大的学生 </span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>通过 <strong>isStreaming</strong> 函数判断一个 DataFrame 是否代表流数据</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.isStreaming()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>基于<strong>事件时间</strong>的时间窗口操作 - 在 <strong>Spark Streaming</strong> 中的热词统计是基于<strong>处理时间</strong></p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">words = ...  # 这个<span class="type">DataFrame</span>代表词语的数据流，schema是 &#123; timestamp: <span class="type">Timestamp</span>, word: <span class="type">String</span>&#125;</span><br><span class="line"></span><br><span class="line">windowedCounts = words.groupBy(</span><br><span class="line">   window(words.timestamp, <span class="string">&quot;1 minute&quot;</span>, <span class="string">&quot;10 seconds&quot;</span>),</span><br><span class="line">   words.word</span><br><span class="line">).count()</span><br><span class="line">.sort(desc(<span class="string">&quot;count&quot;</span>))</span><br><span class="line">.limit(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li>基于<strong>词语的生成时间</strong>（而非 Spark 的处理时间），创建一个窗口长度为 1 分钟，滑动间隔为 10 秒的窗口</li>
<li>把输入的词语表根据窗口和词语本身聚合起来，并统计每个窗口内词语的数量，在根据词语数量倒排 Top 10</li>
</ol>
<h2 id="输出结果流"><a href="#输出结果流" class="headerlink" title="输出结果流"></a>输出结果流</h2><ol>
<li>当经过各种 <strong>SQL</strong> 查询操作后，创建好<strong>代表最终结果</strong>的 <strong>DataFrame</strong></li>
<li>下一步开始对<strong>输入数据流</strong>的处理，并<strong>持续输出结果</strong></li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">query = wordCounts</span><br><span class="line">   .writeStream</span><br><span class="line">   .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">   .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">   .option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/destination/dir&quot;</span>)</span><br><span class="line">   .start()</span><br><span class="line"></span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>

<ol>
<li>通过 <code>Dataset.writeStream()</code> 返回的 <strong>DataStreamWriter</strong> 对象去输出结果</li>
<li>支持多种写入位置，如 File、Kafka、Console、内存等</li>
</ol>
<h1 id="Structured-Streaming-vs-Spark-Streaming"><a href="#Structured-Streaming-vs-Spark-Streaming" class="headerlink" title="Structured Streaming vs Spark Streaming"></a>Structured Streaming vs Spark Streaming</h1><blockquote>
<p>综合来说，<strong>Structured Streaming</strong> 是比 Spark Streaming 更好的流处理工具</p>
</blockquote>
<h2 id="易用性-性能"><a href="#易用性-性能" class="headerlink" title="易用性 + 性能"></a>易用性 + 性能</h2><ol>
<li><p><strong>Spark Streaming</strong> 提供的 <strong>DStream API</strong> 与 <strong>RDD API</strong> 非常类似，<strong>相对底层</strong></p>
<ul>
<li><p>编写 <strong>Spark Streaming</strong> 程序时，本质上是去<strong>构造 RDD 的 DAG 执行图</strong>，然后通过 <strong>Spark Engine</strong> 运行</p>
</li>
<li><p>开发者<strong>心智负担</strong>比较重，需要想办法去<strong>提高程序的处理效率</strong></p>
</li>
<li><p>对于一个<strong>好的框架</strong>来说，开发者只需要<strong>专注于业务逻辑</strong>上，无需担心配置和优化等<strong>繁杂</strong>事项</p>
</li>
</ul>
</li>
<li><p><strong>Structured Streaming</strong> 提供的 <strong>DataFrame API</strong> 是一个<strong>相对高级</strong>的 API</p>
<ul>
<li><strong>统一的数据抽象</strong>可以用一套<strong>统一的方案</strong>去处理<strong>批处理</strong>和<strong>流处理</strong>，而无需关心具体的<strong>执行细节</strong></li>
<li>而且 <strong>DataFrame API</strong> 是在 <strong>Spark SQL 执行引擎</strong>上执行 ，有非常多的<strong>优化功能</strong> - 所以<strong>性能更佳</strong></li>
</ul>
</li>
</ol>
<h2 id="实时性"><a href="#实时性" class="headerlink" title="实时性"></a>实时性</h2><ol>
<li><strong>Spark Streaming</strong> 是<strong>准实时</strong>的，可以做到的<strong>最小延迟</strong>在 <strong>1 秒</strong>左右</li>
<li>虽然 <strong>Structured Streaming</strong> 也是类似的<strong>微批处理</strong>思想<ul>
<li>每过一个<strong>时间间隔</strong>，就去拿<strong>最新的数据</strong>加入到<strong>输入数据表</strong>并<strong>更新结果</strong></li>
<li>但相比于 Spark Streaming，<strong>更接近于实时处理</strong>，可以做到<strong>更小的时间间隔</strong>，最小延迟在 <strong>100 毫秒</strong>左右</li>
<li>从 <strong>Spark 2.3</strong> 开始，Structured Streaming 引入了<strong>连续处理</strong>的模式，可以做到真正的<strong>毫秒级延迟</strong></li>
</ul>
</li>
</ol>
<h2 id="事件时间"><a href="#事件时间" class="headerlink" title="事件时间"></a>事件时间</h2><blockquote>
<p>Spark Streaming - 处理时间<br>Structured Streaming - 处理时间 or <strong>事件时间</strong></p>
</blockquote>
<ol>
<li><strong>Structured Streaming</strong> 对基于<strong>事件时间</strong>的处理有很好的支持</li>
<li>而 <strong>Spark Streaming</strong> 是将数据按照<strong>接收到的时间</strong>切分成一个个 <strong>RDD</strong> 来进行<strong>批处理</strong>的<ul>
<li>很难基于数据本身的<strong>事件时间</strong>进行处理，如果某个数据的处理时间与事件时间<strong>不一致</strong>，很<strong>容易出问题</strong></li>
</ul>
</li>
</ol>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top">zhongmingmao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top/2024/09/21/bigdata-spark-structured-streaming/">https://blog.zhongmingmao.top/2024/09/21/bigdata-spark-structured-streaming/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/big-data/">Big Data</a><a class="post-meta__tags" href="/tags/spark/">Spark</a></div><div class="post-share"><div class="social-share" data-image="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-structured-streaming.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/09/22/bigdata-spark-flink/" title="Big Data - Spark + Flink"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-flink.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Big Data - Spark + Flink</div></div><div class="info-2"><div class="info-item-1">Spark 实时性 无论是 Spark Streaming 还是 Structured Streaming，Spark 流处理的实时性还不够 无法应对实时性要求很高的流处理场景   Spark 的流处理是基于微批处理的思想 把流处理看做批处理的一种特殊形式，没接收到一个时间间隔的数据才会去处理   虽然在 Spark 2.3 中提出连续处理模型，但只支持有限的功能，并不能在大项目中使用 要在流处理的实时性提升，就不能继续用微批处理的模式，而是有数据数据就立即处理，不做等待 Apache Flink 采用了基于操作符（Operator）的连续流模型，可以做到微秒级别的延迟      Flink模型 Flink 中最核心的数据结构是 Stream，代表一个运行在多个分区上的并行流 在 Stream 上可以进行各种转换（Transformation）操作 与 Spark RDD 不同的是，Stream 代表一个数据流而不是静态数据的集合 Stream 所包含的数据随着时间增长而变化的 而且 Stream 上的转换操作都是逐条进行的 - 每当有新数据进入，整个流程都会被执行并更新结果 Flink 比 Spark...</div></div></div></a><a class="pagination-related" href="/2024/09/20/bigdata-spark-streaming/" title="Spark - Streaming"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-streaming.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Spark - Streaming</div></div><div class="info-2"><div class="info-item-1">流处理 Spark SQL 中的 DataFrame API 和 DataSet API 都是基于批处理模式对静态数据进行处理 在 2013，Spark 的流处理组件 Spark Streaming 发布，现在的 Spark Streaming 已经非常成熟，应用非常广泛    原理 Spark Streaming 的原理与微积分的思想很类似 微分是无限细分，而积分是对无限细分的每一段进行求和 本质 - 将一个连续的问题转换成了无限个离散的问题   流处理的数据是一系列连续不断变化，且无边界的，永远无法预测下一秒的数据 Spark Streaming 用时间片拆分了无限的数据流 然后对每个数据片用类似于批处理的方法进行处理，输出的数据也是分块的      Spark Streaming 提供一个对于流数据的抽象 DStream DStream 可以由 Kafka、Flume 或者 HDFS 的流数据生成，也可以由别的 DStream 经过各种转换操作得到 底层 DStream 由多个序列化的 RDD 构成，按时间片（如一秒）切分成的每个数据单位都是一个 RDD Spark 核心引擎将对 DStream ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/09/22/bigdata-spark-flink/" title="Big Data - Spark + Flink"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-flink.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-22</div><div class="info-item-2">Big Data - Spark + Flink</div></div><div class="info-2"><div class="info-item-1">Spark 实时性 无论是 Spark Streaming 还是 Structured Streaming，Spark 流处理的实时性还不够 无法应对实时性要求很高的流处理场景   Spark 的流处理是基于微批处理的思想 把流处理看做批处理的一种特殊形式，没接收到一个时间间隔的数据才会去处理   虽然在 Spark 2.3 中提出连续处理模型，但只支持有限的功能，并不能在大项目中使用 要在流处理的实时性提升，就不能继续用微批处理的模式，而是有数据数据就立即处理，不做等待 Apache Flink 采用了基于操作符（Operator）的连续流模型，可以做到微秒级别的延迟      Flink模型 Flink 中最核心的数据结构是 Stream，代表一个运行在多个分区上的并行流 在 Stream 上可以进行各种转换（Transformation）操作 与 Spark RDD 不同的是，Stream 代表一个数据流而不是静态数据的集合 Stream 所包含的数据随着时间增长而变化的 而且 Stream 上的转换操作都是逐条进行的 - 每当有新数据进入，整个流程都会被执行并更新结果 Flink 比 Spark...</div></div></div></a><a class="pagination-related" href="/2024/09/20/bigdata-spark-streaming/" title="Spark - Streaming"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-streaming.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-20</div><div class="info-item-2">Spark - Streaming</div></div><div class="info-2"><div class="info-item-1">流处理 Spark SQL 中的 DataFrame API 和 DataSet API 都是基于批处理模式对静态数据进行处理 在 2013，Spark 的流处理组件 Spark Streaming 发布，现在的 Spark Streaming 已经非常成熟，应用非常广泛    原理 Spark Streaming 的原理与微积分的思想很类似 微分是无限细分，而积分是对无限细分的每一段进行求和 本质 - 将一个连续的问题转换成了无限个离散的问题   流处理的数据是一系列连续不断变化，且无边界的，永远无法预测下一秒的数据 Spark Streaming 用时间片拆分了无限的数据流 然后对每个数据片用类似于批处理的方法进行处理，输出的数据也是分块的      Spark Streaming 提供一个对于流数据的抽象 DStream DStream 可以由 Kafka、Flume 或者 HDFS 的流数据生成，也可以由别的 DStream 经过各种转换操作得到 底层 DStream 由多个序列化的 RDD 构成，按时间片（如一秒）切分成的每个数据单位都是一个 RDD Spark 核心引擎将对 DStream ...</div></div></div></a><a class="pagination-related" href="/2024/09/19/bigdata-spark-sql/" title="Spark - SQL"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-sql.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-19</div><div class="info-item-2">Spark - SQL</div></div><div class="info-2"><div class="info-item-1">历史Hive 一开始，Hadoop&#x2F;MapReduce 在企业生产中大量使用，在 HDFS 上积累了大量数据 MapReduce 对于开发者而言使用难度较大，大部分开发人员最熟悉的还是传统的关系型数据库 为了方便大多数开发人员使用 Hadoop，诞生了 Hive Hive 提供类似 SQL 的编程接口，HQL 经过语法解析、逻辑计划、物理计划转化成 MapReduce 程序执行 使得开发人员很容易对 HDFS 上存储的数据进行查询和分析      Shark 在 Spark 刚问世时，Spark 团队开发了 Shark 来支持用 SQL 来查询 Spark 的数据 Shark 的本质是 Hive，Shark 修改了 Hive 的内存管理模块，大幅优化了运行速度 Shark 依赖于 Hive，严重影响了 Spark 的发展，Spark 要定义一个统一的技术栈和完整的生态 依赖于 Hive 还会制约 Spark 各个组件的相互集成，Spark 无法利用 Spark 的特性进行深度优化 2014 年 7 月 1 日，Spark 团队将 Shark 交给 Hive 进行管理，即 Hive on Spa...</div></div></div></a><a class="pagination-related" href="/2024/09/18/bigdata-spark-rdd/" title="Spark - RDD"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark-rdd.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-18</div><div class="info-item-2">Spark - RDD</div></div><div class="info-2"><div class="info-item-1">分布式内存 传统的 MapReduce 框架运行缓慢，主要原因是 DAG 的中间计算结果需要写入硬盘来防止运行结果丢失 每次调用中间计算结果都需要进行一次硬盘的读取 反复对硬盘进行读写操作以及潜在的数据复制和序列化操作会大大地提高了计算延迟   新的分布式存储方案 - 保持之前系统的稳定性、错误恢复和可扩展性，并尽可能地减少硬盘 IO 操作 RDD 是基于分布式内存的数据抽象，不仅支持基于工作集的应用，同时具有数据流模型的特点      定义分区 分区代表同一个 RDD 包含的数据被存储在系统的不同节点上，这是可以被并行处理的前提 在逻辑上，可以认为 RDD 是一个大数组，数组中的每个元素代表一个分区（Partition） 在物理存储中，每个分区指向一个存放在内存或者硬盘中的数据块（Block） Block 是独立的，可以被存放在分布式系统中的不同节点   RDD 只是抽象意义的数据集合，分区内部并不会存储具体的数据    RDD 中的每个分区都有它在该 RDD 中的 Index 通过 RDD_ID 和 Partition_Index 可以唯一确定对应 Block 的编号 从而通过底层存储层的接口中提取...</div></div></div></a><a class="pagination-related" href="/2024/09/17/bigdata-spark-overview/" title="Spark - Overview"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/bigdata-spark.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-17</div><div class="info-item-2">Spark - Overview</div></div><div class="info-2"><div class="info-item-1">MapReduce概述 MapReduce 通过简单的 Map 和 Reduce 的抽象提供了一个编程模型 可以在一个由上百台机器组成的集群上并发处理大量的数据集，而把计算细节隐藏起来   各种各样的复杂数据处理都可以分解为 Map 和 Reduce 的基本元素 复杂的数据处理可以分解成由多个 Job（包含一个 Mapper 和一个 Reducer）组成的 DAG 然后，将每个 Mapper 和 Reducer 放到 Hadoop 集群上执行，得到最终结果      不足 高昂的维护成本 时间性能不达标 MapReduce 模型的抽象层次低 大量的底层逻辑需要开发者手工完成 - 用汇编语言开发游戏   只提供 Map 和 Reduce 操作 很多现实的数据处理场景并不适合用这个模型来描述 实现复杂的操作需要技巧，让整个工程变得庞大且难以维护 维护一个多任务协调的状态机成本很高，且扩展性很差   在 Hadoop 中，每个 Job 的计算结果都会存储在 HDFS 文件存储系统中 每一步计算都要进行硬盘的读取和写入，大大增加了系统的延迟   MapReduce 对于迭代算法的处理性能很差，而且非常耗资源 因...</div></div></div></a><a class="pagination-related" href="/2024/10/05/bigdata-beam-future/" title="Beam - Future"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://big-data-1253868755.cos.ap-guangzhou.myqcloud.com/flink-runner-beam-beam-vision.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-05</div><div class="info-item-2">Beam - Future</div></div><div class="info-2"><div class="info-item-1">技术迭代 2006，Apache Hadoop 发布，基于 MapReduce 计算模型 2009，Spark 计算框架在 加州伯克利大学诞生，于 2010 年开源，于 2014 年成为 Apache 的顶级项目 Spark 的数据处理效率远在 Hadoop 之上   2014，Flink 面世，流批一体，于 2018 年被阿里收购    Apache Beam Apache Beam 根据 Dataflow Model API 实现的，能完全胜任批流一体的任务 Apache Beam 有中间的抽象转换层，工程师无需学习新 Runner 的 API 的语法，减少学习新技术的时间成本 Runner 可以专心优化效率和迭代功能，而不必担心迁移  Beam Runner 迭代非常快 - 如 Flink  </div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zhongmingmao</div><div class="author-info-description">Focus on Infrastructure.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">641</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhongmingmao0625@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Things are always unexpected!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Streaming-DataFrame-API"><span class="toc-number">3.</span> <span class="toc-text">Streaming DataFrame API</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-DataFrame"><span class="toc-number">3.1.</span> <span class="toc-text">创建 DataFrame</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">3.2.</span> <span class="toc-text">查询操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C%E6%B5%81"><span class="toc-number">3.3.</span> <span class="toc-text">输出结果流</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Structured-Streaming-vs-Spark-Streaming"><span class="toc-number">4.</span> <span class="toc-text">Structured Streaming vs Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%93%E7%94%A8%E6%80%A7-%E6%80%A7%E8%83%BD"><span class="toc-number">4.1.</span> <span class="toc-text">易用性 + 性能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E6%80%A7"><span class="toc-number">4.2.</span> <span class="toc-text">实时性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8B%E4%BB%B6%E6%97%B6%E9%97%B4"><span class="toc-number">4.3.</span> <span class="toc-text">事件时间</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/23/cloud-native-observability-prometheus-concepts/" title="Observability - Prometheus Concepts"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus-concepts.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Concepts"/></a><div class="content"><a class="title" href="/2025/01/23/cloud-native-observability-prometheus-concepts/" title="Observability - Prometheus Concepts">Observability - Prometheus Concepts</a><time datetime="2025-01-22T16:06:25.000Z" title="Created 2025-01-23 00:06:25">2025-01-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Introduction"/></a><div class="content"><a class="title" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction">Observability - Prometheus Introduction</a><time datetime="2025-01-21T16:06:25.000Z" title="Created 2025-01-22 00:06:25">2025-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java-agent.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java Zero Code"/></a><div class="content"><a class="title" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code">Observability - OpenTelemetry Java Zero Code</a><time datetime="2025-01-20T16:06:25.000Z" title="Created 2025-01-21 00:06:25">2025-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java"/></a><div class="content"><a class="title" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java">Observability - OpenTelemetry Java</a><time datetime="2025-01-19T16:06:25.000Z" title="Created 2025-01-20 00:06:25">2025-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/mcp.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - MCP Overview"/></a><div class="content"><a class="title" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview">AI Agent - MCP Overview</a><time datetime="2025-01-18T16:06:25.000Z" title="Created 2025-01-19 00:06:25">2025-01-19</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2015 - 2025 By zhongmingmao</span></div><div class="footer_custom_text">Life is like a box of chocolates. You can't know what you'll eat until you open it.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div></body></html>