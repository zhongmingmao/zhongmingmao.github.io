<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM PEFT - ChatGLM3-6B + LoRA | ByteCoding</title><meta name="author" content="zhongmingmao"><meta name="copyright" content="zhongmingmao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="通用 LLM 千亿大模型（130B、ChatGPT）和小规模的大模型（6B、LLaMA2）都是通用 LLM   通用 LLM 都是通过常识进行预训练的 在实际使用过程中，需要 LLM 具备某一特定领域知识的能力 - 对 LLM 的能力进行增强">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM PEFT - ChatGLM3-6B + LoRA">
<meta property="og:url" content="https://blog.zhongmingmao.top/2024/06/29/llm-peft-chatglm3-6b-lora/index.html">
<meta property="og:site_name" content="ByteCoding">
<meta property="og:description" content="通用 LLM 千亿大模型（130B、ChatGPT）和小规模的大模型（6B、LLaMA2）都是通用 LLM   通用 LLM 都是通过常识进行预训练的 在实际使用过程中，需要 LLM 具备某一特定领域知识的能力 - 对 LLM 的能力进行增强">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg">
<meta property="article:published_time" content="2024-06-28T16:06:25.000Z">
<meta property="article:modified_time" content="2024-08-16T17:08:30.135Z">
<meta property="article:author" content="zhongmingmao">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LangChain">
<meta property="article:tag" content="AI Agent">
<meta property="article:tag" content="RAG">
<meta property="article:tag" content="LoRA">
<meta property="article:tag" content="PEFT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM PEFT - ChatGLM3-6B + LoRA",
  "url": "https://blog.zhongmingmao.top/2024/06/29/llm-peft-chatglm3-6b-lora/",
  "image": "https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg",
  "datePublished": "2024-06-28T16:06:25.000Z",
  "dateModified": "2024-08-16T17:08:30.135Z",
  "author": [
    {
      "@type": "Person",
      "name": "zhongmingmao",
      "url": "https://blog.zhongmingmao.top"
    }
  ]
}</script><link rel="shortcut icon" href="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png"><link rel="canonical" href="https://blog.zhongmingmao.top/2024/06/29/llm-peft-chatglm3-6b-lora/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":32,"languages":{"author":"Author: zhongmingmao","link":"Link: ","source":"Source: ByteCoding","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM PEFT - ChatGLM3-6B + LoRA',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ByteCoding" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/url(https:/cdn.pixabay.com/photo/2021/07/20/03/39/fisherman-6479663_1280.jpg));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">640</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ByteCoding</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM PEFT - ChatGLM3-6B + LoRA</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM PEFT - ChatGLM3-6B + LoRA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2024-06-28T16:06:25.000Z" title="Created 2024-06-29 00:06:25">2024-06-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/llm/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">1.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>5mins</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:512,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2024-08-17 01:08:30&quot;}" hidden></div><h1 id="通用-LLM"><a href="#通用-LLM" class="headerlink" title="通用 LLM"></a>通用 LLM</h1><blockquote>
<p>千亿大模型（130B、ChatGPT）和小规模的大模型（6B、LLaMA2）都是通用 LLM</p>
</blockquote>
<ol>
<li><strong>通用 LLM</strong> 都是通过<strong>常识</strong>进行<strong>预训练</strong>的</li>
<li>在实际使用过程中，需要 LLM 具备某一特定<strong>领域知识</strong>的能力 - 对 LLM 的能力进行<strong>增强</strong></li>
</ol>
<span id="more"></span>

<h1 id="增强方式"><a href="#增强方式" class="headerlink" title="增强方式"></a>增强方式</h1><table>
<thead>
<tr>
<th>Method</th>
<th>Desc</th>
</tr>
</thead>
<tbody><tr>
<td>微调</td>
<td>让预先训练好的 LLM <strong>适应</strong>特定任务或数据集的方案，<strong>成本相对低</strong><br />LLM 学会训练者提供的微调数据，并具备一定的<strong>理解</strong>能力</td>
</tr>
<tr>
<td>知识库</td>
<td>使用<strong>向量数据库</strong>或者其它数据库存储数据，为 LLM 提供<strong>信息来源外挂</strong></td>
</tr>
<tr>
<td>API</td>
<td>与知识库类似，为 LLM 提供<strong>信息来源外挂</strong></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>互不冲突</strong>，可以<strong>同时使用</strong>几种方案来优化 LLM，提升内容输出能力</p>
</blockquote>
<blockquote>
<p>LoRA &#x2F; QLoRA &#x2F; 知识库 &#x2F; API</p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816163919085.png" alt="image-20240816163919085"></p>
<blockquote>
<p>LLM Performance &#x3D; <strong>推理效果</strong></p>
</blockquote>
<h1 id="落地过程"><a href="#落地过程" class="headerlink" title="落地过程"></a>落地过程</h1><table>
<thead>
<tr>
<th>Method</th>
<th>Pipeline</th>
</tr>
</thead>
<tbody><tr>
<td>微调</td>
<td>准备数据 -&gt; 微调 -&gt; 验证 -&gt; 提供服务</td>
</tr>
<tr>
<td>知识库</td>
<td>准备数据 -&gt; 构建<strong>向量库</strong> -&gt; 构建<strong>智能体</strong> -&gt; 提供服务</td>
</tr>
<tr>
<td>API</td>
<td>准备数据 -&gt; 开发<strong>接口</strong> -&gt; 构建<strong>智能体</strong> -&gt; 提供服务</td>
</tr>
</tbody></table>
<h1 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h1><blockquote>
<p>法律小助手用来解决日常生活中遇到的法律问题，以<strong>问答</strong>的方式进行 - <strong>知识库 or 微调</strong></p>
</blockquote>
<h2 id="知识库"><a href="#知识库" class="headerlink" title="知识库"></a>知识库</h2><blockquote>
<p>一旦数据集不足，可以<strong>随时补充</strong>，<strong>即时生效</strong></p>
</blockquote>
<ol>
<li>将数据集<strong>拆分</strong>成一条一条的<strong>知识</strong>，放入到<strong>向量库</strong></li>
<li>然后通过 <strong>Agent</strong> 从向量库<strong>检索</strong>，在<strong>输入</strong>给 LLM</li>
</ol>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><ol>
<li>法律知识有时候需要一定的<strong>逻辑能力</strong>，不是纯文本检索</li>
<li>微调 - 通过在一定量的数据集上的训练，<strong>增加</strong> LLM 法律相关的<strong>常识及思维</strong>，从而进行<strong>推理</strong></li>
</ol>
<h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><h2 id="原始数据"><a href="#原始数据" class="headerlink" title="原始数据"></a>原始数据</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/lawzhidao/intro.ipynb">https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/lawzhidao/intro.ipynb</a></p>
</blockquote>
<table>
<thead>
<tr>
<th>字段</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>title</td>
<td>问题的标题</td>
</tr>
<tr>
<td>question</td>
<td>问题内容（可为空）</td>
</tr>
<tr>
<td>reply</td>
<td>回复内容</td>
</tr>
<tr>
<td>is_best</td>
<td>是否为页面上显示的最佳回答</td>
</tr>
</tbody></table>
<h2 id="微调数据"><a href="#微调数据" class="headerlink" title="微调数据"></a>微调数据</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;conversations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;类型#裙*裙长#半身裙&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;这款百搭时尚的仙女半身裙，整体设计非常的飘逸随性，穿上之后每个女孩子都能瞬间变成小仙女啦。料子非常的轻盈，透气性也很好，穿到夏天也很舒适。&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>让 ChatGPT 生成转换代码</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">原始数据是CSV格式，包含4列：title、question、reply、is_best，需要通过Python语言处理该CSV文件，来构建大语言模型的微调数据集，目标数据集格式是JSON的，单条数据格式为：&#123;&quot;conversations&quot;:[&#123;&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;value1&quot;&#125;,&#123;&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;value2&quot;&#125;]&#125;，需要将原始CSV文件里的title列填充到目标JSON文件里的value1处，原始CSV文件里的reply填充到目标JSON文件里的value1处，请注意：最终生成的不是JSON数组，而是每个JSON对象生成一行，出示示例代码。</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816183045914.png" alt="image-20240816183045914"></p>
<h1 id="微调-1"><a href="#微调-1" class="headerlink" title="微调"></a>微调</h1><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install libopenmpi-dev</span><br><span class="line"></span><br><span class="line">$ ChatGLM3/finetune_demo/</span><br><span class="line">$ pip3 install -r requirements.txt</span><br><span class="line"></span><br><span class="line">$ pip3 install nltk</span><br><span class="line">$ pip3 install typer</span><br><span class="line">$ pip3 install sentencepiece</span><br><span class="line">$ pip3 install deepspeed==0.14.4</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816224024027.png" alt="image-20240816224024027"></p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><ol>
<li>训练需要至少准备两个数据集，一个用来<strong>训练</strong>，一个用来<strong>验证</strong></li>
<li>train.json 与 dev.json 格式相同</li>
<li>当 LLM 在训练过程中，会自动进行测试验证，输出<strong>微调效果</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ tree data/</span><br><span class="line">data/</span><br><span class="line">├── dev.json</span><br><span class="line">└── train.json</span><br><span class="line"></span><br><span class="line">$ du -sh data/*</span><br><span class="line">64K    data/dev.json</span><br><span class="line">29M    data/train.json</span><br></pre></td></tr></table></figure>

<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><blockquote>
<p>configs&#x2F;lora.yaml</p>
</blockquote>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">data_config:</span></span><br><span class="line">  <span class="attr">train_file:</span> <span class="string">/home/ubuntu/peft/ChatGLM3/finetune_demo/data/train.json</span></span><br><span class="line">  <span class="attr">val_file:</span> <span class="string">/home/ubuntu/peft/ChatGLM3/finetune_demo/data/dev.json</span></span><br><span class="line">  <span class="attr">test_file:</span> <span class="string">/home/ubuntu/peft/ChatGLM3/finetune_demo/data/dev.json</span></span><br><span class="line"><span class="attr">training_args:</span></span><br><span class="line">  <span class="attr">output_dir:</span> <span class="string">/home/ubuntu/peft/ChatGLM3/finetune_demo/output</span></span><br><span class="line">  <span class="attr">max_steps:</span> <span class="number">3000</span> <span class="comment"># 最大训练轮数</span></span><br><span class="line">  <span class="attr">save_steps:</span> <span class="number">500</span> <span class="comment"># 每训练多少轮保存权重</span></span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816225750003.png" alt="image-20240816225750003"></p>
<h2 id="微调-2"><a href="#微调-2" class="headerlink" title="微调"></a>微调</h2><table>
<thead>
<tr>
<th>Parameter</th>
<th>Desc</th>
</tr>
</thead>
<tbody><tr>
<td>训练数据集所在的目录</td>
<td>data</td>
</tr>
<tr>
<td>模型所在目录</td>
<td>..&#x2F;model</td>
</tr>
<tr>
<td>微调配置</td>
<td>configs&#x2F;lora.yaml</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 finetune_hf.py data ../model/ configs/lora.yaml</span><br></pre></td></tr></table></figure>

<blockquote>
<p>trainable params 为 <strong>1.9M</strong>，整个参数量为 <strong>6B</strong>，训练比为 <strong>3%</strong></p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240817001459446.png" alt="image-20240817001459446"></p>
<ol>
<li><strong>trainable params</strong> 指的是在模型训练过程中可以被<strong>优化</strong>或<strong>更新</strong>的参数数量</li>
<li>在<strong>深度学习</strong>模型中，这些参数通常是<strong>网络</strong>的<strong>权重</strong>和<strong>偏置</strong></li>
<li>它们是<strong>可训练</strong>的<ul>
<li>因为在训练过程中，通过<strong>反向传播算法</strong>，这些<strong>参数</strong>会根据<strong>损失函数</strong>的<strong>梯度</strong>不断<strong>更新</strong></li>
<li>以减少<strong>模型输出</strong>与真<strong>实标签</strong>之间的<strong>差异</strong></li>
</ul>
</li>
<li>通过调整 lora.yaml 中的 peft_config 的 <strong>r</strong> 参数来改变<strong>可训练参数的数量</strong><ul>
<li><strong>r</strong> 越大，<strong>trainable params</strong> 就越大</li>
<li>r - <strong>LoRA 矩阵的秩</strong></li>
</ul>
</li>
</ol>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240817001802203.png" alt="image-20240817001802203"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240817003456160.png" alt="image-20240817003456160"></p>
<h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3><ol>
<li><strong>损失函数</strong>衡量<strong>模型预测的输出</strong>与<strong>实际数据</strong>之间的<strong>差异</strong></li>
<li>在训练过程中，目标是<strong>最小化</strong>该<strong>损失值</strong>，从而<strong>提高</strong>模型的<strong>准确性</strong></li>
</ol>
<h3 id="grad-norm"><a href="#grad-norm" class="headerlink" title="grad_norm"></a>grad_norm</h3><blockquote>
<p>梯度范数 - 参数更新幅度</p>
</blockquote>
<ol>
<li>在训练<strong>深度学习</strong>模型时，通过<strong>反向传播</strong>算法计算<strong>参数的梯度</strong>，以便更新这些参数</li>
<li><strong>梯度范数</strong>是这些<strong>梯度向量</strong>的<strong>大小</strong>或者<strong>长度</strong>，提供了关于<strong>参数更新幅度</strong>的信息</li>
<li>如果梯度范数<strong>非常大</strong>，可能表示模型在训练过程中遇到了<strong>梯度爆炸</strong>问题</li>
<li>如果梯度范数<strong>太小</strong>，可能表示<strong>梯度消失</strong>问题</li>
</ol>
<h3 id="learning-rate"><a href="#learning-rate" class="headerlink" title="learning_rate"></a>learning_rate</h3><blockquote>
<p>学习率 - 控制<strong>参数更新幅度</strong>的<strong>超参数</strong></p>
</blockquote>
<ol>
<li>在优化算法中，学习率决定了在<strong>反向传播</strong>期间<strong>参数更新</strong>的<strong>步长</strong>大小</li>
<li>学习率<strong>太高</strong>，会导致训练过程<strong>不稳定</strong></li>
<li>学习率<strong>太低</strong>，会导致训练<strong>进展缓慢</strong>或者陷入<strong>局部最小值</strong></li>
</ol>
<h3 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a>epoch</h3><ol>
<li>epoch - <strong>训练算法</strong>在<strong>整个训练数据集</strong>上的一次<strong>完整遍历</strong></li>
<li>通常需要<strong>多个 epochs</strong> 来训练模型，以确保模型能够<strong>充分学习</strong>数据<strong>集中</strong>的模式</li>
<li>每个 epoch 后，通常会<strong>评估</strong>模型在<strong>验证集</strong>上的表现，以<strong>监控</strong>和<strong>调整</strong>训练过程</li>
</ol>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240817004644007.png" alt="image-20240817004644007"></p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><h3 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h3><blockquote>
<p>output&#x2F;checkpoint-3000 - 新生成的<strong>权重</strong><br>模型启动时，会将<strong>原模型</strong>和<strong>新权重</strong>全部加载，然后进行<strong>推理</strong></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 inference_hf.py output/checkpoint-3000/ --prompt &quot;xxxxxxxxxxxx&quot;</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240817005053174.png" alt="image-20240817005053174"></p>
<h3 id="微调前"><a href="#微调前" class="headerlink" title="微调前"></a>微调前</h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240817005425390.png" alt="image-20240817005425390"></p>
<h3 id="微调后"><a href="#微调后" class="headerlink" title="微调后"></a>微调后</h3><blockquote>
<p>微调数据集中有对应内容，微调<strong>效果明显</strong></p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240817005718296.png" alt="image-20240817005718296"></p>
<h1 id="服务"><a href="#服务" class="headerlink" title="服务"></a>服务</h1><blockquote>
<p><strong>微调</strong>完成后，即<strong>验证</strong>后得知<strong>整体效果</strong>满足一定的<strong>百分比</strong>，可以<strong>对外服务</strong><br>通过 <strong>API 组件</strong>将模型的输入输出封装成接口对外提供服务</p>
</blockquote>
<ol>
<li>模型的<strong>推理性能</strong> - 效果</li>
<li>模型的<strong>推理吞吐量</strong></li>
<li>服务的<strong>限流</strong>，适当保护 LLM 集群</li>
<li>当 LLM 服务不可用时，服务<strong>降级</strong>，开关控制</li>
</ol>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top">zhongmingmao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top/2024/06/29/llm-peft-chatglm3-6b-lora/">https://blog.zhongmingmao.top/2024/06/29/llm-peft-chatglm3-6b-lora/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ai/">AI</a><a class="post-meta__tags" href="/tags/llm/">LLM</a><a class="post-meta__tags" href="/tags/nlp/">NLP</a><a class="post-meta__tags" href="/tags/langchain/">LangChain</a><a class="post-meta__tags" href="/tags/ai-agent/">AI Agent</a><a class="post-meta__tags" href="/tags/rag/">RAG</a><a class="post-meta__tags" href="/tags/lora/">LoRA</a><a class="post-meta__tags" href="/tags/peft/">PEFT</a></div><div class="post-share"><div class="social-share" data-image="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/06/30/llm-rag-chatglm3-6b-langchain-faiss/" title="LLM RAG - ChatGLM3-6B + LangChain + Faiss"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">LLM RAG - ChatGLM3-6B + LangChain + Faiss</div></div><div class="info-2"><div class="info-item-1">RAG 使用知识库，用来增强 LLM 信息检索的能力   知识准确 先把知识进行向量化，存储到向量数据库中 使用的时候通过向量检索从向量数据库中将知识检索出来，确保知识的准确性   更新频率快 当发现知识库里面的知识不全时，可以随时补充 不需要像微调一样，重新跑微调任务、验证结果、重新部署等      应用场景 ChatOps   知识库模式适用于相对固定的场景做推理 如企业内部使用的员工小助手，不需要太多的逻辑推理 使用知识库模式检索精度高，且可以随时更新 LLM 基础能力 + Agent 进行堆叠，可以产生智能化的效果  LangChain-Chatchat组成模块   模块 作用 支持列表    大语言模型 智能体核心引擎 ChatGLM &#x2F; Qwen &#x2F; Baichuan &#x2F; LLaMa   Embedding 模型 文本向量化 m3e-* &#x2F; bge-*   分词器 按照规则将句子分成短句或者单词 LangChain Text Splitter   向量数据库 向量化数据存储 Faiss &#x2F; Milvus   Agent Tools 调用第三方...</div></div></div></a><a class="pagination-related" href="/2024/06/28/llm-deploy-chatglm3-6b/" title="LLM Deploy - ChatGLM3-6B"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">LLM Deploy - ChatGLM3-6B</div></div><div class="info-2"><div class="info-item-1">LLM 选择核心玩家 厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 LLaMA 套壳     ChatGLM-6B ChatGLM-6B 和 LLaMA2 是比较热门的开源项目，国产 LLM 是首选 企业布局 LLM 选择 MaaS 服务，调用大厂 LLM API，但会面临数据安全问题 选择开源 LLM，自己微调、部署、为上层应用提供服务   企业一般会选择私有化部署 + 公有云 MaaS 的混合架构 在国产厂商中，从技术角度来看，智谱 AI 是国内 LLM 研发水平最高的厂商 6B 的参数规模为 62 亿，单张 3090 显卡就可以进行微调和推理 企业预算充足（百万以上，GPU 费用 + 商业授权） 可以尝试 GLM-130B，千亿参数规模，推理能力更强 GLM-130B 轻量化后，可以在 3090 × 4 上进行推理 训练 GLM-130B 大概需要 96 台 A100（320G），历时两个多月     计算资源 适合 CPU 计算的 LLM 不多 有些 LLM 可以在 CPU 上进行推理，但需要使用低精度轻量化的 LLM 但在低精度下，LLM 会失真，效果较差   要真正体验并应...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/06/30/llm-rag-chatglm3-6b-langchain-faiss/" title="LLM RAG - ChatGLM3-6B + LangChain + Faiss"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-30</div><div class="info-item-2">LLM RAG - ChatGLM3-6B + LangChain + Faiss</div></div><div class="info-2"><div class="info-item-1">RAG 使用知识库，用来增强 LLM 信息检索的能力   知识准确 先把知识进行向量化，存储到向量数据库中 使用的时候通过向量检索从向量数据库中将知识检索出来，确保知识的准确性   更新频率快 当发现知识库里面的知识不全时，可以随时补充 不需要像微调一样，重新跑微调任务、验证结果、重新部署等      应用场景 ChatOps   知识库模式适用于相对固定的场景做推理 如企业内部使用的员工小助手，不需要太多的逻辑推理 使用知识库模式检索精度高，且可以随时更新 LLM 基础能力 + Agent 进行堆叠，可以产生智能化的效果  LangChain-Chatchat组成模块   模块 作用 支持列表    大语言模型 智能体核心引擎 ChatGLM &#x2F; Qwen &#x2F; Baichuan &#x2F; LLaMa   Embedding 模型 文本向量化 m3e-* &#x2F; bge-*   分词器 按照规则将句子分成短句或者单词 LangChain Text Splitter   向量数据库 向量化数据存储 Faiss &#x2F; Milvus   Agent Tools 调用第三方...</div></div></div></a><a class="pagination-related" href="/2024/06/28/llm-deploy-chatglm3-6b/" title="LLM Deploy - ChatGLM3-6B"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-28</div><div class="info-item-2">LLM Deploy - ChatGLM3-6B</div></div><div class="info-2"><div class="info-item-1">LLM 选择核心玩家 厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 LLaMA 套壳     ChatGLM-6B ChatGLM-6B 和 LLaMA2 是比较热门的开源项目，国产 LLM 是首选 企业布局 LLM 选择 MaaS 服务，调用大厂 LLM API，但会面临数据安全问题 选择开源 LLM，自己微调、部署、为上层应用提供服务   企业一般会选择私有化部署 + 公有云 MaaS 的混合架构 在国产厂商中，从技术角度来看，智谱 AI 是国内 LLM 研发水平最高的厂商 6B 的参数规模为 62 亿，单张 3090 显卡就可以进行微调和推理 企业预算充足（百万以上，GPU 费用 + 商业授权） 可以尝试 GLM-130B，千亿参数规模，推理能力更强 GLM-130B 轻量化后，可以在 3090 × 4 上进行推理 训练 GLM-130B 大概需要 96 台 A100（320G），历时两个多月     计算资源 适合 CPU 计算的 LLM 不多 有些 LLM 可以在 CPU 上进行推理，但需要使用低精度轻量化的 LLM 但在低精度下，LLM 会失真，效果较差   要真正体验并应...</div></div></div></a><a class="pagination-related" href="/2024/06/27/llm-langchain-rag/" title="LLM - LangChain + RAG"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-27</div><div class="info-item-2">LLM - LangChain + RAG</div></div><div class="info-2"><div class="info-item-1">局限 大模型的核心能力 - 意图理解 + 文本生成     局限 描述    数据的及时性 大部分 AI 大模型都是预训练的，如果要问一些最新的消息，大模型是不知道的   复杂任务处理 AI 大模型在问答方面表现出色，但不总是能够处理复杂任务AI 大模型主要是基于文本的交互（多模态除外）   代码生成与下载 根据需求描述生成对应的代码，并提供下载链接 - 暂时不支持   与企业应用场景的集成 读取关系型数据库里面的数据，并根据提示进行任务处理 - 暂时不支持    在实际应用过程中，输入数据和输出数据，不仅仅是纯文本 AI Agent - 需要解析用户的输入输出    AI Agent AI Agent 是以 LLM 为核心控制器的一套代理系统    控制端处于核心地位，承担记忆、思考以及决策等基础工作 感知模块负责接收和处理来自于外部环境的多样化信息 - 文字、声音、图片、位置等 行动模块通过生成文本、API 调用、使用工具等方式来执行任务以及改变环境   LangChain - 开源 + 提供一整套围绕 LLM 的 Agent 工具   AI Agent 很有可能在未来一段时间内成为 AI 发展的一...</div></div></div></a><a class="pagination-related" href="/2024/08/04/rag-langchain/" title="RAG - LangChain"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-agent.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-04</div><div class="info-item-2">RAG - LangChain</div></div><div class="info-2"><div class="info-item-1">Practice   LangChain RAG https://github.com/langchain-ai/rag-from-scratch   RAG 如何随着长期 LLM 而改变 Is RAG Really Dead? https://www.youtube.com/watch?v=SsHUNfhF32s     自适应 RAG 根据复杂程度动态地将查询路由到不同的 RAG 方法 - Command-R @ LangGraph Adaptive RAG https://www.youtube.com/watch?v=04ighIjMcAI   Code https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb   Paper https://arxiv.org/abs/2403.14403     Adaptive RAG 在循环单元测试中自我纠正检索错误，以确定文档相关性并返回到网络搜索 在 LangGraph 中实现 Mistral 7B + Ol...</div></div></div></a><a class="pagination-related" href="/2024/09/09/rag-vector-qdrant/" title="RAG - Qdrant"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/qdrant.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="info-item-2">RAG - Qdrant</div></div><div class="info-2"><div class="info-item-1">Features   Getting StartedIntroduction Vector databases are a relatively new way for interacting with abstract data representations derived from opaque machine learning models such as deep learning architectures. These representations are often called vectors or embeddings and they are a compressed version of the data used to train a machine learning model to accomplish a task like sentiment analysis, speech recognition, object detection, and many others.  What is Qdrant? Qdrant “is a vector similarity s...</div></div></div></a><a class="pagination-related" href="/2024/08/17/rag-graphrag/" title="RAG - GraphRAG"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/graphrag-9781829.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-17</div><div class="info-item-2">RAG - GraphRAG</div></div><div class="info-2"><div class="info-item-1">向量检索 信息片段之间的连接能力有限   RAG 在跨越多个信息片段以获取综合见解时表现不足 当要回答一个复杂问题时，必须要通过共享属性在不同信息之间建立联系 RAG 无法有效捕捉这些关系 限制了 RAG 在处理需要多跳推理或整合多源数据的复杂查询时的能力       归纳总结能力不足   在处理大型数据集或长文档时，RAG 难以有效地归纳和总结复杂的语义概念 RAG 在需要全面理解和总结复杂语义信息的场景中表现不佳  GraphRAG 利用 LLM 生成的知识图谱来改进 RAG 的检索部分   GraphRAG 利用结构化的实体和关系信息，使得检索过程更加精准和全面 GraphRAG 在处理多跳问题和复杂文档分析时表现出色 GraphRAG 在处理复杂信息处理任务时，显著提升问答性能，提供比 RAG 更为准确和全面的答案 GraphRAG 通过知识图谱有效地连接不同的信息片段 不仅能够提供准确答案，还能展示答案之间的内在联系，提供更丰富和有价值的结果     GraphRAG 先利用知识图谱，关联查询的实体和关系从与知识图谱实体直接相关的文档中检索片段，提供一个更全面、指标化、高信息密度的总结   主...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zhongmingmao</div><div class="author-info-description">Focus on Infrastructure.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">640</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhongmingmao0625@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Things are always unexpected!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%9A%E7%94%A8-LLM"><span class="toc-number">1.</span> <span class="toc-text">通用 LLM</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A2%9E%E5%BC%BA%E6%96%B9%E5%BC%8F"><span class="toc-number">2.</span> <span class="toc-text">增强方式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%90%BD%E5%9C%B0%E8%BF%87%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">落地过程</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">需求分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%BA%93"><span class="toc-number">4.1.</span> <span class="toc-text">知识库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83"><span class="toc-number">4.2.</span> <span class="toc-text">微调</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-number">5.</span> <span class="toc-text">准备数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE"><span class="toc-number">5.1.</span> <span class="toc-text">原始数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE"><span class="toc-number">5.2.</span> <span class="toc-text">微调数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83-1"><span class="toc-number">6.</span> <span class="toc-text">微调</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96"><span class="toc-number">6.1.</span> <span class="toc-text">依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">6.2.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE"><span class="toc-number">6.3.</span> <span class="toc-text">配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83-2"><span class="toc-number">6.4.</span> <span class="toc-text">微调</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#loss"><span class="toc-number">6.4.1.</span> <span class="toc-text">loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#grad-norm"><span class="toc-number">6.4.2.</span> <span class="toc-text">grad_norm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#learning-rate"><span class="toc-number">6.4.3.</span> <span class="toc-text">learning_rate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#epoch"><span class="toc-number">6.4.4.</span> <span class="toc-text">epoch</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81"><span class="toc-number">6.5.</span> <span class="toc-text">验证</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">6.5.1.</span> <span class="toc-text">加载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E5%89%8D"><span class="toc-number">6.5.2.</span> <span class="toc-text">微调前</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E5%90%8E"><span class="toc-number">6.5.3.</span> <span class="toc-text">微调后</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%8D%E5%8A%A1"><span class="toc-number">7.</span> <span class="toc-text">服务</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Introduction"/></a><div class="content"><a class="title" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction">Observability - Prometheus Introduction</a><time datetime="2025-01-21T16:06:25.000Z" title="Created 2025-01-22 00:06:25">2025-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java-agent.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java Zero Code"/></a><div class="content"><a class="title" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code">Observability - OpenTelemetry Java Zero Code</a><time datetime="2025-01-20T16:06:25.000Z" title="Created 2025-01-21 00:06:25">2025-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java"/></a><div class="content"><a class="title" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java">Observability - OpenTelemetry Java</a><time datetime="2025-01-19T16:06:25.000Z" title="Created 2025-01-20 00:06:25">2025-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/mcp.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - MCP Overview"/></a><div class="content"><a class="title" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview">AI Agent - MCP Overview</a><time datetime="2025-01-18T16:06:25.000Z" title="Created 2025-01-19 00:06:25">2025-01-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/18/ai-agent-overview/" title="AI Agent - Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/ai-agent.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - Overview"/></a><div class="content"><a class="title" href="/2025/01/18/ai-agent-overview/" title="AI Agent - Overview">AI Agent - Overview</a><time datetime="2025-01-17T16:06:25.000Z" title="Created 2025-01-18 00:06:25">2025-01-18</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2015 - 2025 By zhongmingmao</span></div><div class="footer_custom_text">Life is like a box of chocolates. You can't know what you'll eat until you open it.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div></body></html>