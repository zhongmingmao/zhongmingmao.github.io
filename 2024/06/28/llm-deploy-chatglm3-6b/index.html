<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM Deploy - ChatGLM3-6B | ByteCoding</title><meta name="author" content="zhongmingmao"><meta name="copyright" content="zhongmingmao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="LLM 选择核心玩家 厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 LLaMA 套壳">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM Deploy - ChatGLM3-6B">
<meta property="og:url" content="https://blog.zhongmingmao.top/2024/06/28/llm-deploy-chatglm3-6b/index.html">
<meta property="og:site_name" content="ByteCoding">
<meta property="og:description" content="LLM 选择核心玩家 厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 LLaMA 套壳">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png">
<meta property="article:published_time" content="2024-06-27T16:06:25.000Z">
<meta property="article:modified_time" content="2024-08-16T08:03:15.938Z">
<meta property="article:author" content="zhongmingmao">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LangChain">
<meta property="article:tag" content="AI Agent">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM Deploy - ChatGLM3-6B",
  "url": "https://blog.zhongmingmao.top/2024/06/28/llm-deploy-chatglm3-6b/",
  "image": "https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png",
  "datePublished": "2024-06-27T16:06:25.000Z",
  "dateModified": "2024-08-16T08:03:15.938Z",
  "author": [
    {
      "@type": "Person",
      "name": "zhongmingmao",
      "url": "https://blog.zhongmingmao.top"
    }
  ]
}</script><link rel="shortcut icon" href="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png"><link rel="canonical" href="https://blog.zhongmingmao.top/2024/06/28/llm-deploy-chatglm3-6b/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":32,"languages":{"author":"Author: zhongmingmao","link":"Link: ","source":"Source: ByteCoding","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM Deploy - ChatGLM3-6B',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ByteCoding" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/url(https:/cdn.pixabay.com/photo/2021/07/20/03/39/fisherman-6479663_1280.jpg));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">641</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ByteCoding</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM Deploy - ChatGLM3-6B</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM Deploy - ChatGLM3-6B</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2024-06-27T16:06:25.000Z" title="Created 2024-06-28 00:06:25">2024-06-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/llm/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">1.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>5mins</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:512,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2024-08-16 16:03:15&quot;}" hidden></div><h1 id="LLM-选择"><a href="#LLM-选择" class="headerlink" title="LLM 选择"></a>LLM 选择</h1><h2 id="核心玩家"><a href="#核心玩家" class="headerlink" title="核心玩家"></a>核心玩家</h2><blockquote>
<p>厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 <strong>LLaMA</strong> 套壳</p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240815080217105.png" alt="image-20240815080217105"></p>
<span id="more"></span>

<h2 id="ChatGLM-6B"><a href="#ChatGLM-6B" class="headerlink" title="ChatGLM-6B"></a>ChatGLM-6B</h2><ol>
<li><strong>ChatGLM-6B</strong> 和 <strong>LLaMA2</strong> 是比较热门的开源项目，<strong>国产</strong> LLM 是首选</li>
<li>企业布局 LLM<ul>
<li>选择 <strong>MaaS</strong> 服务，调用大厂 LLM API，但会面临<strong>数据安全</strong>问题</li>
<li>选择<strong>开源 LLM</strong>，自己<strong>微调</strong>、<strong>部署</strong>、为上层应用提供服务</li>
</ul>
</li>
<li>企业一般会选择<strong>私有化部署</strong> + <strong>公有云 MaaS</strong> 的混合架构</li>
<li>在<strong>国产</strong>厂商中，从<strong>技术</strong>角度来看，<strong>智谱 AI</strong> 是国内 LLM <strong>研发水平最高</strong>的厂商</li>
<li>6B 的参数规模为 <strong>62 亿</strong>，单张 <strong>3090</strong> 显卡就可以进行<strong>微调</strong>和<strong>推理</strong></li>
<li>企业预算充足（<strong>百万</strong>以上，<strong>GPU 费用</strong> + <strong>商业授权</strong>）<ul>
<li>可以尝试 <strong>GLM-130B</strong>，<strong>千亿</strong>参数规模，<strong>推理</strong>能力更强</li>
<li>GLM-130B <strong>轻量化</strong>后，可以在 <strong>3090 × 4</strong> 上进行<strong>推理</strong></li>
<li>训练 GLM-130B 大概需要 96 台 A100（320G），历时两个多月</li>
</ul>
</li>
</ol>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240815081907748.png" alt="image-20240815081907748"></p>
<h1 id="计算资源"><a href="#计算资源" class="headerlink" title="计算资源"></a>计算资源</h1><ol>
<li>适合 CPU 计算的 LLM 不多<ul>
<li>有些 LLM 可以在 <strong>CPU</strong> 上进行<strong>推理</strong>，但需要使用<strong>低精度轻量化</strong>的 LLM</li>
<li>但在低精度下，LLM 会<strong>失真</strong>，效果较差</li>
</ul>
</li>
<li>要真正体验并应用到实际项目，需要 <strong>GPU</strong></li>
</ol>
<h1 id="ChatGLM3-6B"><a href="#ChatGLM3-6B" class="headerlink" title="ChatGLM3-6B"></a>ChatGLM3-6B</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ol>
<li>ChatGLM-6B 目前已经发展到<strong>第 3 代</strong> ChatGLM3-6B - <strong>中英文推理</strong> + <strong>数学</strong> + <strong>代码</strong>等推理能力</li>
<li>在<u>语义、数学、推理、代码、知识</u>等不同角度的数据集上测评<ul>
<li><strong>ChatGLM3-6B-Base</strong> 在 <strong>10B 以下</strong>的<strong>基础模型</strong>中是<strong>性能最强</strong>的</li>
</ul>
</li>
<li>除此之外，还具有 <strong>8K、32K、128K</strong> 等多个<strong>长文本</strong>理解能力版本</li>
</ol>
<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><table>
<thead>
<tr>
<th>Key</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>OS</td>
<td>Ubuntu &#x2F; CentOS</td>
</tr>
<tr>
<td>Python</td>
<td>3.10～3.11</td>
</tr>
<tr>
<td>Transformers</td>
<td>4.36.2</td>
</tr>
<tr>
<td>Torch</td>
<td>≥ 2.0 - 最佳的推理性能</td>
</tr>
</tbody></table>
<blockquote>
<p>算力</p>
</blockquote>
<table>
<thead>
<tr>
<th>Key</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>CPU 运行</td>
<td>内存 ≥ 32GB - 很慢，不推荐</td>
</tr>
<tr>
<td>低精度运行</td>
<td>内存 ≥ 8GB 显存 ≥ <strong>5GB</strong></td>
</tr>
<tr>
<td>高精度运行</td>
<td>内存 ≥ 16GB 显存 ≥ <strong>13GB</strong></td>
</tr>
</tbody></table>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/THUDM/ChatGLM3</span><br></pre></td></tr></table></figure>

<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd ChatGLM3</span><br><span class="line">$ pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240815233929736.png" alt="image-20240815233929736"></p>
<h2 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h2><blockquote>
<p>Git Large File Storage</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install git-lfs</span><br><span class="line">$ git lfs install</span><br></pre></td></tr></table></figure>

<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://huggingface.co/THUDM/chatglm3-6b</span><br><span class="line">$ du -sh chatglm3-6b/</span><br><span class="line">47G     chatglm3-6b/</span><br></pre></td></tr></table></figure>

<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><h3 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h3><blockquote>
<p>修改 basic_demo&#x2F;cli_demo.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MODEL_PATH = os.environ.get(<span class="string">&#x27;MODEL_PATH&#x27;</span>, <span class="string">&#x27;../chatglm3-6b&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd basic_demo/</span><br><span class="line">$ python3 cli_demo.py</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816012418015.png" alt="image-20240816012418015"></p>
<h3 id="Web-Console"><a href="#Web-Console" class="headerlink" title="Web Console"></a>Web Console</h3><blockquote>
<p>修改 basic_demo&#x2F;web_demo_gradio.py</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MODEL_PATH = os.environ.get(<span class="string">&#x27;MODEL_PATH&#x27;</span>, <span class="string">&#x27;../chatglm3-6b&#x27;</span>)</span><br><span class="line">...</span><br><span class="line">demo.launch(server_name=<span class="string">&quot;172.19.0.5&quot;</span>, server_port=<span class="number">7870</span>, inbrowser=<span class="literal">True</span>, share=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>python3 web_demo_gradio.py</p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816013110982.png" alt="image-20240816013110982"></p>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816013236394.png" alt="image-20240816013236394"></p>
<h3 id="Composite"><a href="#Composite" class="headerlink" title="Composite"></a>Composite</h3><blockquote>
<p>支持 <strong>Chat、Tool、Code Interpreter</strong></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cd composite_demo</span><br><span class="line">$ pip install -r requirements.txt</span><br><span class="line">$ export MODEL_PATH=../chatglm3-6b</span><br><span class="line"></span><br><span class="line">$ streamlit run main.py</span><br><span class="line">Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.</span><br><span class="line"></span><br><span class="line">  You can now view your Streamlit app in your browser.</span><br><span class="line"></span><br><span class="line">  Local URL: http://localhost:8501</span><br><span class="line">  Network URL: http://172.19.0.5:8501</span><br><span class="line">  External URL: http://43.129.218.160:8501</span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816015253345.png" alt="image-20240816015253345"></p>
<h2 id="精度"><a href="#精度" class="headerlink" title="精度"></a>精度</h2><ol>
<li>默认情况下，模型以 <strong>FP16</strong> 精度加载，大概需要 <strong>13GB 显存</strong></li>
<li>也可以通过 <strong>CPU</strong> 启动，大概需要 <strong>32GB 内存</strong></li>
<li>如果显存不足，可以在 <strong>4-bit</strong> 量化下运行，大概需要 <strong>6GB 显存</strong></li>
</ol>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816013525197.png" alt="image-20240816013525197"></p>
<h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><ol>
<li>超参数用来控制模型的<strong>推理准确度</strong></li>
<li>LLM 推理每次给的回答可能都不一样，因此 LLM 不能用于处理<strong>精确度要求很高</strong>的任务</li>
</ol>
<blockquote>
<p>ChatGLM3-6B</p>
</blockquote>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>max_length</td>
<td>模型的总 Token 限制 - 输入和输出</td>
</tr>
<tr>
<td>temperature</td>
<td>模型的温度 - 调整<strong>单词的概率分布</strong><br />在<strong>较低温度</strong>下，模型<strong>更具有确定性</strong> - <u>数字越小，给出的答案越精确</u></td>
</tr>
<tr>
<td>top_p</td>
<td>模型<strong>采样策略</strong>参数<br />每一步只从<strong>累计概率</strong>超过某个阈值 p 的最小单词集合中进行<strong>随机</strong>采样<br />不考虑其它<strong>低概率</strong>的词，只关注分布的<strong>核心</strong>部分，忽略<strong>尾部</strong></td>
</tr>
</tbody></table>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240816020235518.png" alt="image-20240816020235518"></p>
<h2 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt"></a>Prompt</h2><blockquote>
<p>在对话场景中，有且仅有三种角色</p>
</blockquote>
<table>
<thead>
<tr>
<th>Role</th>
<th>Desc</th>
</tr>
</thead>
<tbody><tr>
<td>system</td>
<td>系统信息，出现在消息的最前面，可以<strong>指定回答问题的角色</strong></td>
</tr>
<tr>
<td>user</td>
<td>我们提出的问题</td>
</tr>
<tr>
<td>assistant</td>
<td>LLM 给出的回复</td>
</tr>
</tbody></table>
<blockquote>
<p>在代码场景中，有且仅有 user、assistant、system、observation 四种角色</p>
</blockquote>
<ol>
<li>observation 是<strong>外部返回的结果</strong> - 调用外部 API、代码执行逻辑等返回结果</li>
<li><u>observation 必须放在 assistant 之后</u></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;|system|&gt;</span><br><span class="line">Answer the following questions as best as you can. You have access to the following tools:</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;get_current_weather&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;Get the current weather in a given location&quot;,</span><br><span class="line">        &quot;parameters&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;location&quot;: &#123;</span><br><span class="line">                    &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                    &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;unit&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;required&quot;: [&quot;location&quot;],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">&lt;|user|&gt;</span><br><span class="line">今天北京的天气怎么样？</span><br><span class="line">&lt;|assistant|&gt;</span><br><span class="line">好的，让我们来查看今天的天气</span><br><span class="line">&lt;|assistant|&gt;get_current_weather</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">tool_call(location=&quot;beijing&quot;, unit=&quot;celsius&quot;)</span><br><span class="line"></span><br><span class="line">&lt;|observation|&gt;</span><br><span class="line">&#123;&quot;temperature&quot;: 22&#125;</span><br><span class="line">&lt;|assistant|&gt;</span><br><span class="line">根据查询结果，今天北京的气温为 22 摄氏度。</span><br></pre></td></tr></table></figure>

<ol>
<li>当前阶段的 LLM 经过训练后，都可以<strong>遵循系统消息</strong></li>
<li><strong>系统消息</strong>不算用户对话的一部分，<strong>与用户隔离</strong></li>
<li><strong>系统消息</strong>可以控制 <strong>LLM</strong> 与<strong>用户</strong>的<strong>交互范围</strong><ul>
<li>在 system 角色指定模型充当 Java 技术专家</li>
<li>则可以指导 LLM 的输出<strong>偏向</strong>于 Java 技术范围</li>
</ul>
</li>
<li>可以<strong>防止</strong>用户进行<strong>输入注入攻击</strong><ul>
<li>在进行多轮对话时，每次新的对话都会把<strong>历史对话</strong>带进去</li>
<li>如果在前面的对话中，告诉 LLM <strong>错误的提示</strong><ul>
<li>那么这些错误的提示会在后续的对话中被当成<strong>正确的上下文</strong>带进去</li>
</ul>
</li>
<li>基于<strong>自回归</strong>的模型，会根据<strong>上下文</strong>进行<strong>内容推理</strong>，因此可能会生成<strong>错误内容</strong></li>
<li>角色可以<u>使内容更加容易区分</u>，<strong>增加注入攻击的复杂度</strong></li>
<li><strong>只能尽量减少，无法完全避免</strong></li>
</ul>
</li>
</ol>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top">zhongmingmao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top/2024/06/28/llm-deploy-chatglm3-6b/">https://blog.zhongmingmao.top/2024/06/28/llm-deploy-chatglm3-6b/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ai/">AI</a><a class="post-meta__tags" href="/tags/llm/">LLM</a><a class="post-meta__tags" href="/tags/nlp/">NLP</a><a class="post-meta__tags" href="/tags/langchain/">LangChain</a><a class="post-meta__tags" href="/tags/ai-agent/">AI Agent</a><a class="post-meta__tags" href="/tags/rag/">RAG</a></div><div class="post-share"><div class="social-share" data-image="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/06/29/llm-peft-chatglm3-6b-lora/" title="LLM PEFT - ChatGLM3-6B + LoRA"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">LLM PEFT - ChatGLM3-6B + LoRA</div></div><div class="info-2"><div class="info-item-1">通用 LLM 千亿大模型（130B、ChatGPT）和小规模的大模型（6B、LLaMA2）都是通用 LLM   通用 LLM 都是通过常识进行预训练的 在实际使用过程中，需要 LLM 具备某一特定领域知识的能力 - 对 LLM 的能力进行增强    增强方式   Method Desc    微调 让预先训练好的 LLM 适应特定任务或数据集的方案，成本相对低LLM 学会训练者提供的微调数据，并具备一定的理解能力   知识库 使用向量数据库或者其它数据库存储数据，为 LLM 提供信息来源外挂   API 与知识库类似，为 LLM 提供信息来源外挂    互不冲突，可以同时使用几种方案来优化 LLM，提升内容输出能力   LoRA &#x2F; QLoRA &#x2F; 知识库 &#x2F; API    LLM Performance &#x3D; 推理效果  落地过程   Method Pipeline    微调 准备数据 -&gt; 微调 -&gt; 验证 -&gt; 提供服务   知识库 准备数据 -&gt; 构建向量库 -&gt; 构建智能体 -&gt; 提供服务   API 准备数据 -&g...</div></div></div></a><a class="pagination-related" href="/2024/06/27/llm-langchain-rag/" title="LLM - LangChain + RAG"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">LLM - LangChain + RAG</div></div><div class="info-2"><div class="info-item-1">局限 大模型的核心能力 - 意图理解 + 文本生成     局限 描述    数据的及时性 大部分 AI 大模型都是预训练的，如果要问一些最新的消息，大模型是不知道的   复杂任务处理 AI 大模型在问答方面表现出色，但不总是能够处理复杂任务AI 大模型主要是基于文本的交互（多模态除外）   代码生成与下载 根据需求描述生成对应的代码，并提供下载链接 - 暂时不支持   与企业应用场景的集成 读取关系型数据库里面的数据，并根据提示进行任务处理 - 暂时不支持    在实际应用过程中，输入数据和输出数据，不仅仅是纯文本 AI Agent - 需要解析用户的输入输出    AI Agent AI Agent 是以 LLM 为核心控制器的一套代理系统    控制端处于核心地位，承担记忆、思考以及决策等基础工作 感知模块负责接收和处理来自于外部环境的多样化信息 - 文字、声音、图片、位置等 行动模块通过生成文本、API 调用、使用工具等方式来执行任务以及改变环境   LangChain - 开源 + 提供一整套围绕 LLM 的 Agent 工具   AI Agent 很有可能在未来一段时间内成为 AI 发展的一...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/06/30/llm-rag-chatglm3-6b-langchain-faiss/" title="LLM RAG - ChatGLM3-6B + LangChain + Faiss"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-30</div><div class="info-item-2">LLM RAG - ChatGLM3-6B + LangChain + Faiss</div></div><div class="info-2"><div class="info-item-1">RAG 使用知识库，用来增强 LLM 信息检索的能力   知识准确 先把知识进行向量化，存储到向量数据库中 使用的时候通过向量检索从向量数据库中将知识检索出来，确保知识的准确性   更新频率快 当发现知识库里面的知识不全时，可以随时补充 不需要像微调一样，重新跑微调任务、验证结果、重新部署等      应用场景 ChatOps   知识库模式适用于相对固定的场景做推理 如企业内部使用的员工小助手，不需要太多的逻辑推理 使用知识库模式检索精度高，且可以随时更新 LLM 基础能力 + Agent 进行堆叠，可以产生智能化的效果  LangChain-Chatchat组成模块   模块 作用 支持列表    大语言模型 智能体核心引擎 ChatGLM &#x2F; Qwen &#x2F; Baichuan &#x2F; LLaMa   Embedding 模型 文本向量化 m3e-* &#x2F; bge-*   分词器 按照规则将句子分成短句或者单词 LangChain Text Splitter   向量数据库 向量化数据存储 Faiss &#x2F; Milvus   Agent Tools 调用第三方...</div></div></div></a><a class="pagination-related" href="/2024/06/29/llm-peft-chatglm3-6b-lora/" title="LLM PEFT - ChatGLM3-6B + LoRA"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-29</div><div class="info-item-2">LLM PEFT - ChatGLM3-6B + LoRA</div></div><div class="info-2"><div class="info-item-1">通用 LLM 千亿大模型（130B、ChatGPT）和小规模的大模型（6B、LLaMA2）都是通用 LLM   通用 LLM 都是通过常识进行预训练的 在实际使用过程中，需要 LLM 具备某一特定领域知识的能力 - 对 LLM 的能力进行增强    增强方式   Method Desc    微调 让预先训练好的 LLM 适应特定任务或数据集的方案，成本相对低LLM 学会训练者提供的微调数据，并具备一定的理解能力   知识库 使用向量数据库或者其它数据库存储数据，为 LLM 提供信息来源外挂   API 与知识库类似，为 LLM 提供信息来源外挂    互不冲突，可以同时使用几种方案来优化 LLM，提升内容输出能力   LoRA &#x2F; QLoRA &#x2F; 知识库 &#x2F; API    LLM Performance &#x3D; 推理效果  落地过程   Method Pipeline    微调 准备数据 -&gt; 微调 -&gt; 验证 -&gt; 提供服务   知识库 准备数据 -&gt; 构建向量库 -&gt; 构建智能体 -&gt; 提供服务   API 准备数据 -&g...</div></div></div></a><a class="pagination-related" href="/2024/06/27/llm-langchain-rag/" title="LLM - LangChain + RAG"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-27</div><div class="info-item-2">LLM - LangChain + RAG</div></div><div class="info-2"><div class="info-item-1">局限 大模型的核心能力 - 意图理解 + 文本生成     局限 描述    数据的及时性 大部分 AI 大模型都是预训练的，如果要问一些最新的消息，大模型是不知道的   复杂任务处理 AI 大模型在问答方面表现出色，但不总是能够处理复杂任务AI 大模型主要是基于文本的交互（多模态除外）   代码生成与下载 根据需求描述生成对应的代码，并提供下载链接 - 暂时不支持   与企业应用场景的集成 读取关系型数据库里面的数据，并根据提示进行任务处理 - 暂时不支持    在实际应用过程中，输入数据和输出数据，不仅仅是纯文本 AI Agent - 需要解析用户的输入输出    AI Agent AI Agent 是以 LLM 为核心控制器的一套代理系统    控制端处于核心地位，承担记忆、思考以及决策等基础工作 感知模块负责接收和处理来自于外部环境的多样化信息 - 文字、声音、图片、位置等 行动模块通过生成文本、API 调用、使用工具等方式来执行任务以及改变环境   LangChain - 开源 + 提供一整套围绕 LLM 的 Agent 工具   AI Agent 很有可能在未来一段时间内成为 AI 发展的一...</div></div></div></a><a class="pagination-related" href="/2024/08/04/rag-langchain/" title="RAG - LangChain"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-agent.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-04</div><div class="info-item-2">RAG - LangChain</div></div><div class="info-2"><div class="info-item-1">Practice   LangChain RAG https://github.com/langchain-ai/rag-from-scratch   RAG 如何随着长期 LLM 而改变 Is RAG Really Dead? https://www.youtube.com/watch?v=SsHUNfhF32s     自适应 RAG 根据复杂程度动态地将查询路由到不同的 RAG 方法 - Command-R @ LangGraph Adaptive RAG https://www.youtube.com/watch?v=04ighIjMcAI   Code https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb   Paper https://arxiv.org/abs/2403.14403     Adaptive RAG 在循环单元测试中自我纠正检索错误，以确定文档相关性并返回到网络搜索 在 LangGraph 中实现 Mistral 7B + Ol...</div></div></div></a><a class="pagination-related" href="/2024/09/09/rag-vector-qdrant/" title="RAG - Qdrant"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/qdrant.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="info-item-2">RAG - Qdrant</div></div><div class="info-2"><div class="info-item-1">Features   Getting StartedIntroduction Vector databases are a relatively new way for interacting with abstract data representations derived from opaque machine learning models such as deep learning architectures. These representations are often called vectors or embeddings and they are a compressed version of the data used to train a machine learning model to accomplish a task like sentiment analysis, speech recognition, object detection, and many others.  What is Qdrant? Qdrant “is a vector similarity s...</div></div></div></a><a class="pagination-related" href="/2024/08/17/rag-graphrag/" title="RAG - GraphRAG"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/graphrag-9781829.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-17</div><div class="info-item-2">RAG - GraphRAG</div></div><div class="info-2"><div class="info-item-1">向量检索 信息片段之间的连接能力有限   RAG 在跨越多个信息片段以获取综合见解时表现不足 当要回答一个复杂问题时，必须要通过共享属性在不同信息之间建立联系 RAG 无法有效捕捉这些关系 限制了 RAG 在处理需要多跳推理或整合多源数据的复杂查询时的能力       归纳总结能力不足   在处理大型数据集或长文档时，RAG 难以有效地归纳和总结复杂的语义概念 RAG 在需要全面理解和总结复杂语义信息的场景中表现不佳  GraphRAG 利用 LLM 生成的知识图谱来改进 RAG 的检索部分   GraphRAG 利用结构化的实体和关系信息，使得检索过程更加精准和全面 GraphRAG 在处理多跳问题和复杂文档分析时表现出色 GraphRAG 在处理复杂信息处理任务时，显著提升问答性能，提供比 RAG 更为准确和全面的答案 GraphRAG 通过知识图谱有效地连接不同的信息片段 不仅能够提供准确答案，还能展示答案之间的内在联系，提供更丰富和有价值的结果     GraphRAG 先利用知识图谱，关联查询的实体和关系从与知识图谱实体直接相关的文档中检索片段，提供一个更全面、指标化、高信息密度的总结   主...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zhongmingmao</div><div class="author-info-description">Focus on Infrastructure.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">641</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhongmingmao0625@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Things are always unexpected!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LLM-%E9%80%89%E6%8B%A9"><span class="toc-number">1.</span> <span class="toc-text">LLM 选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%8E%A9%E5%AE%B6"><span class="toc-number">1.1.</span> <span class="toc-text">核心玩家</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ChatGLM-6B"><span class="toc-number">1.2.</span> <span class="toc-text">ChatGLM-6B</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90"><span class="toc-number">2.</span> <span class="toc-text">计算资源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ChatGLM3-6B"><span class="toc-number">3.</span> <span class="toc-text">ChatGLM3-6B</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">3.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-number">3.2.</span> <span class="toc-text">环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">3.3.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96"><span class="toc-number">3.4.</span> <span class="toc-text">依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Git"><span class="toc-number">3.5.</span> <span class="toc-text">Git</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.6.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="toc-number">3.7.</span> <span class="toc-text">启动</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C"><span class="toc-number">3.7.1.</span> <span class="toc-text">命令行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Web-Console"><span class="toc-number">3.7.2.</span> <span class="toc-text">Web Console</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Composite"><span class="toc-number">3.7.3.</span> <span class="toc-text">Composite</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B2%BE%E5%BA%A6"><span class="toc-number">3.8.</span> <span class="toc-text">精度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">3.9.</span> <span class="toc-text">超参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prompt"><span class="toc-number">3.10.</span> <span class="toc-text">Prompt</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/23/cloud-native-observability-prometheus-concepts/" title="Observability - Prometheus Concepts"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus-concepts.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Concepts"/></a><div class="content"><a class="title" href="/2025/01/23/cloud-native-observability-prometheus-concepts/" title="Observability - Prometheus Concepts">Observability - Prometheus Concepts</a><time datetime="2025-01-22T16:06:25.000Z" title="Created 2025-01-23 00:06:25">2025-01-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Introduction"/></a><div class="content"><a class="title" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction">Observability - Prometheus Introduction</a><time datetime="2025-01-21T16:06:25.000Z" title="Created 2025-01-22 00:06:25">2025-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java-agent.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java Zero Code"/></a><div class="content"><a class="title" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code">Observability - OpenTelemetry Java Zero Code</a><time datetime="2025-01-20T16:06:25.000Z" title="Created 2025-01-21 00:06:25">2025-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java"/></a><div class="content"><a class="title" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java">Observability - OpenTelemetry Java</a><time datetime="2025-01-19T16:06:25.000Z" title="Created 2025-01-20 00:06:25">2025-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/mcp.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - MCP Overview"/></a><div class="content"><a class="title" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview">AI Agent - MCP Overview</a><time datetime="2025-01-18T16:06:25.000Z" title="Created 2025-01-19 00:06:25">2025-01-19</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2015 - 2025 By zhongmingmao</span></div><div class="footer_custom_text">Life is like a box of chocolates. You can't know what you'll eat until you open it.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div></body></html>