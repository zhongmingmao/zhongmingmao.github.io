<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM - LangChain + RAG | ByteCoding</title><meta name="author" content="zhongmingmao"><meta name="copyright" content="zhongmingmao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="局限 大模型的核心能力 - 意图理解 + 文本生成     局限 描述    数据的及时性 大部分 AI 大模型都是预训练的，如果要问一些最新的消息，大模型是不知道的   复杂任务处理 AI 大模型在问答方面表现出色，但不总是能够处理复杂任务AI 大模型主要是基于文本的交互（多模态除外）   代码生成与下载 根据需求描述生成对应的代码，并提供下载链接 - 暂时不支持   与企业应用场景的集成 读取">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM - LangChain + RAG">
<meta property="og:url" content="https://blog.zhongmingmao.top/2024/06/27/llm-langchain-rag/index.html">
<meta property="og:site_name" content="ByteCoding">
<meta property="og:description" content="局限 大模型的核心能力 - 意图理解 + 文本生成     局限 描述    数据的及时性 大部分 AI 大模型都是预训练的，如果要问一些最新的消息，大模型是不知道的   复杂任务处理 AI 大模型在问答方面表现出色，但不总是能够处理复杂任务AI 大模型主要是基于文本的交互（多模态除外）   代码生成与下载 根据需求描述生成对应的代码，并提供下载链接 - 暂时不支持   与企业应用场景的集成 读取">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp">
<meta property="article:published_time" content="2024-06-26T16:06:25.000Z">
<meta property="article:modified_time" content="2024-08-14T12:09:09.499Z">
<meta property="article:author" content="zhongmingmao">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="LangChain">
<meta property="article:tag" content="AI Agent">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM - LangChain + RAG",
  "url": "https://blog.zhongmingmao.top/2024/06/27/llm-langchain-rag/",
  "image": "https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp",
  "datePublished": "2024-06-26T16:06:25.000Z",
  "dateModified": "2024-08-14T12:09:09.499Z",
  "author": [
    {
      "@type": "Person",
      "name": "zhongmingmao",
      "url": "https://blog.zhongmingmao.top"
    }
  ]
}</script><link rel="shortcut icon" href="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png"><link rel="canonical" href="https://blog.zhongmingmao.top/2024/06/27/llm-langchain-rag/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":32,"languages":{"author":"Author: zhongmingmao","link":"Link: ","source":"Source: ByteCoding","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM - LangChain + RAG',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ByteCoding" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/url(https:/cdn.pixabay.com/photo/2021/07/20/03/39/fisherman-6479663_1280.jpg));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">640</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ByteCoding</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM - LangChain + RAG</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM - LangChain + RAG</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2024-06-26T16:06:25.000Z" title="Created 2024-06-27 00:06:25">2024-06-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/llm/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">1.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>5mins</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:512,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2024-08-14 20:09:09&quot;}" hidden></div><h1 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h1><blockquote>
<p>大模型的核心能力 - <strong>意图理解</strong> + <strong>文本生成</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>局限</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>数据的<strong>及时性</strong></td>
<td>大部分 AI 大模型都是<strong>预训练</strong>的，如果要问一些最新的消息，大模型是不知道的</td>
</tr>
<tr>
<td><strong>复杂任务</strong>处理</td>
<td>AI 大模型在<strong>问答</strong>方面表现出色，但不总是能够处理复杂任务<br />AI 大模型主要是基于<strong>文本</strong>的交互（<strong>多模态</strong>除外）</td>
</tr>
<tr>
<td><strong>代码</strong>生成与下载</td>
<td>根据需求描述生成对应的代码，并提供下载链接 - 暂时不支持</td>
</tr>
<tr>
<td>与<strong>企业应用</strong>场景的<strong>集成</strong></td>
<td>读取关系型数据库里面的数据，并根据提示进行任务处理 - 暂时不支持</td>
</tr>
</tbody></table>
<ol>
<li>在实际应用过程中，输入数据和输出数据，不仅仅是纯文本</li>
<li><strong>AI Agent</strong> - 需要<strong>解析</strong>用户的输入输出</li>
</ol>
<span id="more"></span>

<h1 id="AI-Agent"><a href="#AI-Agent" class="headerlink" title="AI Agent"></a>AI Agent</h1><blockquote>
<p>AI Agent 是以 LLM 为<strong>核心控制器</strong>的一套<strong>代理</strong>系统</p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/b6651abc9df589fe238fe79e33677a36.png.webp" alt="b6651abc9df589fe238fe79e33677a36.png"></p>
<ol>
<li>控制端处于<strong>核心</strong>地位，承担<strong>记忆</strong>、<strong>思考</strong>以及<strong>决策</strong>等基础工作</li>
<li>感知模块负责<strong>接收</strong>和<strong>处理</strong>来自于外部环境的多样化信息 - 文字、声音、图片、位置等</li>
<li>行动模块通过生成文本、API 调用、使用工具等方式来<strong>执行任务</strong>以及<strong>改变环境</strong></li>
</ol>
<blockquote>
<p>LangChain - <strong>开源</strong> + 提供一整套围绕 LLM 的 <strong>Agent 工具</strong></p>
</blockquote>
<ol>
<li>AI Agent 很有可能在未来一段时间内成为 AI 发展的一个重要方向</li>
<li><strong>LLM</strong> 是<strong>大厂的游戏</strong> - 除非能开发出能够<strong>低成本训练和推理</strong>的 LLM</li>
<li><strong>AI Agent</strong> 是<strong>普通玩家</strong>可以入局的</li>
</ol>
<h1 id="LangChain"><a href="#LangChain" class="headerlink" title="LangChain"></a>LangChain</h1><ol>
<li>一开始，LangChain 只是一个<strong>技术框架</strong>，可以快速开发 AI 应用<ul>
<li>不需要储备太多算法知识，只需要知道如何与 LLM 进行交互即可 - API 接口和参数</li>
</ul>
</li>
<li>至今，LangChain 成为了一个 <strong>AI 应用开发平台</strong>，包含 4 大组件</li>
</ol>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/star-history-2024814.png" alt="star-history-2024814"></p>
<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><blockquote>
<p>LangChain 是当下<strong>最火</strong>的 AI Agent 技术框架，使用 <strong>MIT</strong> 开源协议</p>
</blockquote>
<table>
<thead>
<tr>
<th>Component</th>
<th>Desc</th>
<th>≈</th>
</tr>
</thead>
<tbody><tr>
<td>LangChain</td>
<td>LLM <strong>应用开发框架</strong></td>
<td>SpringCloud</td>
</tr>
<tr>
<td>LangSmith</td>
<td>统一的 <strong>DevOps</strong> 平台<br />也是一套 <strong>Agent DevOps 规范</strong>，可以应用于其它框架</td>
<td>K8S + GitLab CI&#x2F;CD + …</td>
</tr>
<tr>
<td>LangServe</td>
<td>用于<strong>部署</strong> LangChain 应用程序，并提供 <strong>API 管理能力</strong></td>
<td>APISIX</td>
</tr>
<tr>
<td>LangGraph</td>
<td>用于使用 LLM 构建<strong>有状态</strong>、<strong>多参与者</strong>应用程序的库</td>
<td>Nacos</td>
</tr>
</tbody></table>
<h2 id="技术架构"><a href="#技术架构" class="headerlink" title="技术架构"></a>技术架构</h2><table>
<thead>
<tr>
<th>Module</th>
<th>Desc</th>
</tr>
</thead>
<tbody><tr>
<td>LangChain-Core</td>
<td>基础抽象 + LangChain 表达式语言</td>
</tr>
<tr>
<td>LangChain-Community</td>
<td>第三方集成</td>
</tr>
<tr>
<td>LangChain</td>
<td>构成应用程序认知架构的链、代理、检索策略</td>
</tr>
</tbody></table>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240814150218300.png" alt="image-20240814150218300"></p>
<h3 id="Model-I-O"><a href="#Model-I-O" class="headerlink" title="Model I&#x2F;O"></a>Model I&#x2F;O</h3><blockquote>
<p>主要与 <strong>LLM</strong> 打交道</p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240814150744886.png" alt="image-20240814150744886"></p>
<ol>
<li>主要组成部分：<strong>Format + Predict + Parse</strong></li>
<li>LLM 可以理解为只接受<strong>文本输入</strong>和<strong>文本输出</strong>的模型</li>
<li>LangChain 中的 LLM 是<strong>纯文本补全</strong>模型，不包含<strong>多模态</strong>模型</li>
</ol>
<h4 id="Format"><a href="#Format" class="headerlink" title="Format"></a>Format</h4><ol>
<li>在把数据输入到 LLM 之前，无论数据来自于搜索引擎、<strong>向量数据库</strong>还是接口</li>
<li>都必须先对数据进行格式化，转换成 LLM 能<strong>理解</strong>的格式</li>
</ol>
<h4 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h4><ol>
<li>LangChain <strong>原生支持</strong>的丰富 <strong>API</strong>，可以实现对各个 LLM 的调用</li>
</ol>
<h4 id="Parse"><a href="#Parse" class="headerlink" title="Parse"></a>Parse</h4><ol>
<li>对 LLM 返回的文本内容进行解析</li>
<li>随着<strong>多模态</strong>模型的日益成熟，未来会实现对多模态模型输出结构的解析</li>
</ol>
<h3 id="Retrieval"><a href="#Retrieval" class="headerlink" title="Retrieval"></a>Retrieval</h3><blockquote>
<p>从各种数据源中<strong>抓取数据</strong>，进行<strong>词向量化</strong>、<strong>向量数据存储</strong>、<strong>向量数据检索</strong></p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240814153637472.png" alt="image-20240814153637472"></p>
<h3 id="Agents"><a href="#Agents" class="headerlink" title="Agents"></a>Agents</h3><ol>
<li>Agents 是指实现<strong>具体任务</strong>的模块，如从某个第三方接口获取数据，用作 LLM 的输入</li>
<li>LangChain <strong>原生支持</strong>多种类型的 Agents，可以根据实际需要进行<strong>自定义</strong></li>
</ol>
<h3 id="Chains"><a href="#Chains" class="headerlink" title="Chains"></a>Chains</h3><ol>
<li>Chains 为<strong>顺序调用</strong>，类似于 <strong>Linux Pipelines</strong> - 文件处理链条、SQL 查询链条、搜索链条</li>
<li>LangChain Chains <strong>主要</strong>通过 <strong>LangChain 表达式</strong>实现 - <strong>LCEL</strong> - LangChain Expression Language</li>
<li>一些底层的 Chains 并没有通过 LCEL 实现，而是通过 <strong>LegacyChain</strong></li>
</ol>
<h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240814192502410.png" alt="image-20240814192502410"></p>
<ol>
<li>Memory 指 LLM 的一些输入和输出，包含<strong>历史</strong>对话信息，可以放入<strong>缓存</strong>，提升<strong>性能</strong></li>
<li>使用流程<ul>
<li>在执行核心逻辑之前先查询缓存，如果查询到则直接使用</li>
<li>在执行完核心逻辑，返回给用户前，将内容写入缓存，方便后面使用</li>
</ul>
</li>
</ol>
<h3 id="Callbacks"><a href="#Callbacks" class="headerlink" title="Callbacks"></a>Callbacks</h3><blockquote>
<p>LangChain 针对各个<strong>组件</strong>提供<strong>回调</strong>机制 - 链条、模型、代理、工具等</p>
</blockquote>
<table>
<thead>
<tr>
<th>Callback</th>
<th>Desc</th>
</tr>
</thead>
<tbody><tr>
<td><strong>构造函数</strong>回调</td>
<td>只适用于<strong>对象本身</strong></td>
</tr>
<tr>
<td><strong>请求</strong>回调</td>
<td>适用于<strong>对象本身</strong>以及其<strong>所有子对象</strong></td>
</tr>
</tbody></table>
<h3 id="LCEL"><a href="#LCEL" class="headerlink" title="LCEL"></a>LCEL</h3><blockquote>
<p>LCEL 用于<strong>构建 Chains</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;tell me a short joke about &#123;topic&#125;&quot;</span>)</span><br><span class="line">model = ChatOpenAI(model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">output_parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line">chain = prompt | model | output_parser</span><br><span class="line"></span><br><span class="line">chain.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;ice cream&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>

<ol>
<li>Chains 与 Linux Pipelines 类似，通过 <code>|</code> 来连接不同组件，构成复杂 Chains，以实现特定功能</li>
<li>每个组件的输出会作为下一个组件的输入，直到最后一个组件执行完</li>
</ol>
<blockquote>
<p>可以通过 LCEL 将多个 Chains 关联在一起</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">chain1 = prompt1 | model | StrOutputParser()</span><br><span class="line">chain2 = (</span><br><span class="line">    &#123;<span class="string">&quot;city&quot;</span>: chain1, <span class="string">&quot;language&quot;</span>: itemgetter(<span class="string">&quot;language&quot;</span>)&#125;</span><br><span class="line">    | prompt2</span><br><span class="line">    | model</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line">chain2.invoke(&#123;<span class="string">&quot;person&quot;</span>: <span class="string">&quot;obama&quot;</span>, <span class="string">&quot;language&quot;</span>: <span class="string">&quot;spanish&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>

<h2 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h2><ol>
<li><strong>LLM</strong> 是<strong>核心控制器</strong>，所有的操作都是围绕 LLM 的输入和输出在进行</li>
<li><strong>Chains</strong> 可以将一系列组件串起来进行功能叠加 - <strong>逻辑抽象</strong> + <strong>组件复用</strong></li>
</ol>
<h1 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h1><h2 id="知识滞后"><a href="#知识滞后" class="headerlink" title="知识滞后"></a>知识滞后</h2><ol>
<li>LLM 是基于<strong>预训练</strong>的，一般 LLM 的训练周期为 1~3 个月</li>
<li>因为<strong>成本</strong>过高，所以 LLM 注定不可能频繁更新知识 - <strong>知识滞后</strong> - GPT-4 为 2023.12</li>
<li>如果想 LLM 能够<strong>理解</strong>数据，或者将数据喂给 LLM 做<strong>推理</strong>时，必须进行 <strong>RAG</strong></li>
</ol>
<h2 id="技术流程"><a href="#技术流程" class="headerlink" title="技术流程"></a>技术流程</h2><blockquote>
<p>通过 RAG 让 LLM 支持<strong>最新</strong>的知识索引 - 使用最新的知识进行<strong>推理</strong></p>
</blockquote>
<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240814194840237.png" alt="image-20240814194840237"></p>
<ol>
<li>任务 1<ul>
<li>通过网络爬虫，爬取大量信息</li>
<li>与搜索引擎的数据爬取过程一样，但不涉及 Page Rank</li>
<li>只是纯粹的<strong>知识爬取</strong>，并<strong>向量化存储</strong> - 最新数据</li>
</ul>
</li>
<li>任务 2<ul>
<li>用户提问时，先将<strong>问题向量化</strong></li>
<li>然后在<strong>向量库</strong>里检索，将检索到的信息构建成<strong>提示</strong>，喂给 LLM，LLM 处理完后进行输出</li>
</ul>
</li>
</ol>
<blockquote>
<p>也可以<strong>实时调用</strong>搜索引擎的接口，获取最新数据，然后<strong>向量化</strong>后喂给 LLM</p>
</blockquote>
<h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><ol>
<li>向量化 - 将<strong>语言</strong>通过<strong>数学</strong>的方式进行表达</li>
<li>生成的<strong>向量数据</strong>取决于使用的 <strong>Embedding</strong> 模型</li>
</ol>
<h3 id="向量存储"><a href="#向量存储" class="headerlink" title="向量存储"></a>向量存储</h3><ol>
<li>将向量化后的数据存储在向量数据库中</li>
<li>常见的向量数据库 - <strong>Faiss + Milvus</strong></li>
</ol>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top">zhongmingmao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top/2024/06/27/llm-langchain-rag/">https://blog.zhongmingmao.top/2024/06/27/llm-langchain-rag/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ai/">AI</a><a class="post-meta__tags" href="/tags/llm/">LLM</a><a class="post-meta__tags" href="/tags/nlp/">NLP</a><a class="post-meta__tags" href="/tags/langchain/">LangChain</a><a class="post-meta__tags" href="/tags/ai-agent/">AI Agent</a><a class="post-meta__tags" href="/tags/rag/">RAG</a></div><div class="post-share"><div class="social-share" data-image="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.webp" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/06/28/llm-deploy-chatglm3-6b/" title="LLM Deploy - ChatGLM3-6B"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">LLM Deploy - ChatGLM3-6B</div></div><div class="info-2"><div class="info-item-1">LLM 选择核心玩家 厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 LLaMA 套壳     ChatGLM-6B ChatGLM-6B 和 LLaMA2 是比较热门的开源项目，国产 LLM 是首选 企业布局 LLM 选择 MaaS 服务，调用大厂 LLM API，但会面临数据安全问题 选择开源 LLM，自己微调、部署、为上层应用提供服务   企业一般会选择私有化部署 + 公有云 MaaS 的混合架构 在国产厂商中，从技术角度来看，智谱 AI 是国内 LLM 研发水平最高的厂商 6B 的参数规模为 62 亿，单张 3090 显卡就可以进行微调和推理 企业预算充足（百万以上，GPU 费用 + 商业授权） 可以尝试 GLM-130B，千亿参数规模，推理能力更强 GLM-130B 轻量化后，可以在 3090 × 4 上进行推理 训练 GLM-130B 大概需要 96 台 A100（320G），历时两个多月     计算资源 适合 CPU 计算的 LLM 不多 有些 LLM 可以在 CPU 上进行推理，但需要使用低精度轻量化的 LLM 但在低精度下，LLM 会失真，效果较差   要真正体验并应...</div></div></div></a><a class="pagination-related" href="/2024/06/26/llm-prompt/" title="LLM - Prompt"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/llm-prompt.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">LLM - Prompt</div></div><div class="info-2"><div class="info-item-1">Prompt 是否充分使用好 AI 大模型，提示是关键 OpenAI   question &#x2F; answer prompt &#x2F; completion - 给 LLM 一个提示，让 LLM 进行补全   LLM 训练原理 GPT 系列模型基于 Transformer 架构的解码器机制，使用自回归无监督方式进行预训练 训练过程 - 大量的文本输入，不断进行记忆 相比于监督学习，训练效率更低，但训练过程简单，可以喂大量的文本语料，上限比较高   completion 根据训练过的记忆，一个字一个字地计算概率，取概率最大的那个字进行输出 因此有人吐槽 LLM 输出很慢 - 逐字计算并输出      Prompt Engineering 需求描述越详细越准确，LLM 输出的内容就越符合要求 Prompt Engineering 是一门专门研究与 LLM 交互的新型学科 通过不断地开发和优化，帮助用户更好地了解 LLM 的能力和局限性   探讨如何设计出最佳提示，用于指导 LLM 帮助我们高效完成某项任务 不仅仅是设计和研发提示，还包含了与 LLM 交互的各种技能和技术 在实现与 LLM 交互、...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/06/30/llm-rag-chatglm3-6b-langchain-faiss/" title="LLM RAG - ChatGLM3-6B + LangChain + Faiss"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/rag.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-30</div><div class="info-item-2">LLM RAG - ChatGLM3-6B + LangChain + Faiss</div></div><div class="info-2"><div class="info-item-1">RAG 使用知识库，用来增强 LLM 信息检索的能力   知识准确 先把知识进行向量化，存储到向量数据库中 使用的时候通过向量检索从向量数据库中将知识检索出来，确保知识的准确性   更新频率快 当发现知识库里面的知识不全时，可以随时补充 不需要像微调一样，重新跑微调任务、验证结果、重新部署等      应用场景 ChatOps   知识库模式适用于相对固定的场景做推理 如企业内部使用的员工小助手，不需要太多的逻辑推理 使用知识库模式检索精度高，且可以随时更新 LLM 基础能力 + Agent 进行堆叠，可以产生智能化的效果  LangChain-Chatchat组成模块   模块 作用 支持列表    大语言模型 智能体核心引擎 ChatGLM &#x2F; Qwen &#x2F; Baichuan &#x2F; LLaMa   Embedding 模型 文本向量化 m3e-* &#x2F; bge-*   分词器 按照规则将句子分成短句或者单词 LangChain Text Splitter   向量数据库 向量化数据存储 Faiss &#x2F; Milvus   Agent Tools 调用第三方...</div></div></div></a><a class="pagination-related" href="/2024/06/29/llm-peft-chatglm3-6b-lora/" title="LLM PEFT - ChatGLM3-6B + LoRA"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/lora.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-29</div><div class="info-item-2">LLM PEFT - ChatGLM3-6B + LoRA</div></div><div class="info-2"><div class="info-item-1">通用 LLM 千亿大模型（130B、ChatGPT）和小规模的大模型（6B、LLaMA2）都是通用 LLM   通用 LLM 都是通过常识进行预训练的 在实际使用过程中，需要 LLM 具备某一特定领域知识的能力 - 对 LLM 的能力进行增强    增强方式   Method Desc    微调 让预先训练好的 LLM 适应特定任务或数据集的方案，成本相对低LLM 学会训练者提供的微调数据，并具备一定的理解能力   知识库 使用向量数据库或者其它数据库存储数据，为 LLM 提供信息来源外挂   API 与知识库类似，为 LLM 提供信息来源外挂    互不冲突，可以同时使用几种方案来优化 LLM，提升内容输出能力   LoRA &#x2F; QLoRA &#x2F; 知识库 &#x2F; API    LLM Performance &#x3D; 推理效果  落地过程   Method Pipeline    微调 准备数据 -&gt; 微调 -&gt; 验证 -&gt; 提供服务   知识库 准备数据 -&gt; 构建向量库 -&gt; 构建智能体 -&gt; 提供服务   API 准备数据 -&g...</div></div></div></a><a class="pagination-related" href="/2024/06/28/llm-deploy-chatglm3-6b/" title="LLM Deploy - ChatGLM3-6B"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/chatglm3-6b.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-28</div><div class="info-item-2">LLM Deploy - ChatGLM3-6B</div></div><div class="info-2"><div class="info-item-1">LLM 选择核心玩家 厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 LLaMA 套壳     ChatGLM-6B ChatGLM-6B 和 LLaMA2 是比较热门的开源项目，国产 LLM 是首选 企业布局 LLM 选择 MaaS 服务，调用大厂 LLM API，但会面临数据安全问题 选择开源 LLM，自己微调、部署、为上层应用提供服务   企业一般会选择私有化部署 + 公有云 MaaS 的混合架构 在国产厂商中，从技术角度来看，智谱 AI 是国内 LLM 研发水平最高的厂商 6B 的参数规模为 62 亿，单张 3090 显卡就可以进行微调和推理 企业预算充足（百万以上，GPU 费用 + 商业授权） 可以尝试 GLM-130B，千亿参数规模，推理能力更强 GLM-130B 轻量化后，可以在 3090 × 4 上进行推理 训练 GLM-130B 大概需要 96 台 A100（320G），历时两个多月     计算资源 适合 CPU 计算的 LLM 不多 有些 LLM 可以在 CPU 上进行推理，但需要使用低精度轻量化的 LLM 但在低精度下，LLM 会失真，效果较差   要真正体验并应...</div></div></div></a><a class="pagination-related" href="/2024/08/04/rag-langchain/" title="RAG - LangChain"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-agent.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-04</div><div class="info-item-2">RAG - LangChain</div></div><div class="info-2"><div class="info-item-1">Practice   LangChain RAG https://github.com/langchain-ai/rag-from-scratch   RAG 如何随着长期 LLM 而改变 Is RAG Really Dead? https://www.youtube.com/watch?v=SsHUNfhF32s     自适应 RAG 根据复杂程度动态地将查询路由到不同的 RAG 方法 - Command-R @ LangGraph Adaptive RAG https://www.youtube.com/watch?v=04ighIjMcAI   Code https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb   Paper https://arxiv.org/abs/2403.14403     Adaptive RAG 在循环单元测试中自我纠正检索错误，以确定文档相关性并返回到网络搜索 在 LangGraph 中实现 Mistral 7B + Ol...</div></div></div></a><a class="pagination-related" href="/2024/09/09/rag-vector-qdrant/" title="RAG - Qdrant"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/qdrant.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="info-item-2">RAG - Qdrant</div></div><div class="info-2"><div class="info-item-1">Features   Getting StartedIntroduction Vector databases are a relatively new way for interacting with abstract data representations derived from opaque machine learning models such as deep learning architectures. These representations are often called vectors or embeddings and they are a compressed version of the data used to train a machine learning model to accomplish a task like sentiment analysis, speech recognition, object detection, and many others.  What is Qdrant? Qdrant “is a vector similarity s...</div></div></div></a><a class="pagination-related" href="/2024/08/17/rag-graphrag/" title="RAG - GraphRAG"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/graphrag-9781829.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-17</div><div class="info-item-2">RAG - GraphRAG</div></div><div class="info-2"><div class="info-item-1">向量检索 信息片段之间的连接能力有限   RAG 在跨越多个信息片段以获取综合见解时表现不足 当要回答一个复杂问题时，必须要通过共享属性在不同信息之间建立联系 RAG 无法有效捕捉这些关系 限制了 RAG 在处理需要多跳推理或整合多源数据的复杂查询时的能力       归纳总结能力不足   在处理大型数据集或长文档时，RAG 难以有效地归纳和总结复杂的语义概念 RAG 在需要全面理解和总结复杂语义信息的场景中表现不佳  GraphRAG 利用 LLM 生成的知识图谱来改进 RAG 的检索部分   GraphRAG 利用结构化的实体和关系信息，使得检索过程更加精准和全面 GraphRAG 在处理多跳问题和复杂文档分析时表现出色 GraphRAG 在处理复杂信息处理任务时，显著提升问答性能，提供比 RAG 更为准确和全面的答案 GraphRAG 通过知识图谱有效地连接不同的信息片段 不仅能够提供准确答案，还能展示答案之间的内在联系，提供更丰富和有价值的结果     GraphRAG 先利用知识图谱，关联查询的实体和关系从与知识图谱实体直接相关的文档中检索片段，提供一个更全面、指标化、高信息密度的总结   主...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zhongmingmao</div><div class="author-info-description">Focus on Infrastructure.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">640</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhongmingmao0625@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Things are always unexpected!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B1%80%E9%99%90"><span class="toc-number">1.</span> <span class="toc-text">局限</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AI-Agent"><span class="toc-number">2.</span> <span class="toc-text">AI Agent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LangChain"><span class="toc-number">3.</span> <span class="toc-text">LangChain</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">3.1.</span> <span class="toc-text">核心组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84"><span class="toc-number">3.2.</span> <span class="toc-text">技术架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-I-O"><span class="toc-number">3.2.1.</span> <span class="toc-text">Model I&#x2F;O</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Format"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">Format</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Predict"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">Predict</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Parse"><span class="toc-number">3.2.1.3.</span> <span class="toc-text">Parse</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Retrieval"><span class="toc-number">3.2.2.</span> <span class="toc-text">Retrieval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Agents"><span class="toc-number">3.2.3.</span> <span class="toc-text">Agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Chains"><span class="toc-number">3.2.4.</span> <span class="toc-text">Chains</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Memory"><span class="toc-number">3.2.5.</span> <span class="toc-text">Memory</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Callbacks"><span class="toc-number">3.2.6.</span> <span class="toc-text">Callbacks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LCEL"><span class="toc-number">3.2.7.</span> <span class="toc-text">LCEL</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">3.3.</span> <span class="toc-text">核心思想</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RAG"><span class="toc-number">4.</span> <span class="toc-text">RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E6%BB%9E%E5%90%8E"><span class="toc-number">4.1.</span> <span class="toc-text">知识滞后</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E6%B5%81%E7%A8%8B"><span class="toc-number">4.2.</span> <span class="toc-text">技术流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="toc-number">4.2.1.</span> <span class="toc-text">向量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8"><span class="toc-number">4.2.2.</span> <span class="toc-text">向量存储</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Introduction"/></a><div class="content"><a class="title" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction">Observability - Prometheus Introduction</a><time datetime="2025-01-21T16:06:25.000Z" title="Created 2025-01-22 00:06:25">2025-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java-agent.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java Zero Code"/></a><div class="content"><a class="title" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code">Observability - OpenTelemetry Java Zero Code</a><time datetime="2025-01-20T16:06:25.000Z" title="Created 2025-01-21 00:06:25">2025-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java"/></a><div class="content"><a class="title" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java">Observability - OpenTelemetry Java</a><time datetime="2025-01-19T16:06:25.000Z" title="Created 2025-01-20 00:06:25">2025-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/mcp.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - MCP Overview"/></a><div class="content"><a class="title" href="/2025/01/19/ai-agent-overview-mcp/" title="AI Agent - MCP Overview">AI Agent - MCP Overview</a><time datetime="2025-01-18T16:06:25.000Z" title="Created 2025-01-19 00:06:25">2025-01-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/18/ai-agent-overview/" title="AI Agent - Overview"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ai-agent-1253868755.cos.ap-guangzhou.myqcloud.com/ai-agent.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AI Agent - Overview"/></a><div class="content"><a class="title" href="/2025/01/18/ai-agent-overview/" title="AI Agent - Overview">AI Agent - Overview</a><time datetime="2025-01-17T16:06:25.000Z" title="Created 2025-01-18 00:06:25">2025-01-18</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2015 - 2025 By zhongmingmao</span></div><div class="footer_custom_text">Life is like a box of chocolates. You can't know what you'll eat until you open it.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div></body></html>