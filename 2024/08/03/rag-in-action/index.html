<!DOCTYPE html><html lang="en" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>RAG - In Action | ByteCoding</title><meta name="author" content="zhongmingmao"><meta name="copyright" content="zhongmingmao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="技术选型LangChain LangChain 是专门为开发基于 LLM 应用而设计的全面框架 LangChain 的核心目标是简化开发者的构建流程，使其能够高效地创建 LLM 驱动的应用">
<meta property="og:type" content="article">
<meta property="og:title" content="RAG - In Action">
<meta property="og:url" content="https://blog.zhongmingmao.top/2024/08/03/rag-in-action/index.html">
<meta property="og:site_name" content="ByteCoding">
<meta property="og:description" content="技术选型LangChain LangChain 是专门为开发基于 LLM 应用而设计的全面框架 LangChain 的核心目标是简化开发者的构建流程，使其能够高效地创建 LLM 驱动的应用">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-in-action.webp">
<meta property="article:published_time" content="2024-08-02T16:06:25.000Z">
<meta property="article:modified_time" content="2024-08-28T12:12:53.244Z">
<meta property="article:author" content="zhongmingmao">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-in-action.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RAG - In Action",
  "url": "https://blog.zhongmingmao.top/2024/08/03/rag-in-action/",
  "image": "https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-in-action.webp",
  "datePublished": "2024-08-02T16:06:25.000Z",
  "dateModified": "2024-08-28T12:12:53.244Z",
  "author": [
    {
      "@type": "Person",
      "name": "zhongmingmao",
      "url": "https://blog.zhongmingmao.top"
    }
  ]
}</script><link rel="shortcut icon" href="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png"><link rel="canonical" href="https://blog.zhongmingmao.top/2024/08/03/rag-in-action/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":32,"languages":{"author":"Author: zhongmingmao","link":"Link: ","source":"Source: ByteCoding","info":"Copyright belongs to the author. For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source."}},
  lightbox: 'null',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'RAG - In Action',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ByteCoding" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg" style="background-image: url(/url(https:/cdn.pixabay.com/photo/2021/07/20/03/39/fisherman-6479663_1280.jpg));"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">642</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-in-action.webp);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">ByteCoding</span></a><a class="nav-page-title" href="/"><span class="site-name">RAG - In Action</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">RAG - In Action</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2024-08-02T16:06:25.000Z" title="Created 2024-08-03 00:06:25">2024-08-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/rag/">RAG</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">3.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>11mins</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:512,&quot;messagePrev&quot;:&quot;It has been&quot;,&quot;messageNext&quot;:&quot;days since the last update, the content of the article may be outdated.&quot;,&quot;postUpdate&quot;:&quot;2024-08-28 20:12:53&quot;}" hidden></div><h1 id="技术选型"><a href="#技术选型" class="headerlink" title="技术选型"></a>技术选型</h1><h2 id="LangChain"><a href="#LangChain" class="headerlink" title="LangChain"></a>LangChain</h2><ol>
<li>LangChain 是专门为开发基于 <strong>LLM</strong> 应用而设计的<strong>全面框架</strong></li>
<li>LangChain 的核心目标是<strong>简化</strong>开发者的<strong>构建流程</strong>，使其能够高效地创建 LLM 驱动的应用</li>
</ol>
<span id="more"></span>

<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="文档解析"><a href="#文档解析" class="headerlink" title="文档解析"></a>文档解析</h3><ol>
<li>pypdf 专门用于处理 PDF 文档</li>
<li>pypdf 支持 PDF 文档的<strong>创建</strong>、<strong>读取</strong>、<strong>编辑</strong>和<strong>转换</strong>，能够有效地提取和处理<strong>文本</strong>、<strong>图像</strong>及<strong>页面</strong>内容</li>
</ol>
<h3 id="文档分块"><a href="#文档分块" class="headerlink" title="文档分块"></a>文档分块</h3><ol>
<li><strong>RecursiveCharacterTextSplitter</strong> 是 <strong>LangChain</strong> 默认的文本分割器</li>
<li>RecursiveCharacterTextSplitter 通过<strong>层次化</strong>的分隔符（从<strong>双换行符</strong>到<strong>单字符</strong>）拆分文本<ul>
<li>旨在保持文本的<strong>结构</strong>和<strong>连贯性</strong>，优先考虑<strong>自然边界</strong>（如段落和句子）</li>
</ul>
</li>
</ol>
<h2 id="索引-检索"><a href="#索引-检索" class="headerlink" title="索引 + 检索"></a>索引 + 检索</h2><h3 id="向量化模型"><a href="#向量化模型" class="headerlink" title="向量化模型"></a>向量化模型</h3><ol>
<li><strong>bge-small-zh-v1.5</strong> 是由北京<strong>智源</strong>人工智能研究院（<strong>BAAI</strong>）开发的<strong>开源</strong>向量模型</li>
<li>bge-small-zh-v1.5 的模型<strong>体积较小</strong>，但仍能提供<strong>高精度</strong>和<strong>高效</strong>的<strong>中文</strong>向量检索</li>
<li>bge-small-zh-v1.5 的<strong>向量维度</strong>为 <strong>512</strong>，<strong>最大输入长度</strong>同样为 512</li>
</ol>
<h3 id="向量库"><a href="#向量库" class="headerlink" title="向量库"></a>向量库</h3><ol>
<li>Faiss - Facebook AI <strong>Similarity Search</strong></li>
<li>Faiss 由 Facebook AI Research <strong>开源</strong>的向量库，非常<strong>稳定</strong>和<strong>高效</strong></li>
</ol>
<h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><h3 id="LLM"><a href="#LLM" class="headerlink" title="LLM"></a>LLM</h3><ol>
<li>Qwen 是阿里云推出的一款<strong>超大规模</strong>语言模型</li>
<li>Qwen 支持<strong>多轮对话</strong>、<strong>文案创造</strong>、<strong>逻辑推理</strong>、<strong>多模态理解</strong>和<strong>语言处理</strong></li>
<li>Qwen 在<strong>模型性能</strong>和<strong>工程应用</strong>中表现出色</li>
<li>Qwen 支持<strong>云端 API 服务</strong></li>
</ol>
<h2 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h2><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/65a9694a63bdb6108504f9586c0a05c0.png" alt="65a9694a63bdb6108504f9586c0a05c0"></p>
<ol>
<li><strong>LangChain</strong><ul>
<li>提供用于构建 <strong>LLM RAG</strong> 的<strong>应用程序框架</strong></li>
</ul>
</li>
<li>索引流程<ul>
<li>使用 <strong>pypdf</strong> 对文档进行<strong>解析</strong>并<strong>提取</strong>信息</li>
<li>采用 <strong>RecursiveCharacterTextSplitter</strong> 对文档内容进行<strong>分块</strong>（<strong>Chunk</strong>）</li>
<li>使用 <strong>bge-small-zh-v1.5</strong> 将 <strong>Chunk</strong> 进行<strong>向量化</strong>处理，并将生成的向量存储到 <strong>Faiss 向量库</strong>中</li>
</ul>
</li>
<li>检索流程<ul>
<li>使用 <strong>bge-small-zh-v1.5</strong> 对 <strong>Query</strong> 进行<strong>向量化</strong>处理</li>
<li>通过 <strong>Faiss 向量库</strong>对 <strong>Query 向量</strong>和 <strong>Chunk 向量</strong>进行<strong>相似度匹配</strong></li>
<li>从而检索出与 <strong>Query</strong> 最相似的 <strong>Top K</strong> 个 <strong>Chunk</strong></li>
</ul>
</li>
<li>生成流程<ul>
<li>设定<strong>提示模板</strong>（<strong>Prompt</strong>）</li>
<li>将 <strong>Query</strong> 与 <strong>Chunk</strong> 填充到<strong>提示模板</strong>，生成<strong>增强提示</strong>，输入到 <strong>Qwen LLM</strong>，生成最终的 RAG 回答</li>
</ul>
</li>
</ol>
<h1 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h1><h2 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ python3 -m venv rag_env</span><br><span class="line">$ source rag_env/bin/activate</span><br></pre></td></tr></table></figure>

<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ pip install --upgrade pip</span><br><span class="line">$ pip install langchain langchain_community pypdf sentence-transformers faiss-cpu dashscope</span><br></pre></td></tr></table></figure>

<h2 id="向量化模型-1"><a href="#向量化模型-1" class="headerlink" title="向量化模型"></a>向量化模型</h2><blockquote>
<p>bge-small-zh-v1.5 - 95.8MB - pytorch_model.bin</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://huggingface.co/BAAI/bge-small-zh-v1.5</span><br><span class="line"></span><br><span class="line">$ du -sh *</span><br><span class="line">367M    bge-small-zh-v1.5</span><br><span class="line">332K    corpus.pdf</span><br><span class="line">4.0K    LICENSE</span><br><span class="line">1.1G    rag_env</span><br><span class="line">4.0K    README.md</span><br></pre></td></tr></table></figure>

<h1 id="核心代码"><a href="#核心代码" class="headerlink" title="核心代码"></a>核心代码</h1><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> PyPDFLoader  <span class="comment"># PDF文档提取</span></span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter  <span class="comment"># 文档拆分chunk</span></span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer  <span class="comment"># 加载和使用Embedding模型</span></span><br><span class="line"><span class="keyword">import</span> faiss  <span class="comment"># Faiss向量库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># 处理嵌入向量数据，用于Faiss向量检索</span></span><br><span class="line"><span class="keyword">import</span> dashscope  <span class="comment"># 调用Qwen大模型</span></span><br><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> HTTPStatus  <span class="comment"># 检查与Qwen模型HTTP请求状态</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用分词并行化操作, 避免多线程或多进程环境中运行多个模型引发冲突或死锁</span></span><br><span class="line">os.environ[<span class="string">&quot;TOKENIZERS_PARALLELISM&quot;</span>] = <span class="string">&quot;false&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置Qwen系列具体模型及对应的调用API密钥，从阿里云百炼大模型服务平台获得</span></span><br><span class="line">qwen_model = <span class="string">&quot;qwen-turbo&quot;</span></span><br><span class="line">qwen_api_key = <span class="string">&quot;your_api_key&quot;</span></span><br></pre></td></tr></table></figure>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240828141919941.png" alt="image-20240828141919941"></p>
<h2 id="索引-1"><a href="#索引-1" class="headerlink" title="索引"></a>索引</h2><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><blockquote>
<p>SentenceTransformer - map <strong>sentences &#x2F; text</strong> to <strong>embeddings</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_embedding_model</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    加载bge-small-zh-v1.5模型</span></span><br><span class="line"><span class="string">    :return: 返回加载的bge-small-zh-v1.5模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;加载Embedding模型中&quot;</span>)</span><br><span class="line">    <span class="comment"># SentenceTransformer读取绝对路径下的bge-small-zh-v1.5模型，非下载</span></span><br><span class="line">    embedding_model = SentenceTransformer(os.path.abspath(<span class="string">&#x27;bge-small-zh-v1.5&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;bge-small-zh-v1.5模型最大输入长度: <span class="subst">&#123;embedding_model.max_seq_length&#125;</span>&quot;</span>)  <span class="comment"># 512</span></span><br><span class="line">    <span class="keyword">return</span> embedding_model</span><br></pre></td></tr></table></figure>

<h3 id="Indexing"><a href="#Indexing" class="headerlink" title="Indexing"></a>Indexing</h3><blockquote>
<p>chunk_size + chunk_overlap</p>
</blockquote>
<ol>
<li><strong>chunk_size</strong><ul>
<li>对输入文本序列进行<strong>切分</strong>的<strong>最大长度</strong></li>
<li>GPT-3 的最大输入长度为 <strong>2048</strong> 个 <strong>Token</strong></li>
</ul>
</li>
<li><strong>chunk_overlap</strong><ul>
<li>相邻的两个 Chunk 之间的<strong>重叠 Token 数量</strong></li>
<li>为了保证<strong>文本语义</strong>的<strong>连贯性</strong>，相邻 Chunk 会有一定的重叠</li>
</ul>
</li>
<li>chunk_size &#x3D; 1024，chunk_overlap &#x3D; 128，对于长度为 2560 <strong>Token</strong> 的文本序列，会切分成 3 个 Chunk<ul>
<li><strong>1 ~ 1024</strong> &#x3D; 1024</li>
<li><strong>897~1920</strong> &#x3D; 1024</li>
<li><strong>1793~2560</strong> &#x3D; 768</li>
</ul>
</li>
</ol>
<blockquote>
<p>索引</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">indexing_process</span>(<span class="params">pdf_file, embedding_model</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    索引流程：加载PDF文件，并将其内容分割成小块，计算这些小块的嵌入向量并将其存储在FAISS向量数据库中。</span></span><br><span class="line"><span class="string">    :param pdf_file: PDF文件路径</span></span><br><span class="line"><span class="string">    :param embedding_model: 预加载的嵌入模型</span></span><br><span class="line"><span class="string">    :return: 返回FAISS嵌入向量索引和分割后的文本块原始内容列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># PyPDFLoader加载PDF文件，忽略图片提取</span></span><br><span class="line">    pdf_loader = PyPDFLoader(pdf_file, extract_images=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 配置RecursiveCharacterTextSplitter分割文本块库参数</span></span><br><span class="line">    <span class="comment"># 每个文本块的大小为512字符（非token），相邻文本块之间的重叠128字符（非token）</span></span><br><span class="line">    text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">512</span>, chunk_overlap=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载PDF文档,提取所有页的文本内容</span></span><br><span class="line">    pdf_content_list = pdf_loader.load()</span><br><span class="line">    <span class="comment"># 将每页的文本内容用换行符连接，合并为PDF文档的完整文本</span></span><br><span class="line">    pdf_text = <span class="string">&quot;\n&quot;</span>.join([page.page_content <span class="keyword">for</span> page <span class="keyword">in</span> pdf_content_list])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;PDF文档的总字符数: <span class="subst">&#123;<span class="built_in">len</span>(pdf_text)&#125;</span>&quot;</span>)  <span class="comment"># 9135</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将PDF文档文本分割成文本块Chunk</span></span><br><span class="line">    chunks = text_splitter.split_text(pdf_text)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;分割的文本Chunk数量: <span class="subst">&#123;<span class="built_in">len</span>(chunks)&#125;</span>&quot;</span>)  <span class="comment"># 24 ≈ 9135 / (512-128)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 文本块转化为嵌入向量列表，normalize_embeddings表示对嵌入向量进行归一化，用于准确计算相似度</span></span><br><span class="line">    embeddings = []</span><br><span class="line">    <span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:</span><br><span class="line">        embedding = embedding_model.encode(chunk, normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">        embeddings.append(embedding)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;文本块Chunk转化为嵌入向量完成&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将嵌入向量列表转化为numpy数组，FAISS索引操作需要numpy数组输入</span></span><br><span class="line">    embeddings_np = np.array(embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取嵌入向量的维度（每个向量的长度）</span></span><br><span class="line">    dimension = embeddings_np.shape[<span class="number">1</span>]  <span class="comment"># 512</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用余弦相似度创建FAISS索引</span></span><br><span class="line">    index = faiss.IndexFlatIP(dimension)</span><br><span class="line">    <span class="comment"># 将所有的嵌入向量添加到FAISS索引中，后续可以用来进行相似性检索</span></span><br><span class="line">    index.add(embeddings_np)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;索引过程完成.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> index, chunks</span><br></pre></td></tr></table></figure>

<ol>
<li>使用 PyPDFLoader 加载并预处理 PDF 文档，将其内容提取并合并为<strong>完整文本</strong></li>
<li>利用 RecursiveCharacterTextSplitter 将文本分割为每块 512 字符（非 Token）、重叠 128 字符（非 Token）的<strong>文本块</strong></li>
<li>通过预加载的 bge-small-zh-v1.5 嵌入模型将<strong>文本块</strong>转化为<strong>归一化</strong>的<strong>嵌入向量</strong></li>
<li>将<strong>嵌入向量</strong>存储在基于<strong>余弦相似度</strong>的 <strong>Faiss 向量库</strong>中，以支持后续的<strong>相似性检索</strong>和<strong>生成</strong>任务</li>
</ol>
<blockquote>
<p>余弦相似度 - Cosine Similarity - 在 N 维空间中，两个 N 维向量之间角度的余弦 - <strong>值越大越相似</strong></p>
</blockquote>
<p>$$<br>Similarity(A,B) &#x3D; \frac{A\cdot{B}}{|A|\times|B|} &#x3D; \frac{\sum_{i&#x3D;1}^n(A_i\times{B_i})}{\sqrt{\sum_{i&#x3D;1}^n{A_i^2}}\times\sqrt{\sum_{i&#x3D;1}^n{B_i^2}}}<br>$$</p>
<blockquote>
<p>待优化项</p>
</blockquote>
<ol>
<li><strong>多文档</strong>处理</li>
<li><strong>嵌入模型</strong>的<strong>效率优化</strong>与<strong>并行处理</strong></li>
<li><strong>Faiss</strong> 采用<strong>持久化</strong>存储 - 目前仅在<strong>内存</strong>中存储</li>
</ol>
<h2 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">retrieval_process</span>(<span class="params">query, index, chunks, embedding_model, top_k=<span class="number">3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    检索流程：将用户查询Query转化为嵌入向量，并在Faiss索引中检索最相似的前k个文本块。</span></span><br><span class="line"><span class="string">    :param query: 用户查询语句</span></span><br><span class="line"><span class="string">    :param index: 已建立的Faiss向量索引</span></span><br><span class="line"><span class="string">    :param chunks: 原始文本块内容列表</span></span><br><span class="line"><span class="string">    :param embedding_model: 预加载的嵌入模型</span></span><br><span class="line"><span class="string">    :param top_k: 返回最相似的前K个结果</span></span><br><span class="line"><span class="string">    :return: 返回最相似的文本块及其相似度得分</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将查询转化为嵌入向量，normalize_embeddings表示对嵌入向量进行归一化</span></span><br><span class="line">    query_embedding = embedding_model.encode(query, normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 将嵌入向量转化为numpy数组，Faiss索引操作需要numpy数组输入</span></span><br><span class="line">    query_embedding = np.array([query_embedding])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在 Faiss 索引中使用 query_embedding 进行搜索，检索出最相似的前 top_k 个结果。</span></span><br><span class="line">    <span class="comment"># 返回查询向量与每个返回结果之间的相似度得分（在使用余弦相似度时，值越大越相似）</span></span><br><span class="line">    <span class="comment"># 排名列表distances，最相似的 top_k 个文本块在原始 chunks 列表中的索引indices。</span></span><br><span class="line">    distances, indices = index.search(query_embedding, top_k)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;查询语句: <span class="subst">&#123;query&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;最相似的前<span class="subst">&#123;top_k&#125;</span>个文本块:&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出查询出的top_k个文本块及其相似度得分</span></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(top_k):</span><br><span class="line">        <span class="comment"># 获取相似文本块的原始内容</span></span><br><span class="line">        result_chunk = chunks[indices[<span class="number">0</span>][i]]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;文本块 <span class="subst">&#123;i&#125;</span>:\n<span class="subst">&#123;result_chunk&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取相似文本块的相似度得分</span></span><br><span class="line">        result_distance = distances[<span class="number">0</span>][i]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;相似度得分: <span class="subst">&#123;result_distance&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将相似文本块存储在结果列表中</span></span><br><span class="line">        results.append(result_chunk)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;检索过程完成.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>

<ol>
<li>Query 被预加载的 bge-small-zh-v1.5 嵌入模型转化为归一化的<strong>嵌入向量</strong><ul>
<li>进一步转换为 <strong>numpy 数组</strong>以适配 Faiss 向量库的输入格式</li>
</ul>
</li>
<li>利用 Faiss 向量库中的<strong>向量检索</strong>功能<ul>
<li>计算 Query 与存储向量之间<strong>余弦相似度</strong>，从而筛选出与 Query <strong>最相似</strong>的 <strong>Top K</strong> 个文本块</li>
</ul>
</li>
<li>相似文本块存储在结果列表中，供后续<strong>生成</strong>过程使用</li>
</ol>
<h2 id="生成-1"><a href="#生成-1" class="headerlink" title="生成"></a>生成</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_process</span>(<span class="params">query, chunks</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成流程：调用Qwen大模型云端API，根据查询和文本块生成最终回复。</span></span><br><span class="line"><span class="string">    :param query: 用户查询语句</span></span><br><span class="line"><span class="string">    :param chunks: 从检索过程中获得的相关文本块上下文chunks</span></span><br><span class="line"><span class="string">    :return: 返回生成的响应内容</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 设置Qwen系列具体模型及对应的调用API密钥，从阿里云大模型服务平台百炼获得</span></span><br><span class="line">    llm_model = qwen_model</span><br><span class="line">    dashscope.api_key = qwen_api_key</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建参考文档内容，格式为“参考文档1: \n 参考文档2: \n ...”等</span></span><br><span class="line">    context = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i, chunk <span class="keyword">in</span> <span class="built_in">enumerate</span>(chunks):</span><br><span class="line">        context += <span class="string">f&quot;参考文档<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>: \n<span class="subst">&#123;chunk&#125;</span>\n\n&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建生成模型所需的Prompt，包含用户查询和检索到的上下文</span></span><br><span class="line">    prompt = <span class="string">f&quot;根据参考文档回答问题：<span class="subst">&#123;query&#125;</span>\n\n<span class="subst">&#123;context&#125;</span>&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;生成模型的Prompt: <span class="subst">&#123;prompt&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 准备请求消息，将prompt作为输入</span></span><br><span class="line">    messages = [&#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: prompt&#125;]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用大模型API云服务生成响应</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        responses = dashscope.Generation.call(</span><br><span class="line">            model=llm_model,</span><br><span class="line">            messages=messages,</span><br><span class="line">            result_format=<span class="string">&#x27;message&#x27;</span>,  <span class="comment"># 设置返回格式为&quot;message&quot;</span></span><br><span class="line">            stream=<span class="literal">True</span>,  <span class="comment"># 启用流式输出</span></span><br><span class="line">            incremental_output=<span class="literal">True</span>  <span class="comment"># 获取流式增量输出</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 初始化变量以存储生成的响应内容</span></span><br><span class="line">        generated_response = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;生成过程开始:&quot;</span>)</span><br><span class="line">        <span class="comment"># 逐步获取和处理模型的增量输出</span></span><br><span class="line">        <span class="keyword">for</span> response <span class="keyword">in</span> responses:</span><br><span class="line">            <span class="keyword">if</span> response.status_code == HTTPStatus.OK:</span><br><span class="line">                content = response.output.choices[<span class="number">0</span>][<span class="string">&#x27;message&#x27;</span>][<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">                generated_response += content</span><br><span class="line">                <span class="built_in">print</span>(content, end=<span class="string">&#x27;&#x27;</span>)  <span class="comment"># 实时输出模型生成的内容</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;请求失败: <span class="subst">&#123;response.status_code&#125;</span> - <span class="subst">&#123;response.message&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span>  <span class="comment"># 请求失败时返回 None</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\n生成过程完成.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> generated_response</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;大模型生成过程中发生错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<ol>
<li>结合 Query 与检索到的文本块组织成 <strong>LLM Prompt</strong></li>
<li>调用 Qwen 云端 API，将 Prompt 发送给 LLM</li>
<li>利用<strong>流式</strong>输出的方式逐步获取 LLM 生成的响应内容，<strong>实时</strong>输出并汇总成最终的生成结果</li>
</ol>
<blockquote>
<p>输出</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">生成过程开始:</span><br><span class="line">参考文档中涉及的案例及其面临的挑战如下：</span><br><span class="line"></span><br><span class="line">1. **金融业**：</span><br><span class="line">   - **挑战**：银行面临的主要挑战包括客户服务模式过时（主要依赖实体网点，服务效率低、客户体验差），金融科技企业的竞争压力（凭借创新技术和便捷服务吸引大量客户，尤其是年轻一代），以及数据孤岛和风险管理滞后（各业务部门缺乏数据共享机制，信息无法整合，风险管理效率低）。</span><br><span class="line"></span><br><span class="line">2. **制造业**：</span><br><span class="line">   - **挑战**：制造业面临的主要挑战包括生产效率低、易出错，供应链管理复杂（涉及多个国家和地区，信息传递不及时，造成库存管理困难，甚至存在供应链断裂的风险），以及无法满足市场对个性化定制产品的需求（传统大规模生产方式无法适应）。</span><br><span class="line"></span><br><span class="line">3. **零售业**：</span><br><span class="line">   - **挑战**：零售业面临的主要挑战是线上线下渠道割裂（导致库存管理不统一、客户体验不一致，难以提供无缝购物体验），以及数据利用率低（尽管拥有大量消费者和销售数据，但缺乏先进的数据分析工具，未能转化为可操作的商业洞察）。</span><br><span class="line"></span><br><span class="line">对于每个行业，数字化转型解决方案通常包括：</span><br><span class="line">- **金融业**：构建数字化银行平台，推出移动银行应用、在线服务、虚拟客服和智能理财顾问，同时引入人工智能和大数据分析技术以提升服务便捷性、客户满意度和风险管理能力。</span><br><span class="line">- **制造业**：引入工业4.0技术（如物联网、人工智能、大数据分析和机器人自动化）以优化生产线，构建基于云计算的智能供应链管理系统实现供应链的端到端可视化管理。</span><br><span class="line">- **零售业**：构建全渠道零售平台实现线上与线下购物渠道的无缝整合，引入大数据和人工智能驱动的分析平台以精准预测需求、优化库存，并提供个性化产品推荐和营销活动。</span><br><span class="line">生成过程完成.</span><br></pre></td></tr></table></figure>

<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css"></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top">zhongmingmao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://blog.zhongmingmao.top/2024/08/03/rag-in-action/">https://blog.zhongmingmao.top/2024/08/03/rag-in-action/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ai/">AI</a><a class="post-meta__tags" href="/tags/llm/">LLM</a><a class="post-meta__tags" href="/tags/rag/">RAG</a></div><div class="post-share"><div class="social-share" data-image="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-in-action.webp" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/08/04/rag-langchain/" title="RAG - LangChain"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-agent.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">RAG - LangChain</div></div><div class="info-2"><div class="info-item-1">Practice   LangChain RAG https://github.com/langchain-ai/rag-from-scratch   RAG 如何随着长期 LLM 而改变 Is RAG Really Dead? https://www.youtube.com/watch?v=SsHUNfhF32s     自适应 RAG 根据复杂程度动态地将查询路由到不同的 RAG 方法 - Command-R @ LangGraph Adaptive RAG https://www.youtube.com/watch?v=04ighIjMcAI   Code https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb   Paper https://arxiv.org/abs/2403.14403     Adaptive RAG 在循环单元测试中自我纠正检索错误，以确定文档相关性并返回到网络搜索 在 LangGraph 中实现 Mistral 7B + Ol...</div></div></div></a><a class="pagination-related" href="/2024/08/02/rag-principle/" title="RAG -Principle"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-principle-4736980.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">RAG -Principle</div></div><div class="info-2"><div class="info-item-1">LLM 局限 当设计一个 LLM 问答应用，模型需要处理用户的领域问题时，LLM 通常表现出色 但有时提供的答案并不准确，甚至出现错误 当用户需要获取实时信息时，LLM 无法及时提供最新的答案 LLM 在知识、理解和推理方面展现了卓越的能力，在复杂交互场景中表现尤为突出 LLM 存在无法忽略的局限性     LLM 局限     Limitation Desc    领域知识缺乏 LLM 的知识来源于训练数据，主要为公开数据集，无法覆盖特定领域或高度专业化的内部知识   信息过时 LLM 难以处理实时信息，训练过程耗时且成本高昂，模型一旦训练完成，就难以处理和获取信息   幻觉 模型基于概率生成文本，有时会输出看似合理但实际错误的答案   数据安全 需要在确保数据安全的前提下，使 LLM 有效利用私有数据进行推理和生成    RAG 应运而生   将非参数化的外部知识库和文档与 LLM 结合 RAG 使 LLM 在生成内容之前，能够先检索相关信息 弥补 LLM 在知识专业性和时效性的不足 在确保数据安全的同时，充分利用领域知识和私有数据     选择 RAG 而不是直接将所有知识库数据交给 LLM   ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/09/09/rag-vector-qdrant/" title="RAG - Qdrant"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/qdrant.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-09</div><div class="info-item-2">RAG - Qdrant</div></div><div class="info-2"><div class="info-item-1">Features   Getting StartedIntroduction Vector databases are a relatively new way for interacting with abstract data representations derived from opaque machine learning models such as deep learning architectures. These representations are often called vectors or embeddings and they are a compressed version of the data used to train a machine learning model to accomplish a task like sentiment analysis, speech recognition, object detection, and many others.  What is Qdrant? Qdrant “is a vector similarity s...</div></div></div></a><a class="pagination-related" href="/2024/08/17/rag-graphrag/" title="RAG - GraphRAG"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/graphrag-9781829.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-17</div><div class="info-item-2">RAG - GraphRAG</div></div><div class="info-2"><div class="info-item-1">向量检索 信息片段之间的连接能力有限   RAG 在跨越多个信息片段以获取综合见解时表现不足 当要回答一个复杂问题时，必须要通过共享属性在不同信息之间建立联系 RAG 无法有效捕捉这些关系 限制了 RAG 在处理需要多跳推理或整合多源数据的复杂查询时的能力       归纳总结能力不足   在处理大型数据集或长文档时，RAG 难以有效地归纳和总结复杂的语义概念 RAG 在需要全面理解和总结复杂语义信息的场景中表现不佳  GraphRAG 利用 LLM 生成的知识图谱来改进 RAG 的检索部分   GraphRAG 利用结构化的实体和关系信息，使得检索过程更加精准和全面 GraphRAG 在处理多跳问题和复杂文档分析时表现出色 GraphRAG 在处理复杂信息处理任务时，显著提升问答性能，提供比 RAG 更为准确和全面的答案 GraphRAG 通过知识图谱有效地连接不同的信息片段 不仅能够提供准确答案，还能展示答案之间的内在联系，提供更丰富和有价值的结果     GraphRAG 先利用知识图谱，关联查询的实体和关系从与知识图谱实体直接相关的文档中检索片段，提供一个更全面、指标化、高信息密度的总结   主...</div></div></div></a><a class="pagination-related" href="/2024/08/16/rag-evolution/" title="RAG - Evolution"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-evolution.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-16</div><div class="info-item-2">RAG - Evolution</div></div><div class="info-2"><div class="info-item-1">演进    Naive RAG -&gt; Advanced RAG -&gt; Modular RAG 三个范式之间具有继承与发展的关系 Advanced RAG 是 Modular RAG 的一种特例形式 Naive RAG 是 Advanced RAG 的基础特例   RAG 技术不断演进，以适应更复杂的任务和场景需求  Naive RAG Naive RAG 是最基础的形式，依赖核心的索引和检索策略来增强生成模型的输出 Naive RAG 适用于一些基础任务和产品 MVP 阶段  Advanced RAG 通过增加检索前、检索中以及检索后的优化策略，提高检索的准确性和生成的关联性 - 适用于复杂任务    Advanced RAG 通过优化检索前、检索中、检索后的各个环节 在索引质量、检索效果以及生成内容的上下文相关性方面都取得显著提升  检索前 通过索引、分块、查询优化和内容向量化等技术手段，提高检索内容的精确性和生成内容的相关性  滑动窗口 overlap   经典的 Chunking 技术，通过在相邻的 Chunk 之间创建重叠区域，确保关键信息不会因简单的 Chunking 而丢失 在 ...</div></div></div></a><a class="pagination-related" href="/2024/08/15/rag-optimization-evaluation/" title="RAG - Optimization + Evaluation"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-evaluation.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-15</div><div class="info-item-2">RAG - Optimization + Evaluation</div></div><div class="info-2"><div class="info-item-1">RAG   检索优化数据清洗和预处理 在 RAG 索引流程中，文档解析之后，文档切块之前，进行数据清洗和预处理 减少脏数据和噪音，提升文本的整体质量和信息密度   手段：清除冗余信息、统一格式、处理异常字符等 处理冗余的模板内容 消除文档中的额外空白和格式不一致 去除文档脚注、页眉页脚、版权信息    查询扩写 在 RAG 系统的检索步骤中，用户的查询会转换为向量后进行检索 单个向量查询只能覆盖向量空间中一个有限区域 如果查询中的嵌入向量未能包含所有关键信息，则可能检索到不相关的 Chunk 单点查询的局限性 - 限制系统在庞大文档库中的搜索范围，导致错失与查询语义相关的内容   查询扩写 通过 LLM 从原始查询语句生成多个语义相关的查询，可以覆盖向量空间中的不同区域 提高检索的全面性和准确性 扩写后的查询在被嵌入后，能够击中不同的语义区域 确保系统能够从更广泛的文档中检索到与用户需求相关的有用信息     通过查询扩写，原始问题被分解为多个子查询 每个子查询独立检索相关文档并生成相应的结果 系统将所有子查询的检索结果进行合并和重新排序   能够有效扩展用户的查询意图，确保在复杂信息库中进行更全面的文...</div></div></div></a><a class="pagination-related" href="/2024/08/14/rag-llm-prompt-engineering/" title="RAG - LLM + Prompt Engineering"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/llm-prompting.webp" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-14</div><div class="info-item-2">RAG - LLM + Prompt Engineering</div></div><div class="info-2"><div class="info-item-1">RAG 生成流程    经过 RAG 索引流程（外部知识的解析和向量化）和 RAG 检索流程（语义相似性的匹配及混合检索），进入到 RAG 生成流程 在 RAG 生成流程中，需要组合指令，即携带查询问题及检索到的相关信息输入的 LLM，由 LLM 理解并生成最终的回复 RAG 的本质是通过 LLM 提供外部知识来增强其理解和回答领域问题的能力 LLM 在 RAG 系统中起到了大脑的作用 在面对复杂且多样化的 RAG 任务时，LLM 的性能直接决定了系统的整体效果   提示词工程是生成流程中的另一个关键环节 通过有效的指令的设计和组合，可以帮助 LLM 更好地理解输入内容，从而生成更加精确和相关的回答 精心设计的问题提示词，往往能提升生成效果    LLM发展  RAG 目前更关注通用大模型   原理 Google 于 2017 年发布论文 Attention Is All You Need，引入了 Transformer 模型 Transformer 模型是深度学习领域的一个突破性架构，LLM 的成功得益于对 Transformer 模型的应用 与传统的 RNN（循环神经网络） 相比，Transform...</div></div></div></a><a class="pagination-related" href="/2024/08/13/rag-hybrid-retrieval-rerank/" title="RAG - Hybrid retrieval + Rerank"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/rag-rerank-9097303.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-13</div><div class="info-item-2">RAG - Hybrid retrieval + Rerank</div></div><div class="info-2"><div class="info-item-1">向量检索 当前主流的 RAG 检索方式主要采用向量检索，通过语义相似度来匹配 Chunk 向量检索并非万能，在某些场景下无法替代传统关键词检索的优势 当需要精准搜索的时候，向量检索的准确性就往往不如关键词检索 当用户输入的问题非常简短，语义匹配的效果可能不尽理想   关键词检索的适用场景 精确匹配 少量字符的匹配 - 不适合用向量检索 低频词汇的匹配       混合检索 结合关键词检索和语义匹配的优势   在 RAG 检索场景中，首要目标是确保最相关的结果能够出现在候选列表中 向量检索和关键词检索各具优势，混合检索通过结合多种检索技术，弥补各自不足，提供一种更加全面的搜索方案 重排序技术在检索系统中扮演着至关重要的角色 即使检索算法已经能够捕捉到所有相关的结果，重排序过程依然不可或缺 确保最符合用户意图和查询语义的结果优先展示，提升用户的搜索体验和结果的准确性     在多个数据集和多个检索任务中，混合检索和重排序的组合均取得了最佳表现     融合检索 &#x2F; 多路召回    https://python.langchain.com/v0.2/api_reference/community/r...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhongmingmao-1253868755.cos.ap-guangzhou.myqcloud.com/z.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zhongmingmao</div><div class="author-info-description">Focus on Infrastructure.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">642</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">191</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">83</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="mailto:zhongmingmao0625@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Things are always unexpected!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">技术选型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#LangChain"><span class="toc-number">1.1.</span> <span class="toc-text">LangChain</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95"><span class="toc-number">1.2.</span> <span class="toc-text">索引</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90"><span class="toc-number">1.2.1.</span> <span class="toc-text">文档解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E6%A1%A3%E5%88%86%E5%9D%97"><span class="toc-number">1.2.2.</span> <span class="toc-text">文档分块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95-%E6%A3%80%E7%B4%A2"><span class="toc-number">1.3.</span> <span class="toc-text">索引 + 检索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">向量化模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%BA%93"><span class="toc-number">1.3.2.</span> <span class="toc-text">向量库</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90"><span class="toc-number">1.4.</span> <span class="toc-text">生成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM"><span class="toc-number">1.4.1.</span> <span class="toc-text">LLM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG"><span class="toc-number">1.5.</span> <span class="toc-text">RAG</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83"><span class="toc-number">2.</span> <span class="toc-text">开发环境</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83"><span class="toc-number">2.1.</span> <span class="toc-text">虚拟环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"><span class="toc-number">2.2.</span> <span class="toc-text">安装依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">2.3.</span> <span class="toc-text">向量化模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">3.</span> <span class="toc-text">核心代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96"><span class="toc-number">3.1.</span> <span class="toc-text">依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95-1"><span class="toc-number">3.2.</span> <span class="toc-text">索引</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedding"><span class="toc-number">3.2.1.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Indexing"><span class="toc-number">3.2.2.</span> <span class="toc-text">Indexing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2"><span class="toc-number">3.3.</span> <span class="toc-text">检索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90-1"><span class="toc-number">3.4.</span> <span class="toc-text">生成</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/01/24/cloud-native-observability-prometheus-server-v1/" title="Observability - Prometheus Server V1"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus-server.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Server V1"/></a><div class="content"><a class="title" href="/2025/01/24/cloud-native-observability-prometheus-server-v1/" title="Observability - Prometheus Server V1">Observability - Prometheus Server V1</a><time datetime="2025-01-23T16:06:25.000Z" title="Created 2025-01-24 00:06:25">2025-01-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/23/cloud-native-observability-prometheus-concepts/" title="Observability - Prometheus Concepts"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus-concepts.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Concepts"/></a><div class="content"><a class="title" href="/2025/01/23/cloud-native-observability-prometheus-concepts/" title="Observability - Prometheus Concepts">Observability - Prometheus Concepts</a><time datetime="2025-01-22T16:06:25.000Z" title="Created 2025-01-23 00:06:25">2025-01-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/prometheus.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - Prometheus Introduction"/></a><div class="content"><a class="title" href="/2025/01/22/cloud-native-observability-prometheus-introduction/" title="Observability - Prometheus Introduction">Observability - Prometheus Introduction</a><time datetime="2025-01-21T16:06:25.000Z" title="Created 2025-01-22 00:06:25">2025-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java-agent.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java Zero Code"/></a><div class="content"><a class="title" href="/2025/01/21/cloud-native-observability-opentelemetry-java-zero-code/" title="Observability - OpenTelemetry Java Zero Code">Observability - OpenTelemetry Java Zero Code</a><time datetime="2025-01-20T16:06:25.000Z" title="Created 2025-01-21 00:06:25">2025-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://observability-1253868755.cos.ap-guangzhou.myqcloud.com/otel-java.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Observability - OpenTelemetry Java"/></a><div class="content"><a class="title" href="/2025/01/20/cloud-native-observability-opentelemetry-java/" title="Observability - OpenTelemetry Java">Observability - OpenTelemetry Java</a><time datetime="2025-01-19T16:06:25.000Z" title="Created 2025-01-20 00:06:25">2025-01-20</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2015 - 2025 By zhongmingmao</span></div><div class="footer_custom_text">Life is like a box of chocolates. You can't know what you'll eat until you open it.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional and Simplified Chinese">繁</button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div></body></html>