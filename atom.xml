<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ByteCoding</title>
  
  
  <link href="https://blog.zhongmingmao.top/atom.xml" rel="self"/>
  
  <link href="https://blog.zhongmingmao.top/"/>
  <updated>2024-08-28T12:12:53.244Z</updated>
  <id>https://blog.zhongmingmao.top/</id>
  
  <author>
    <name>zhongmingmao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RAG - In Action</title>
    <link href="https://blog.zhongmingmao.top/2024/08/03/rag-in-action/"/>
    <id>https://blog.zhongmingmao.top/2024/08/03/rag-in-action/</id>
    <published>2024-08-02T16:06:25.000Z</published>
    <updated>2024-08-28T12:12:53.244Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;技术选型&quot;&gt;&lt;a href=&quot;#技术选型&quot; class=&quot;headerlink&quot; title=&quot;技术选型&quot;&gt;&lt;/a&gt;技术选型&lt;/h1&gt;&lt;h2 id=&quot;LangChain&quot;&gt;&lt;a href=&quot;#LangChain&quot; class=&quot;headerlink&quot; title=&quot;LangChain&quot;&gt;&lt;/a&gt;LangChain&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;LangChain 是专门为开发基于 &lt;strong&gt;LLM&lt;/strong&gt; 应用而设计的&lt;strong&gt;全面框架&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;LangChain 的核心目标是&lt;strong&gt;简化&lt;/strong&gt;开发者的&lt;strong&gt;构建流程&lt;/strong&gt;，使其能够高效地创建 LLM 驱动的应用&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG -Principle</title>
    <link href="https://blog.zhongmingmao.top/2024/08/02/rag-principle/"/>
    <id>https://blog.zhongmingmao.top/2024/08/02/rag-principle/</id>
    <published>2024-08-01T16:06:25.000Z</published>
    <updated>2024-08-27T09:47:40.614Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;LLM-局限&quot;&gt;&lt;a href=&quot;#LLM-局限&quot; class=&quot;headerlink&quot; title=&quot;LLM 局限&quot;&gt;&lt;/a&gt;LLM 局限&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;当设计一个 LLM &lt;strong&gt;问答&lt;/strong&gt;应用，模型需要处理用户的&lt;strong&gt;领域问题&lt;/strong&gt;时，LLM &lt;strong&gt;通常&lt;/strong&gt;表现&lt;strong&gt;出色&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;但有时提供的答案并&lt;strong&gt;不准确&lt;/strong&gt;，甚至出现&lt;strong&gt;错误&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;当用户需要&lt;strong&gt;获取实时信息&lt;/strong&gt;时，LLM 无法及时提供最新的答案&lt;/li&gt;
&lt;li&gt;LLM 在&lt;strong&gt;知识&lt;/strong&gt;、&lt;strong&gt;理解&lt;/strong&gt;和&lt;strong&gt;推理&lt;/strong&gt;方面展现了&lt;strong&gt;卓越&lt;/strong&gt;的能力，在&lt;strong&gt;复杂交互场景&lt;/strong&gt;中表现尤为突出&lt;/li&gt;
&lt;li&gt;LLM 存在无法忽略的局限性&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - AI 2.0</title>
    <link href="https://blog.zhongmingmao.top/2024/08/01/rag-ai-2/"/>
    <id>https://blog.zhongmingmao.top/2024/08/01/rag-ai-2/</id>
    <published>2024-07-31T16:06:25.000Z</published>
    <updated>2024-08-27T04:21:25.198Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;AI-技术&quot;&gt;&lt;a href=&quot;#AI-技术&quot; class=&quot;headerlink&quot; title=&quot;AI 技术&quot;&gt;&lt;/a&gt;AI 技术&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;做 &lt;strong&gt;AI 产品&lt;/strong&gt;的工程研发需充分掌握 &lt;strong&gt;AI 技术&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;AI 产品从 &lt;strong&gt;MVP&lt;/strong&gt; 到 &lt;strong&gt;PMF&lt;/strong&gt; 的演进过程中会面临非常多的挑战&lt;ul&gt;
&lt;li&gt;MVP - Minimum Viable Product - 最小可用产品&lt;/li&gt;
&lt;li&gt;PMF - Product-Market Fit - 产品市场契合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;要实现 AI 产品的 PMF&lt;ul&gt;
&lt;li&gt;首先需要&lt;strong&gt;充分了解 AI 技术&lt;/strong&gt;，明确&lt;strong&gt;技术边界&lt;/strong&gt;，找到&lt;strong&gt;合适 AI 技术&lt;/strong&gt;的&lt;strong&gt;应用场景&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;其次，需要深刻理解业务，用户需求决定产品方向，AI 技术是为业务服务的工具&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;验证&lt;/strong&gt;阶段，优先使用&lt;strong&gt;最佳 AI 模型&lt;/strong&gt;以确保产品&lt;strong&gt;满足市场需求&lt;/strong&gt;，确认后再逐步&lt;strong&gt;降低模型成本&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;坚持&lt;strong&gt;业务优先&lt;/strong&gt;、&lt;strong&gt;价值至上&lt;/strong&gt;的原则，避免&lt;strong&gt;纯 AI 科研化&lt;/strong&gt;，脱离实际场景做 AI 技术选型&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Model Structure</title>
    <link href="https://blog.zhongmingmao.top/2024/07/10/llm-core-model-structure/"/>
    <id>https://blog.zhongmingmao.top/2024/07/10/llm-core-model-structure/</id>
    <published>2024-07-09T16:06:25.000Z</published>
    <updated>2024-08-27T15:28:28.065Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;模型文件&quot;&gt;&lt;a href=&quot;#模型文件&quot; class=&quot;headerlink&quot; title=&quot;模型文件&quot;&gt;&lt;/a&gt;模型文件&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;模型文件，也叫&lt;strong&gt;模型权重&lt;/strong&gt;，里面大部分空间存放的是&lt;strong&gt;模型参数&lt;/strong&gt; - 即&lt;strong&gt;权重&lt;/strong&gt;（Weights）和&lt;strong&gt;偏置&lt;/strong&gt;（Biases）&lt;/li&gt;
&lt;li&gt;还有其它信息，如&lt;strong&gt;优化器状态&lt;/strong&gt;和&lt;strong&gt;元数据&lt;/strong&gt;等&lt;/li&gt;
&lt;li&gt;文件格式&lt;ul&gt;
&lt;li&gt;使用 &lt;strong&gt;PyTorch&lt;/strong&gt;，后缀为 &lt;code&gt;.pth&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;TensorFlow&lt;/strong&gt; 或者 &lt;strong&gt;Hugging Face Transformers&lt;/strong&gt;，后缀为 &lt;code&gt;.bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在模型&lt;strong&gt;预训练&lt;/strong&gt;后，可以保存模型，在&lt;strong&gt;生产&lt;/strong&gt;环境，&lt;strong&gt;不建议保存模型架构&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;与 &lt;strong&gt;Python 版本&lt;/strong&gt;和&lt;strong&gt;模型定义的代码&lt;/strong&gt;紧密相关，可能存在&lt;strong&gt;兼容性&lt;/strong&gt;问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Decoder Only</title>
    <link href="https://blog.zhongmingmao.top/2024/07/09/llm-core-decoder-only/"/>
    <id>https://blog.zhongmingmao.top/2024/07/09/llm-core-decoder-only/</id>
    <published>2024-07-08T16:06:25.000Z</published>
    <updated>2024-08-26T09:40:47.186Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;模型架构&quot;&gt;&lt;a href=&quot;#模型架构&quot; class=&quot;headerlink&quot; title=&quot;模型架构&quot;&gt;&lt;/a&gt;模型架构&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;已经演化出很多 Transformer 变体，用来适应不同的任务和性能需求&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;架构名称&lt;/th&gt;
&lt;th&gt;特点&lt;/th&gt;
&lt;th&gt;主要应用&lt;/th&gt;
&lt;th&gt;与原始 Transformer 的关系&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;原始 Transformer&lt;/td&gt;
&lt;td&gt;编码器-解码器结构&lt;/td&gt;
&lt;td&gt;机器翻译、文本摘要&lt;/td&gt;
&lt;td&gt;基础模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Decoder-only&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;只包含解码器&lt;/td&gt;
&lt;td&gt;文本生成&lt;/td&gt;
&lt;td&gt;去除了编码器部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Encoder-only&lt;/td&gt;
&lt;td&gt;只包含编码器&lt;/td&gt;
&lt;td&gt;文本分类、信息提取&lt;/td&gt;
&lt;td&gt;去除了解码器部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Transformer-XL&lt;/td&gt;
&lt;td&gt;加入循环机制&lt;/td&gt;
&lt;td&gt;长文本处理&lt;/td&gt;
&lt;td&gt;扩展了处理长序列的能力&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sparse Transformer&lt;/td&gt;
&lt;td&gt;引入稀疏注意力机制&lt;/td&gt;
&lt;td&gt;长序列处理&lt;/td&gt;
&lt;td&gt;优化了注意力计算效率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Universal Transformer&lt;/td&gt;
&lt;td&gt;递归的编码器结构&lt;/td&gt;
&lt;td&gt;各种序列处理&lt;/td&gt;
&lt;td&gt;引入递归机制，多次使用相同的参数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Conformer&lt;/td&gt;
&lt;td&gt;结合 CNN 和 Transformer 优势&lt;/td&gt;
&lt;td&gt;音频处理、语音识别&lt;/td&gt;
&lt;td&gt;引入卷积层处理局部特征&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vision Transformer&lt;/td&gt;
&lt;td&gt;应用于视觉领域&lt;/td&gt;
&lt;td&gt;图像分类、视觉任务&lt;/td&gt;
&lt;td&gt;将图像块处理为序列的 Transformer 编码器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Switch Transformer&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;使用稀疏性路由机制&lt;/td&gt;
&lt;td&gt;大规模模型训练&lt;/td&gt;
&lt;td&gt;提高了模型的可扩展性和效率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performer&lt;/td&gt;
&lt;td&gt;使用随机特征映射技术近似注意力机制&lt;/td&gt;
&lt;td&gt;处理非常长的序列&lt;/td&gt;
&lt;td&gt;降低计算负担，提高处理效率&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Transformer" scheme="https://blog.zhongmingmao.top/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Transformer</title>
    <link href="https://blog.zhongmingmao.top/2024/07/08/llm-core-transformer/"/>
    <id>https://blog.zhongmingmao.top/2024/07/08/llm-core-transformer/</id>
    <published>2024-07-07T16:06:25.000Z</published>
    <updated>2024-08-26T09:37:55.867Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;不论是 &lt;strong&gt;GRU&lt;/strong&gt; 还是 &lt;strong&gt;LSTM&lt;/strong&gt; 都面临&lt;strong&gt;梯度消失&lt;/strong&gt;和&lt;strong&gt;梯度爆炸&lt;/strong&gt;的问题&lt;/li&gt;
&lt;li&gt;RNN 必须&lt;strong&gt;按照顺序处理&lt;/strong&gt;序列中的每个元素，&lt;strong&gt;无法并发处理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;RNN 还有&lt;strong&gt;长依赖&lt;/strong&gt;问题，虽然可以处理&lt;strong&gt;长序列&lt;/strong&gt;，但&lt;strong&gt;实战效果不佳&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Attention Is All You Need - &lt;a href=&quot;http://arxiv.org/pdf/1706.03762&quot;&gt;http://arxiv.org/pdf/1706.03762&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Seq2Seq" scheme="https://blog.zhongmingmao.top/tags/seq2seq/"/>
    
    <category term="Transformer" scheme="https://blog.zhongmingmao.top/tags/transformer/"/>
    
    <category term="Attention" scheme="https://blog.zhongmingmao.top/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Seq2Seq</title>
    <link href="https://blog.zhongmingmao.top/2024/07/07/llm-core-seq2seq/"/>
    <id>https://blog.zhongmingmao.top/2024/07/07/llm-core-seq2seq/</id>
    <published>2024-07-06T16:06:25.000Z</published>
    <updated>2024-08-24T16:38:44.933Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;简单介绍&quot;&gt;&lt;a href=&quot;#简单介绍&quot; class=&quot;headerlink&quot; title=&quot;简单介绍&quot;&gt;&lt;/a&gt;简单介绍&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt; 的主要能力是将&lt;strong&gt;词汇&lt;/strong&gt;放在&lt;strong&gt;多维空间&lt;/strong&gt;中，&lt;strong&gt;相似的词汇&lt;/strong&gt;会被放在&lt;strong&gt;邻近的位置&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Seq2Seq 不仅能&lt;strong&gt;理解词汇&lt;/strong&gt;，还能将词汇&lt;strong&gt;串联&lt;/strong&gt;成完整的&lt;strong&gt;句子&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Seq2Seq 即&lt;strong&gt;从一个序列到另一个序列的转换&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;不仅仅能&lt;strong&gt;理解单词之间的关系&lt;/strong&gt;，还能把整个句子的意思&lt;strong&gt;打包&lt;/strong&gt;，并&lt;strong&gt;解压&lt;/strong&gt;成另一种形式的表达&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Seq2Seq 的核心角色 - &lt;strong&gt;编码器&lt;/strong&gt;（Encoder） + &lt;strong&gt;解码器&lt;/strong&gt;（Decoder）&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Role&lt;/th&gt;
&lt;th&gt;Desc&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Encoder&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;理解和压缩信息&lt;/strong&gt; - 把一封长信函整理成一个精简的&lt;strong&gt;摘要&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;将&lt;strong&gt;摘要&lt;/strong&gt;打开，并翻译成另一种语言或形式的&lt;strong&gt;完整信息&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Seq2Seq" scheme="https://blog.zhongmingmao.top/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Word2Vec</title>
    <link href="https://blog.zhongmingmao.top/2024/07/06/llm-core-word2vec/"/>
    <id>https://blog.zhongmingmao.top/2024/07/06/llm-core-word2vec/</id>
    <published>2024-07-05T16:06:25.000Z</published>
    <updated>2024-08-24T09:43:10.720Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Word2Vec&quot;&gt;&lt;a href=&quot;#Word2Vec&quot; class=&quot;headerlink&quot; title=&quot;Word2Vec&quot;&gt;&lt;/a&gt;Word2Vec&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;在 &lt;strong&gt;NLP&lt;/strong&gt; 中，在&lt;strong&gt;文本预处理&lt;/strong&gt;后，进行&lt;strong&gt;特征提取&lt;/strong&gt;，涉及到将&lt;strong&gt;词语&lt;/strong&gt;转化成&lt;strong&gt;数值&lt;/strong&gt;的形式，&lt;strong&gt;方便计算机理解&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Word2Vec 的目的 - 将&lt;strong&gt;词语&lt;/strong&gt;转换成&lt;strong&gt;向量&lt;/strong&gt;形式，使得&lt;strong&gt;计算机&lt;/strong&gt;能够&lt;strong&gt;理解&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;通过学习&lt;strong&gt;大量文本数据&lt;/strong&gt;，捕捉到&lt;strong&gt;词语之间的上下文关系&lt;/strong&gt;，进而生成&lt;strong&gt;词的高维表示&lt;/strong&gt; - 即&lt;strong&gt;词向量&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Word2Vec" scheme="https://blog.zhongmingmao.top/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - NLP</title>
    <link href="https://blog.zhongmingmao.top/2024/07/05/llm-core-nlp/"/>
    <id>https://blog.zhongmingmao.top/2024/07/05/llm-core-nlp/</id>
    <published>2024-07-04T16:06:25.000Z</published>
    <updated>2024-08-22T06:02:09.742Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;NLP 的研究目的是让计算机能够&lt;strong&gt;理解&lt;/strong&gt;、&lt;strong&gt;解释&lt;/strong&gt;和&lt;strong&gt;生成&lt;/strong&gt;人类语言，一般包含 4 个步骤&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Step&lt;/th&gt;
&lt;th&gt;Desc&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;文本预处理&lt;/td&gt;
&lt;td&gt;将&lt;strong&gt;原始文本&lt;/strong&gt;转换成&lt;strong&gt;机器容易理解&lt;/strong&gt;的格式&lt;br /&gt;&lt;strong&gt;分词&lt;/strong&gt;（单词或短语）、&lt;strong&gt;去除停用词&lt;/strong&gt;、&lt;strong&gt;词干提取&lt;/strong&gt;、&lt;strong&gt;词性标注&lt;/strong&gt;等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;特征提取&lt;/td&gt;
&lt;td&gt;从处理过的文本中提取特征，以便用于机器学习模型&lt;br /&gt;将&lt;strong&gt;文本&lt;/strong&gt;转换成&lt;strong&gt;数值&lt;/strong&gt;形式 - &lt;strong&gt;向量化&lt;/strong&gt; - &lt;strong&gt;词袋模型&lt;/strong&gt; or &lt;strong&gt;&lt;u&gt;词嵌入&lt;/u&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;模型训练&lt;/td&gt;
&lt;td&gt;使用提取到的&lt;strong&gt;特征&lt;/strong&gt;和相应的&lt;strong&gt;机器学习算法&lt;/strong&gt;来训练模型&lt;br /&gt;分类器、回归模型、聚类算法等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;评估与应用&lt;/td&gt;
&lt;td&gt;评估模型的&lt;strong&gt;性能&lt;/strong&gt;，并在实际应用中使用模型来&lt;strong&gt;解释&lt;/strong&gt;、&lt;strong&gt;生成&lt;/strong&gt;或&lt;strong&gt;翻译&lt;/strong&gt;文本&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;应用场景 - 搜索引擎 &amp;#x2F; 语音转换 &amp;#x2F; 文本翻译 &amp;#x2F; 系统问答&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - RNN</title>
    <link href="https://blog.zhongmingmao.top/2024/07/04/llm-core-rnn/"/>
    <id>https://blog.zhongmingmao.top/2024/07/04/llm-core-rnn/</id>
    <published>2024-07-03T16:06:25.000Z</published>
    <updated>2024-08-18T17:01:37.767Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;RNN 主要用来处理&lt;strong&gt;序列数据&lt;/strong&gt;，目前大部分 &lt;strong&gt;LLM&lt;/strong&gt; 都是基于 &lt;strong&gt;Transformer&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;通过学习 RNN，有助于理解 Transformer&lt;ul&gt;
&lt;li&gt;有助于理解&lt;strong&gt;神经网络&lt;/strong&gt;如何处理&lt;strong&gt;序列中的依赖关系&lt;/strong&gt;、&lt;strong&gt;记忆过去的信息&lt;/strong&gt;，并在此基础上&lt;strong&gt;生成预测&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;有助于理解关键问题 - &lt;strong&gt;梯度消失&lt;/strong&gt; &amp;#x2F; &lt;strong&gt;梯度爆炸&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Machine Learning" scheme="https://blog.zhongmingmao.top/tags/machine-learning/"/>
    
    <category term="RNN" scheme="https://blog.zhongmingmao.top/tags/rnn/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Machine Learning Algorithm</title>
    <link href="https://blog.zhongmingmao.top/2024/07/03/llm-core-ml-algorithm/"/>
    <id>https://blog.zhongmingmao.top/2024/07/03/llm-core-ml-algorithm/</id>
    <published>2024-07-02T16:06:25.000Z</published>
    <updated>2024-08-18T12:39:46.686Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;线性回归是一种&lt;strong&gt;预测分析&lt;/strong&gt;技术，用于研究两个或者多个变量之间的关系&lt;/li&gt;
&lt;li&gt;尝试用一条&lt;strong&gt;直线&lt;/strong&gt;（二维）或者一个&lt;strong&gt;平面&lt;/strong&gt;（三维）的去&lt;strong&gt;拟合&lt;/strong&gt;数据点&lt;/li&gt;
&lt;li&gt;这条&lt;strong&gt;直线&lt;/strong&gt;或者&lt;strong&gt;平面&lt;/strong&gt;，可以用来&lt;strong&gt;预测&lt;/strong&gt;或者估计一个变量基于另一个变量的值&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Machine Learning" scheme="https://blog.zhongmingmao.top/tags/machine-learning/"/>
    
    <category term="SVM" scheme="https://blog.zhongmingmao.top/tags/svm/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Machine Learning Concept</title>
    <link href="https://blog.zhongmingmao.top/2024/07/02/llm-core-ml-concept/"/>
    <id>https://blog.zhongmingmao.top/2024/07/02/llm-core-ml-concept/</id>
    <published>2024-07-01T16:06:25.000Z</published>
    <updated>2024-08-18T02:07:14.895Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;机器学习&quot;&gt;&lt;a href=&quot;#机器学习&quot; class=&quot;headerlink&quot; title=&quot;机器学习&quot;&gt;&lt;/a&gt;机器学习&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;机器学习是让计算机&lt;strong&gt;利用数据&lt;/strong&gt;来&lt;strong&gt;学习&lt;/strong&gt;如何&lt;strong&gt;完成任务&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;机器学习允许计算机通过&lt;strong&gt;分析&lt;/strong&gt;和&lt;strong&gt;学习&lt;/strong&gt;数据来&lt;strong&gt;自我改进&lt;/strong&gt;以及&lt;strong&gt;作出决策&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Machine Learning" scheme="https://blog.zhongmingmao.top/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>LLM - API</title>
    <link href="https://blog.zhongmingmao.top/2024/07/01/llm-api/"/>
    <id>https://blog.zhongmingmao.top/2024/07/01/llm-api/</id>
    <published>2024-06-30T16:06:25.000Z</published>
    <updated>2024-08-17T14:08:28.541Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;LLM 是没有 &lt;strong&gt;Web API&lt;/strong&gt; 的，需要进行一次&lt;strong&gt;封装&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;将 LLM 的核心接口封装成 Web API 来为用户提供服务 - &lt;strong&gt;必经之路&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Python" scheme="https://blog.zhongmingmao.top/tags/python/"/>
    
    <category term="FastAPI" scheme="https://blog.zhongmingmao.top/tags/fastapi/"/>
    
    <category term="Uvicorn" scheme="https://blog.zhongmingmao.top/tags/uvicorn/"/>
    
  </entry>
  
  <entry>
    <title>LLM RAG - ChatGLM3-6B + LangChain + Faiss</title>
    <link href="https://blog.zhongmingmao.top/2024/06/30/llm-rag-chatglm3-6b-langchain-faiss/"/>
    <id>https://blog.zhongmingmao.top/2024/06/30/llm-rag-chatglm3-6b-langchain-faiss/</id>
    <published>2024-06-29T16:06:25.000Z</published>
    <updated>2024-08-17T10:09:12.128Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;RAG&quot;&gt;&lt;a href=&quot;#RAG&quot; class=&quot;headerlink&quot; title=&quot;RAG&quot;&gt;&lt;/a&gt;RAG&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;使用&lt;strong&gt;知识库&lt;/strong&gt;，用来增强 LLM &lt;strong&gt;信息检索&lt;/strong&gt;的能力&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;知识准确&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;先把&lt;strong&gt;知识&lt;/strong&gt;进行&lt;strong&gt;向量化&lt;/strong&gt;，存储到&lt;strong&gt;向量数据库&lt;/strong&gt;中&lt;/li&gt;
&lt;li&gt;使用的时候通过&lt;strong&gt;向量检索&lt;/strong&gt;从向量数据库中将知识检索出来，确保知识的准确性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新频率快&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;当发现知识库里面的&lt;strong&gt;知识不全&lt;/strong&gt;时，可以&lt;strong&gt;随时补充&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;不需要像&lt;strong&gt;微调&lt;/strong&gt;一样，重新跑&lt;strong&gt;微调&lt;/strong&gt;任务、&lt;strong&gt;验证&lt;/strong&gt;结果、重新&lt;strong&gt;部署&lt;/strong&gt;等&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="LangChain" scheme="https://blog.zhongmingmao.top/tags/langchain/"/>
    
    <category term="AI Agent" scheme="https://blog.zhongmingmao.top/tags/ai-agent/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
    <category term="Faiss" scheme="https://blog.zhongmingmao.top/tags/faiss/"/>
    
  </entry>
  
  <entry>
    <title>LLM PEFT - ChatGLM3-6B + LoRA</title>
    <link href="https://blog.zhongmingmao.top/2024/06/29/llm-peft-chatglm3-6b-lora/"/>
    <id>https://blog.zhongmingmao.top/2024/06/29/llm-peft-chatglm3-6b-lora/</id>
    <published>2024-06-28T16:06:25.000Z</published>
    <updated>2024-08-16T17:08:30.135Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;通用-LLM&quot;&gt;&lt;a href=&quot;#通用-LLM&quot; class=&quot;headerlink&quot; title=&quot;通用 LLM&quot;&gt;&lt;/a&gt;通用 LLM&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;千亿大模型（130B、ChatGPT）和小规模的大模型（6B、LLaMA2）都是通用 LLM&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;通用 LLM&lt;/strong&gt; 都是通过&lt;strong&gt;常识&lt;/strong&gt;进行&lt;strong&gt;预训练&lt;/strong&gt;的&lt;/li&gt;
&lt;li&gt;在实际使用过程中，需要 LLM 具备某一特定&lt;strong&gt;领域知识&lt;/strong&gt;的能力 - 对 LLM 的能力进行&lt;strong&gt;增强&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="LangChain" scheme="https://blog.zhongmingmao.top/tags/langchain/"/>
    
    <category term="AI Agent" scheme="https://blog.zhongmingmao.top/tags/ai-agent/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
    <category term="LoRA" scheme="https://blog.zhongmingmao.top/tags/lora/"/>
    
    <category term="PEFT" scheme="https://blog.zhongmingmao.top/tags/peft/"/>
    
  </entry>
  
  <entry>
    <title>LLM Deploy - ChatGLM3-6B</title>
    <link href="https://blog.zhongmingmao.top/2024/06/28/llm-deploy-chatglm3-6b/"/>
    <id>https://blog.zhongmingmao.top/2024/06/28/llm-deploy-chatglm3-6b/</id>
    <published>2024-06-27T16:06:25.000Z</published>
    <updated>2024-08-16T08:03:15.938Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;LLM-选择&quot;&gt;&lt;a href=&quot;#LLM-选择&quot; class=&quot;headerlink&quot; title=&quot;LLM 选择&quot;&gt;&lt;/a&gt;LLM 选择&lt;/h1&gt;&lt;h2 id=&quot;核心玩家&quot;&gt;&lt;a href=&quot;#核心玩家&quot; class=&quot;headerlink&quot; title=&quot;核心玩家&quot;&gt;&lt;/a&gt;核心玩家&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;厂家很多，但没多少真正在研究技术 - 成本 - 不少厂商是基于 &lt;strong&gt;LLaMA&lt;/strong&gt; 套壳&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240815080217105.png&quot; alt=&quot;image-20240815080217105&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="LangChain" scheme="https://blog.zhongmingmao.top/tags/langchain/"/>
    
    <category term="AI Agent" scheme="https://blog.zhongmingmao.top/tags/ai-agent/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>LLM - LangChain + RAG</title>
    <link href="https://blog.zhongmingmao.top/2024/06/27/llm-langchain-rag/"/>
    <id>https://blog.zhongmingmao.top/2024/06/27/llm-langchain-rag/</id>
    <published>2024-06-26T16:06:25.000Z</published>
    <updated>2024-08-14T12:09:09.499Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;局限&quot;&gt;&lt;a href=&quot;#局限&quot; class=&quot;headerlink&quot; title=&quot;局限&quot;&gt;&lt;/a&gt;局限&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;大模型的核心能力 - &lt;strong&gt;意图理解&lt;/strong&gt; + &lt;strong&gt;文本生成&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;局限&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;数据的&lt;strong&gt;及时性&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;大部分 AI 大模型都是&lt;strong&gt;预训练&lt;/strong&gt;的，如果要问一些最新的消息，大模型是不知道的&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;复杂任务&lt;/strong&gt;处理&lt;/td&gt;
&lt;td&gt;AI 大模型在&lt;strong&gt;问答&lt;/strong&gt;方面表现出色，但不总是能够处理复杂任务&lt;br /&gt;AI 大模型主要是基于&lt;strong&gt;文本&lt;/strong&gt;的交互（&lt;strong&gt;多模态&lt;/strong&gt;除外）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;代码&lt;/strong&gt;生成与下载&lt;/td&gt;
&lt;td&gt;根据需求描述生成对应的代码，并提供下载链接 - 暂时不支持&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;与&lt;strong&gt;企业应用&lt;/strong&gt;场景的&lt;strong&gt;集成&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;读取关系型数据库里面的数据，并根据提示进行任务处理 - 暂时不支持&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;ol&gt;
&lt;li&gt;在实际应用过程中，输入数据和输出数据，不仅仅是纯文本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI Agent&lt;/strong&gt; - 需要&lt;strong&gt;解析&lt;/strong&gt;用户的输入输出&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="LangChain" scheme="https://blog.zhongmingmao.top/tags/langchain/"/>
    
    <category term="AI Agent" scheme="https://blog.zhongmingmao.top/tags/ai-agent/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>LLM - Prompt</title>
    <link href="https://blog.zhongmingmao.top/2024/06/26/llm-prompt/"/>
    <id>https://blog.zhongmingmao.top/2024/06/26/llm-prompt/</id>
    <published>2024-06-25T16:06:25.000Z</published>
    <updated>2024-08-08T04:24:39.420Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Prompt&quot;&gt;&lt;a href=&quot;#Prompt&quot; class=&quot;headerlink&quot; title=&quot;Prompt&quot;&gt;&lt;/a&gt;Prompt&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;是否充分使用好 AI 大模型，提示是关键&lt;/li&gt;
&lt;li&gt;OpenAI&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;del&gt;question &amp;#x2F; answer&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;prompt &amp;#x2F; completion - 给 LLM 一个提示，让 LLM 进行补全&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;LLM 训练原理&lt;ul&gt;
&lt;li&gt;GPT 系列模型基于 &lt;code&gt;Transformer&lt;/code&gt; 架构的&lt;code&gt;解码器&lt;/code&gt;机制，使用&lt;code&gt;自回归无监督&lt;/code&gt;方式进行&lt;code&gt;预训练&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;训练过程 - &lt;code&gt;大量&lt;/code&gt;的文本输入，不断进行&lt;code&gt;记忆&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;相比于监督学习，&lt;code&gt;训练效率更低&lt;/code&gt;，但&lt;code&gt;训练过程简单&lt;/code&gt;，可以喂大量的文本语料，上限比较高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;completion&lt;ul&gt;
&lt;li&gt;根据训练过的&lt;code&gt;记忆&lt;/code&gt;，一个字一个字地计算&lt;code&gt;概率&lt;/code&gt;，取&lt;code&gt;概率最大&lt;/code&gt;的那个字进行输出&lt;/li&gt;
&lt;li&gt;因此有人吐槽 LLM 输出很慢 - &lt;code&gt;逐字计算&lt;/code&gt;并输出&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>LLM - ChatGPT</title>
    <link href="https://blog.zhongmingmao.top/2024/06/25/llm-chatgpt/"/>
    <id>https://blog.zhongmingmao.top/2024/06/25/llm-chatgpt/</id>
    <published>2024-06-24T16:06:25.000Z</published>
    <updated>2024-08-08T04:24:31.047Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Timeline&quot;&gt;&lt;a href=&quot;#Timeline&quot; class=&quot;headerlink&quot; title=&quot;Timeline&quot;&gt;&lt;/a&gt;Timeline&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://llm-1253868755.cos.ap-guangzhou.myqcloud.com/image-20240715121726607.png&quot; alt=&quot;image-20240715121726607&quot;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;OpenAI 在 &lt;code&gt;NLP&lt;/code&gt; 领域取得了突破性进展&lt;/li&gt;
&lt;li&gt;ChatGPT 背后包含了一系列的资源整合 - 技术、资源、大厂背书、国际巨头的通力合作 - 工程 + 产品&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="ChatGPT" scheme="https://blog.zhongmingmao.top/tags/chatgpt/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes - Helm Doc</title>
    <link href="https://blog.zhongmingmao.top/2023/05/11/cloud-native-foundation-k8s-helm-doc/"/>
    <id>https://blog.zhongmingmao.top/2023/05/11/cloud-native-foundation-k8s-helm-doc/</id>
    <published>2023-05-10T16:06:25.000Z</published>
    <updated>2024-07-07T07:40:13.319Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;Chart&lt;ul&gt;
&lt;li&gt;包含在 Kubernetes 集群内部运行的应用程序、工具和服务所需的&lt;code&gt;所有资源定义&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repository&lt;ul&gt;
&lt;li&gt;用于存放和共享 Chart&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Release&lt;ul&gt;
&lt;li&gt;运行在 Kubernetes 集群中的 Chart 实例&lt;/li&gt;
&lt;li&gt;一个 Chart 可以在同一个集群中被安装多次，每次安装都会创建一个 Release&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Cloud Native" scheme="https://blog.zhongmingmao.top/categories/cloud-native/"/>
    
    <category term="Cloud Native Foundation" scheme="https://blog.zhongmingmao.top/categories/cloud-native/cloud-native-foundation/"/>
    
    <category term="Kubernetes" scheme="https://blog.zhongmingmao.top/categories/cloud-native/cloud-native-foundation/kubernetes/"/>
    
    
    <category term="Cloud Native" scheme="https://blog.zhongmingmao.top/tags/cloud-native/"/>
    
    <category term="Kubernetes" scheme="https://blog.zhongmingmao.top/tags/kubernetes/"/>
    
    <category term="Cloud Native Foundation" scheme="https://blog.zhongmingmao.top/tags/cloud-native-foundation/"/>
    
  </entry>
  
</feed>
