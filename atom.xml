<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ByteCoding</title>
  
  
  <link href="https://blog.zhongmingmao.top/atom.xml" rel="self"/>
  
  <link href="https://blog.zhongmingmao.top/"/>
  <updated>2024-10-23T06:08:30.462Z</updated>
  <id>https://blog.zhongmingmao.top/</id>
  
  <author>
    <name>zhongmingmao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RAG - Optimization + Evaluation</title>
    <link href="https://blog.zhongmingmao.top/2024/08/15/rag-optimization-evaluation/"/>
    <id>https://blog.zhongmingmao.top/2024/08/15/rag-optimization-evaluation/</id>
    <published>2024-08-14T16:06:25.000Z</published>
    <updated>2024-10-23T06:08:30.462Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;RAG&quot;&gt;&lt;a href=&quot;#RAG&quot; class=&quot;headerlink&quot; title=&quot;RAG&quot;&gt;&lt;/a&gt;RAG&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/image-20241022233308732.png&quot; alt=&quot;image-20241022233308732&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - LLM + Prompt Engineering</title>
    <link href="https://blog.zhongmingmao.top/2024/08/14/rag-llm-prompt-engineering/"/>
    <id>https://blog.zhongmingmao.top/2024/08/14/rag-llm-prompt-engineering/</id>
    <published>2024-08-13T16:06:25.000Z</published>
    <updated>2024-10-22T15:13:32.414Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;RAG-生成流程&quot;&gt;&lt;a href=&quot;#RAG-生成流程&quot; class=&quot;headerlink&quot; title=&quot;RAG 生成流程&quot;&gt;&lt;/a&gt;RAG 生成流程&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://rag-1253868755.cos.ap-guangzhou.myqcloud.com/image-20241022212827467.png&quot; alt=&quot;image-20241022212827467&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
    <category term="Prompting" scheme="https://blog.zhongmingmao.top/tags/prompting/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Hybrid retrieval + Rerank</title>
    <link href="https://blog.zhongmingmao.top/2024/08/13/rag-hybrid-retrieval-rerank/"/>
    <id>https://blog.zhongmingmao.top/2024/08/13/rag-hybrid-retrieval-rerank/</id>
    <published>2024-08-12T16:06:25.000Z</published>
    <updated>2024-10-21T17:19:12.136Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;向量检索&quot;&gt;&lt;a href=&quot;#向量检索&quot; class=&quot;headerlink&quot; title=&quot;向量检索&quot;&gt;&lt;/a&gt;向量检索&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;当前主流的 RAG 检索方式主要采用&lt;strong&gt;向量检索&lt;/strong&gt;，通过&lt;strong&gt;语义相似度&lt;/strong&gt;来匹配 &lt;strong&gt;Chunk&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;向量检索&lt;strong&gt;并非万能&lt;/strong&gt;，在某些场景下无法替代&lt;strong&gt;传统关键词检索&lt;/strong&gt;的优势&lt;ul&gt;
&lt;li&gt;当需要&lt;strong&gt;精准搜索&lt;/strong&gt;的时候，向量检索的&lt;strong&gt;准确性&lt;/strong&gt;就往往不如关键词检索&lt;/li&gt;
&lt;li&gt;当用户输入的&lt;strong&gt;问题&lt;/strong&gt;非常&lt;strong&gt;简短&lt;/strong&gt;，&lt;strong&gt;语义匹配&lt;/strong&gt;的效果可能&lt;strong&gt;不尽理想&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键词检索&lt;/strong&gt;的适用场景&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;精确&lt;/strong&gt;匹配&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;少量字符&lt;/strong&gt;的匹配 - 不适合用向量检索&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;低频词汇&lt;/strong&gt;的匹配&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
    <category term="Rerank" scheme="https://blog.zhongmingmao.top/tags/rerank/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Vector Stores</title>
    <link href="https://blog.zhongmingmao.top/2024/08/12/rag-vector-stores/"/>
    <id>https://blog.zhongmingmao.top/2024/08/12/rag-vector-stores/</id>
    <published>2024-08-11T16:06:25.000Z</published>
    <updated>2024-10-17T13:53:42.010Z</updated>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Embedding&quot;&gt;&lt;a href=&quot;#Embedding&quot; class=&quot;headerlink&quot; title=&quot;Embedding&quot;&gt;&lt;/a&gt;Embedding&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://rag-1253868755.cos.ap-gu</summary>
      
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
    <category term="Vector" scheme="https://blog.zhongmingmao.top/tags/vector/"/>
    
  </entry>
  
  <entry>
    <title>RAG - KG-RAG</title>
    <link href="https://blog.zhongmingmao.top/2024/08/11/rag-kg-rag/"/>
    <id>https://blog.zhongmingmao.top/2024/08/11/rag-kg-rag/</id>
    <published>2024-08-10T16:06:25.000Z</published>
    <updated>2024-09-03T11:26:34.120Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Knowledge-Graph&quot;&gt;&lt;a href=&quot;#Knowledge-Graph&quot; class=&quot;headerlink&quot; title=&quot;Knowledge Graph&quot;&gt;&lt;/a&gt;Knowledge Graph&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;知识图谱也称为&lt;strong&gt;语义网络&lt;/strong&gt;，表示现实世界&lt;strong&gt;实体&lt;/strong&gt;的网络，并说明它们之间的关系&lt;/li&gt;
&lt;li&gt;信息通常存储在&lt;strong&gt;图形数据库&lt;/strong&gt;中，并以图形结构直观呈现&lt;/li&gt;
&lt;li&gt;知识图谱由三部分组成 - &lt;strong&gt;节点&lt;/strong&gt; + &lt;strong&gt;边&lt;/strong&gt; + &lt;strong&gt;标签&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
    <category term="KG-RAG" scheme="https://blog.zhongmingmao.top/tags/kg-rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Chatbot</title>
    <link href="https://blog.zhongmingmao.top/2024/08/10/rag-chatbot/"/>
    <id>https://blog.zhongmingmao.top/2024/08/10/rag-chatbot/</id>
    <published>2024-08-09T16:06:25.000Z</published>
    <updated>2024-09-03T06:19:17.435Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Fine-tuning-vs-RAG&quot;&gt;&lt;a href=&quot;#Fine-tuning-vs-RAG&quot; class=&quot;headerlink&quot; title=&quot;Fine-tuning vs RAG&quot;&gt;&lt;/a&gt;Fine-tuning vs RAG&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;核心诉求 - &lt;strong&gt;实时更新&lt;/strong&gt;知识库，不需要模型去深度探讨问题，使用&lt;strong&gt;已有知识经验&lt;/strong&gt;去解答问题&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Fine-tuning&lt;/th&gt;
&lt;th&gt;RAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;知识整合&lt;/td&gt;
&lt;td&gt;直接把数据融入到&lt;strong&gt;模型参数&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;存储在&lt;strong&gt;外部知识库&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;知识更新&lt;/td&gt;
&lt;td&gt;每次更新内容都需要&lt;strong&gt;重新训练模型&lt;/strong&gt;，更新成本&lt;strong&gt;高&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;只需要在外部知识库&lt;strong&gt;插入记录&lt;/strong&gt;，更新成本&lt;strong&gt;低&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;响应速度&lt;/td&gt;
&lt;td&gt;很快，&lt;strong&gt;直接给出回答&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;现在&lt;strong&gt;外部知识库&lt;/strong&gt;进行&lt;strong&gt;检索&lt;/strong&gt;，然后再&lt;strong&gt;生成&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;实时更新&lt;/td&gt;
&lt;td&gt;很难做到实时更新&lt;/td&gt;
&lt;td&gt;外部知识库可以&lt;strong&gt;实时更新&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;人为干预&lt;/td&gt;
&lt;td&gt;只能通过 &lt;strong&gt;Prompt&lt;/strong&gt; 干预&lt;/td&gt;
&lt;td&gt;可以通过&lt;strong&gt;外部知识库的语料&lt;/strong&gt;和 &lt;strong&gt;Prompt&lt;/strong&gt; 控制&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;领域定制&lt;/td&gt;
&lt;td&gt;可以针对&lt;strong&gt;特定领域&lt;/strong&gt;进行&lt;strong&gt;深度定制&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;依赖&lt;strong&gt;通用模型能力&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
    <category term="ChatOps" scheme="https://blog.zhongmingmao.top/tags/chatops/"/>
    
    <category term="Chatbot" scheme="https://blog.zhongmingmao.top/tags/chatbot/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Chunking + Embedding</title>
    <link href="https://blog.zhongmingmao.top/2024/08/09/rag-chunking-embedding/"/>
    <id>https://blog.zhongmingmao.top/2024/08/09/rag-chunking-embedding/</id>
    <published>2024-08-08T16:06:25.000Z</published>
    <updated>2024-09-03T02:17:03.107Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Chunking&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Documents 经过解析后，通过 &lt;strong&gt;Chunking&lt;/strong&gt; 将信息内容划分为适当大小的 &lt;strong&gt;Chunks&lt;/strong&gt; - 能够&lt;strong&gt;高效处理&lt;/strong&gt;和&lt;strong&gt;精准检索&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Chunk 的本质在于依据一定的&lt;strong&gt;逻辑&lt;/strong&gt;和&lt;strong&gt;语义原则&lt;/strong&gt;，将&lt;strong&gt;长文本&lt;/strong&gt;拆解为&lt;strong&gt;更小的单元&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Chunking 有多种&lt;strong&gt;策略&lt;/strong&gt;，各有&lt;strong&gt;侧重&lt;/strong&gt;，选择&lt;strong&gt;适合特定场景&lt;/strong&gt;的 Chunking 策略，有助于&lt;strong&gt;提升 RAG 召回率&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Frameworks</title>
    <link href="https://blog.zhongmingmao.top/2024/08/08/rag-frameworks/"/>
    <id>https://blog.zhongmingmao.top/2024/08/08/rag-frameworks/</id>
    <published>2024-08-07T16:06:25.000Z</published>
    <updated>2024-08-31T10:50:48.896Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;Retrieval-Augmented Generation (RAG) is an AI framework that enhances the capabilities of large language models (LLMs) by &lt;strong&gt;incorporating external knowledge sources&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;It helps overcome limitations such as knowledge &lt;strong&gt;cutoff dates&lt;/strong&gt; and reduces the risk of &lt;strong&gt;hallucinations&lt;/strong&gt; in LLM outputs.&lt;/li&gt;
&lt;li&gt;RAG works by &lt;strong&gt;retrieving relevant information&lt;/strong&gt; from a &lt;strong&gt;knowledge base&lt;/strong&gt; and using it to &lt;strong&gt;augment the LLM’s input&lt;/strong&gt;, allowing the model to generate more &lt;strong&gt;accurate&lt;/strong&gt;, &lt;strong&gt;up-to-date&lt;/strong&gt;, and &lt;strong&gt;contextually relevant&lt;/strong&gt; responses.&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Data Processing</title>
    <link href="https://blog.zhongmingmao.top/2024/08/07/rag-data-processing/"/>
    <id>https://blog.zhongmingmao.top/2024/08/07/rag-data-processing/</id>
    <published>2024-08-06T16:06:25.000Z</published>
    <updated>2024-08-31T09:07:21.402Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;数据存储&quot;&gt;&lt;a href=&quot;#数据存储&quot; class=&quot;headerlink&quot; title=&quot;数据存储&quot;&gt;&lt;/a&gt;数据存储&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;LLM 变成&lt;strong&gt;生产力&lt;/strong&gt;，有两个制约因素 - 交互过程中的&lt;strong&gt;长文本&lt;/strong&gt; + 内容的&lt;strong&gt;实时更新&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;传统&lt;/strong&gt;的应用开发中，数据存储在&lt;strong&gt;数据库&lt;/strong&gt;中，保留了应用的&lt;strong&gt;全部记忆&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;在 AI 时代，&lt;strong&gt;向量数据库&lt;/strong&gt;充当了这一角色&lt;ul&gt;
&lt;li&gt;在 RAG 系统中，数据被转换为&lt;strong&gt;高维向量&lt;/strong&gt;形式，使得语言模型能够进行高效的&lt;strong&gt;语义相似度&lt;/strong&gt;计算和检索&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在向量数据库中，查找变成了&lt;strong&gt;计算&lt;/strong&gt;每条记录的&lt;strong&gt;向量近似度&lt;/strong&gt;，然后按照&lt;strong&gt;分值&lt;/strong&gt;倒序返回结果&lt;/li&gt;
&lt;li&gt;RAG 就&lt;strong&gt;如何存储向量&lt;/strong&gt;的方法论，根据不同的&lt;strong&gt;实现策略&lt;/strong&gt;，衍生出了不同的 RAG 技术&lt;ul&gt;
&lt;li&gt;利用&lt;strong&gt;图结构&lt;/strong&gt;表示和检索知识的 &lt;strong&gt;GraphRAG&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;结合知识图谱增强生成能力的 &lt;strong&gt;KG-RAG&lt;/strong&gt; - Knowledge Graph Augmented Generation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;AI 应用的&lt;strong&gt;数据建模&lt;/strong&gt;强调的是数据的&lt;strong&gt;语义表示和关联&lt;/strong&gt;，以支持更灵活的&lt;strong&gt;查询&lt;/strong&gt;和&lt;strong&gt;推理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高质量的数据处理&lt;/strong&gt;，不仅影响&lt;strong&gt;检索的准确性&lt;/strong&gt;，还直接决定了 LLM &lt;strong&gt;生成内容的质量和可靠性&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Methodology</title>
    <link href="https://blog.zhongmingmao.top/2024/08/06/rag-methodology/"/>
    <id>https://blog.zhongmingmao.top/2024/08/06/rag-methodology/</id>
    <published>2024-08-05T16:06:25.000Z</published>
    <updated>2024-08-30T10:48:35.345Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;场景识别&quot;&gt;&lt;a href=&quot;#场景识别&quot; class=&quot;headerlink&quot; title=&quot;场景识别&quot;&gt;&lt;/a&gt;场景识别&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;分析&lt;strong&gt;业务流程&lt;/strong&gt;，找出业务中依赖大量&lt;strong&gt;知识&lt;/strong&gt;和&lt;strong&gt;信息&lt;/strong&gt;处理的环节&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;复杂决策&lt;/strong&gt;环节&lt;ul&gt;
&lt;li&gt;在需要&lt;strong&gt;多维度&lt;/strong&gt;信息综合分析与判断的业务流程中，RAG 可以为决策者提供&lt;strong&gt;实时的信息支持&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;使用 LLM 从各种来源的信息&lt;strong&gt;提炼&lt;/strong&gt;出&lt;strong&gt;关键点&lt;/strong&gt;，加权求和 - &lt;strong&gt;压缩&lt;/strong&gt;和整理&lt;strong&gt;非格式化&lt;/strong&gt;信息&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重复性&lt;/strong&gt;内容生成&lt;ul&gt;
&lt;li&gt;企业中有大量&lt;strong&gt;重复&lt;/strong&gt;且&lt;strong&gt;标准化&lt;/strong&gt;的内容生成任务&lt;/li&gt;
&lt;li&gt;可以用程序去&lt;strong&gt;归纳流程&lt;/strong&gt;，用 LLM 和 RAG 去&lt;strong&gt;填充&lt;/strong&gt;每个环节&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;用户交互&lt;/strong&gt;场景&lt;ul&gt;
&lt;li&gt;增加&lt;strong&gt;自然语言&lt;/strong&gt;交互&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - Document Parsing</title>
    <link href="https://blog.zhongmingmao.top/2024/08/05/rag-doc-parse/"/>
    <id>https://blog.zhongmingmao.top/2024/08/05/rag-doc-parse/</id>
    <published>2024-08-04T16:06:25.000Z</published>
    <updated>2024-08-30T08:51:37.864Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;文档解析&quot;&gt;&lt;a href=&quot;#文档解析&quot; class=&quot;headerlink&quot; title=&quot;文档解析&quot;&gt;&lt;/a&gt;文档解析&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;文档解析的本质 - 将&lt;strong&gt;格式各异&lt;/strong&gt;、&lt;strong&gt;版本多样&lt;/strong&gt;、&lt;strong&gt;元素多种&lt;/strong&gt;的文档数据，转化为&lt;strong&gt;阅读顺序正确&lt;/strong&gt;的&lt;strong&gt;字符串&lt;/strong&gt;信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quality in, Quality out&lt;/strong&gt; 是 LLM 的典型特征&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;高质量&lt;/strong&gt;的文档解析能够从各种&lt;strong&gt;复杂格式&lt;/strong&gt;的&lt;strong&gt;非结构化&lt;/strong&gt;数据中提取出&lt;strong&gt;高精度信息&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;对 RAG 系统的&lt;strong&gt;最终效果&lt;/strong&gt;起到&lt;strong&gt;决定性&lt;/strong&gt;作用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RAG 系统的应用场景主要集中在&lt;strong&gt;专业领域&lt;/strong&gt;和&lt;strong&gt;企业场景&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;除了&lt;strong&gt;数据库&lt;/strong&gt;，更多的数据以 PDF、Word 等&lt;strong&gt;多种格式&lt;/strong&gt;存储&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PDF&lt;/strong&gt; 文件有统一的排版和多样化的结构形式，是&lt;strong&gt;最为常见&lt;/strong&gt;的文档数据格式和交换格式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - LangChain</title>
    <link href="https://blog.zhongmingmao.top/2024/08/04/rag-langchain/"/>
    <id>https://blog.zhongmingmao.top/2024/08/04/rag-langchain/</id>
    <published>2024-08-03T16:06:25.000Z</published>
    <updated>2024-08-29T16:49:15.415Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Practice&quot;&gt;&lt;a href=&quot;#Practice&quot; class=&quot;headerlink&quot; title=&quot;Practice&quot;&gt;&lt;/a&gt;Practice&lt;/h1&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="LangChain" scheme="https://blog.zhongmingmao.top/tags/langchain/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - In Action</title>
    <link href="https://blog.zhongmingmao.top/2024/08/03/rag-in-action/"/>
    <id>https://blog.zhongmingmao.top/2024/08/03/rag-in-action/</id>
    <published>2024-08-02T16:06:25.000Z</published>
    <updated>2024-08-28T12:12:53.244Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;技术选型&quot;&gt;&lt;a href=&quot;#技术选型&quot; class=&quot;headerlink&quot; title=&quot;技术选型&quot;&gt;&lt;/a&gt;技术选型&lt;/h1&gt;&lt;h2 id=&quot;LangChain&quot;&gt;&lt;a href=&quot;#LangChain&quot; class=&quot;headerlink&quot; title=&quot;LangChain&quot;&gt;&lt;/a&gt;LangChain&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;LangChain 是专门为开发基于 &lt;strong&gt;LLM&lt;/strong&gt; 应用而设计的&lt;strong&gt;全面框架&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;LangChain 的核心目标是&lt;strong&gt;简化&lt;/strong&gt;开发者的&lt;strong&gt;构建流程&lt;/strong&gt;，使其能够高效地创建 LLM 驱动的应用&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG -Principle</title>
    <link href="https://blog.zhongmingmao.top/2024/08/02/rag-principle/"/>
    <id>https://blog.zhongmingmao.top/2024/08/02/rag-principle/</id>
    <published>2024-08-01T16:06:25.000Z</published>
    <updated>2024-08-27T09:47:40.614Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;LLM-局限&quot;&gt;&lt;a href=&quot;#LLM-局限&quot; class=&quot;headerlink&quot; title=&quot;LLM 局限&quot;&gt;&lt;/a&gt;LLM 局限&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;当设计一个 LLM &lt;strong&gt;问答&lt;/strong&gt;应用，模型需要处理用户的&lt;strong&gt;领域问题&lt;/strong&gt;时，LLM &lt;strong&gt;通常&lt;/strong&gt;表现&lt;strong&gt;出色&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;但有时提供的答案并&lt;strong&gt;不准确&lt;/strong&gt;，甚至出现&lt;strong&gt;错误&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;当用户需要&lt;strong&gt;获取实时信息&lt;/strong&gt;时，LLM 无法及时提供最新的答案&lt;/li&gt;
&lt;li&gt;LLM 在&lt;strong&gt;知识&lt;/strong&gt;、&lt;strong&gt;理解&lt;/strong&gt;和&lt;strong&gt;推理&lt;/strong&gt;方面展现了&lt;strong&gt;卓越&lt;/strong&gt;的能力，在&lt;strong&gt;复杂交互场景&lt;/strong&gt;中表现尤为突出&lt;/li&gt;
&lt;li&gt;LLM 存在无法忽略的局限性&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>RAG - AI 2.0</title>
    <link href="https://blog.zhongmingmao.top/2024/08/01/rag-ai-2/"/>
    <id>https://blog.zhongmingmao.top/2024/08/01/rag-ai-2/</id>
    <published>2024-07-31T16:06:25.000Z</published>
    <updated>2024-08-27T04:21:25.198Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;AI-技术&quot;&gt;&lt;a href=&quot;#AI-技术&quot; class=&quot;headerlink&quot; title=&quot;AI 技术&quot;&gt;&lt;/a&gt;AI 技术&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;做 &lt;strong&gt;AI 产品&lt;/strong&gt;的工程研发需充分掌握 &lt;strong&gt;AI 技术&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;AI 产品从 &lt;strong&gt;MVP&lt;/strong&gt; 到 &lt;strong&gt;PMF&lt;/strong&gt; 的演进过程中会面临非常多的挑战&lt;ul&gt;
&lt;li&gt;MVP - Minimum Viable Product - 最小可用产品&lt;/li&gt;
&lt;li&gt;PMF - Product-Market Fit - 产品市场契合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;要实现 AI 产品的 PMF&lt;ul&gt;
&lt;li&gt;首先需要&lt;strong&gt;充分了解 AI 技术&lt;/strong&gt;，明确&lt;strong&gt;技术边界&lt;/strong&gt;，找到&lt;strong&gt;合适 AI 技术&lt;/strong&gt;的&lt;strong&gt;应用场景&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;其次，需要深刻理解业务，用户需求决定产品方向，AI 技术是为业务服务的工具&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;验证&lt;/strong&gt;阶段，优先使用&lt;strong&gt;最佳 AI 模型&lt;/strong&gt;以确保产品&lt;strong&gt;满足市场需求&lt;/strong&gt;，确认后再逐步&lt;strong&gt;降低模型成本&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;坚持&lt;strong&gt;业务优先&lt;/strong&gt;、&lt;strong&gt;价值至上&lt;/strong&gt;的原则，避免&lt;strong&gt;纯 AI 科研化&lt;/strong&gt;，脱离实际场景做 AI 技术选型&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/categories/ai/rag/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="RAG" scheme="https://blog.zhongmingmao.top/tags/rag/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Model Structure</title>
    <link href="https://blog.zhongmingmao.top/2024/07/10/llm-core-model-structure/"/>
    <id>https://blog.zhongmingmao.top/2024/07/10/llm-core-model-structure/</id>
    <published>2024-07-09T16:06:25.000Z</published>
    <updated>2024-08-27T15:28:28.065Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;模型文件&quot;&gt;&lt;a href=&quot;#模型文件&quot; class=&quot;headerlink&quot; title=&quot;模型文件&quot;&gt;&lt;/a&gt;模型文件&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;模型文件，也叫&lt;strong&gt;模型权重&lt;/strong&gt;，里面大部分空间存放的是&lt;strong&gt;模型参数&lt;/strong&gt; - 即&lt;strong&gt;权重&lt;/strong&gt;（Weights）和&lt;strong&gt;偏置&lt;/strong&gt;（Biases）&lt;/li&gt;
&lt;li&gt;还有其它信息，如&lt;strong&gt;优化器状态&lt;/strong&gt;和&lt;strong&gt;元数据&lt;/strong&gt;等&lt;/li&gt;
&lt;li&gt;文件格式&lt;ul&gt;
&lt;li&gt;使用 &lt;strong&gt;PyTorch&lt;/strong&gt;，后缀为 &lt;code&gt;.pth&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;TensorFlow&lt;/strong&gt; 或者 &lt;strong&gt;Hugging Face Transformers&lt;/strong&gt;，后缀为 &lt;code&gt;.bin&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在模型&lt;strong&gt;预训练&lt;/strong&gt;后，可以保存模型，在&lt;strong&gt;生产&lt;/strong&gt;环境，&lt;strong&gt;不建议保存模型架构&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;与 &lt;strong&gt;Python 版本&lt;/strong&gt;和&lt;strong&gt;模型定义的代码&lt;/strong&gt;紧密相关，可能存在&lt;strong&gt;兼容性&lt;/strong&gt;问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Decoder Only</title>
    <link href="https://blog.zhongmingmao.top/2024/07/09/llm-core-decoder-only/"/>
    <id>https://blog.zhongmingmao.top/2024/07/09/llm-core-decoder-only/</id>
    <published>2024-07-08T16:06:25.000Z</published>
    <updated>2024-08-26T09:40:47.186Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;模型架构&quot;&gt;&lt;a href=&quot;#模型架构&quot; class=&quot;headerlink&quot; title=&quot;模型架构&quot;&gt;&lt;/a&gt;模型架构&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;已经演化出很多 Transformer 变体，用来适应不同的任务和性能需求&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;架构名称&lt;/th&gt;
&lt;th&gt;特点&lt;/th&gt;
&lt;th&gt;主要应用&lt;/th&gt;
&lt;th&gt;与原始 Transformer 的关系&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;原始 Transformer&lt;/td&gt;
&lt;td&gt;编码器-解码器结构&lt;/td&gt;
&lt;td&gt;机器翻译、文本摘要&lt;/td&gt;
&lt;td&gt;基础模型&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Decoder-only&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;只包含解码器&lt;/td&gt;
&lt;td&gt;文本生成&lt;/td&gt;
&lt;td&gt;去除了编码器部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Encoder-only&lt;/td&gt;
&lt;td&gt;只包含编码器&lt;/td&gt;
&lt;td&gt;文本分类、信息提取&lt;/td&gt;
&lt;td&gt;去除了解码器部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Transformer-XL&lt;/td&gt;
&lt;td&gt;加入循环机制&lt;/td&gt;
&lt;td&gt;长文本处理&lt;/td&gt;
&lt;td&gt;扩展了处理长序列的能力&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sparse Transformer&lt;/td&gt;
&lt;td&gt;引入稀疏注意力机制&lt;/td&gt;
&lt;td&gt;长序列处理&lt;/td&gt;
&lt;td&gt;优化了注意力计算效率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Universal Transformer&lt;/td&gt;
&lt;td&gt;递归的编码器结构&lt;/td&gt;
&lt;td&gt;各种序列处理&lt;/td&gt;
&lt;td&gt;引入递归机制，多次使用相同的参数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Conformer&lt;/td&gt;
&lt;td&gt;结合 CNN 和 Transformer 优势&lt;/td&gt;
&lt;td&gt;音频处理、语音识别&lt;/td&gt;
&lt;td&gt;引入卷积层处理局部特征&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vision Transformer&lt;/td&gt;
&lt;td&gt;应用于视觉领域&lt;/td&gt;
&lt;td&gt;图像分类、视觉任务&lt;/td&gt;
&lt;td&gt;将图像块处理为序列的 Transformer 编码器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Switch Transformer&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;使用稀疏性路由机制&lt;/td&gt;
&lt;td&gt;大规模模型训练&lt;/td&gt;
&lt;td&gt;提高了模型的可扩展性和效率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performer&lt;/td&gt;
&lt;td&gt;使用随机特征映射技术近似注意力机制&lt;/td&gt;
&lt;td&gt;处理非常长的序列&lt;/td&gt;
&lt;td&gt;降低计算负担，提高处理效率&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Transformer" scheme="https://blog.zhongmingmao.top/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Transformer</title>
    <link href="https://blog.zhongmingmao.top/2024/07/08/llm-core-transformer/"/>
    <id>https://blog.zhongmingmao.top/2024/07/08/llm-core-transformer/</id>
    <published>2024-07-07T16:06:25.000Z</published>
    <updated>2024-08-26T09:37:55.867Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;不论是 &lt;strong&gt;GRU&lt;/strong&gt; 还是 &lt;strong&gt;LSTM&lt;/strong&gt; 都面临&lt;strong&gt;梯度消失&lt;/strong&gt;和&lt;strong&gt;梯度爆炸&lt;/strong&gt;的问题&lt;/li&gt;
&lt;li&gt;RNN 必须&lt;strong&gt;按照顺序处理&lt;/strong&gt;序列中的每个元素，&lt;strong&gt;无法并发处理&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;RNN 还有&lt;strong&gt;长依赖&lt;/strong&gt;问题，虽然可以处理&lt;strong&gt;长序列&lt;/strong&gt;，但&lt;strong&gt;实战效果不佳&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Attention Is All You Need - &lt;a href=&quot;http://arxiv.org/pdf/1706.03762&quot;&gt;http://arxiv.org/pdf/1706.03762&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Transformer" scheme="https://blog.zhongmingmao.top/tags/transformer/"/>
    
    <category term="Seq2Seq" scheme="https://blog.zhongmingmao.top/tags/seq2seq/"/>
    
    <category term="Attention" scheme="https://blog.zhongmingmao.top/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Seq2Seq</title>
    <link href="https://blog.zhongmingmao.top/2024/07/07/llm-core-seq2seq/"/>
    <id>https://blog.zhongmingmao.top/2024/07/07/llm-core-seq2seq/</id>
    <published>2024-07-06T16:06:25.000Z</published>
    <updated>2024-08-24T16:38:44.933Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;简单介绍&quot;&gt;&lt;a href=&quot;#简单介绍&quot; class=&quot;headerlink&quot; title=&quot;简单介绍&quot;&gt;&lt;/a&gt;简单介绍&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt; 的主要能力是将&lt;strong&gt;词汇&lt;/strong&gt;放在&lt;strong&gt;多维空间&lt;/strong&gt;中，&lt;strong&gt;相似的词汇&lt;/strong&gt;会被放在&lt;strong&gt;邻近的位置&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Seq2Seq 不仅能&lt;strong&gt;理解词汇&lt;/strong&gt;，还能将词汇&lt;strong&gt;串联&lt;/strong&gt;成完整的&lt;strong&gt;句子&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Seq2Seq 即&lt;strong&gt;从一个序列到另一个序列的转换&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;不仅仅能&lt;strong&gt;理解单词之间的关系&lt;/strong&gt;，还能把整个句子的意思&lt;strong&gt;打包&lt;/strong&gt;，并&lt;strong&gt;解压&lt;/strong&gt;成另一种形式的表达&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Seq2Seq 的核心角色 - &lt;strong&gt;编码器&lt;/strong&gt;（Encoder） + &lt;strong&gt;解码器&lt;/strong&gt;（Decoder）&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Role&lt;/th&gt;
&lt;th&gt;Desc&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Encoder&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;理解和压缩信息&lt;/strong&gt; - 把一封长信函整理成一个精简的&lt;strong&gt;摘要&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;将&lt;strong&gt;摘要&lt;/strong&gt;打开，并翻译成另一种语言或形式的&lt;strong&gt;完整信息&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Seq2Seq" scheme="https://blog.zhongmingmao.top/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>LLM Core - Word2Vec</title>
    <link href="https://blog.zhongmingmao.top/2024/07/06/llm-core-word2vec/"/>
    <id>https://blog.zhongmingmao.top/2024/07/06/llm-core-word2vec/</id>
    <published>2024-07-05T16:06:25.000Z</published>
    <updated>2024-08-24T09:43:10.720Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Word2Vec&quot;&gt;&lt;a href=&quot;#Word2Vec&quot; class=&quot;headerlink&quot; title=&quot;Word2Vec&quot;&gt;&lt;/a&gt;Word2Vec&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;在 &lt;strong&gt;NLP&lt;/strong&gt; 中，在&lt;strong&gt;文本预处理&lt;/strong&gt;后，进行&lt;strong&gt;特征提取&lt;/strong&gt;，涉及到将&lt;strong&gt;词语&lt;/strong&gt;转化成&lt;strong&gt;数值&lt;/strong&gt;的形式，&lt;strong&gt;方便计算机理解&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Word2Vec 的目的 - 将&lt;strong&gt;词语&lt;/strong&gt;转换成&lt;strong&gt;向量&lt;/strong&gt;形式，使得&lt;strong&gt;计算机&lt;/strong&gt;能够&lt;strong&gt;理解&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;通过学习&lt;strong&gt;大量文本数据&lt;/strong&gt;，捕捉到&lt;strong&gt;词语之间的上下文关系&lt;/strong&gt;，进而生成&lt;strong&gt;词的高维表示&lt;/strong&gt; - 即&lt;strong&gt;词向量&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/categories/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/categories/ai/llm/"/>
    
    
    <category term="AI" scheme="https://blog.zhongmingmao.top/tags/ai/"/>
    
    <category term="LLM" scheme="https://blog.zhongmingmao.top/tags/llm/"/>
    
    <category term="NLP" scheme="https://blog.zhongmingmao.top/tags/nlp/"/>
    
    <category term="Word2Vec" scheme="https://blog.zhongmingmao.top/tags/word2vec/"/>
    
  </entry>
  
</feed>
